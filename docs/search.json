[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Welcome!",
    "text": "Welcome!\nWelcome to Ecosystem Science and Sustainability 523c: Environmental Data Science Applications: Water Resources! This class is meant to build on the technical skills you learned in ESS 523a, with a focus on water resource examples. We will cover a range of topics, including data science tools, working with vector and raster data, and machine learning."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including all slides, are made with Quarto. Please submit an issue on the GitHub repo for this course if you find something that could be fixed or improved.\nWe borrow significant content from the amazing R community and do our best to curate and design course content for students."
  },
  {
    "objectID": "index.html#reuse-and-licensing",
    "href": "index.html#reuse-and-licensing",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.¬†not an original creation and reused from another source), these educational materials are licensed under Apache 2."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Schedule",
    "text": "Schedule\n\nComponent 1: Data Science Tools\n Week 01: Level Setting\n Tech Talk 01: Quarto/Flextable\n Lab 01: Lab 1: COVID Trends\n\n\n\nComponent 2: Working with Vector Data\n Week 02: Projections & Measures\n Tech Talk 02: Interactive Mapping\n Lab 02: Lab 2: Border summaries\n Lab 02: Lab 2: Hints & Tricks\n\n Week 03: Predicates, Simplification & Tesselations\n Tech Talk 03: Functions\n Lab 03: Lab 3: Dams in the US\n\n\n\nComponent 3: Working with Raster Data\n Week 04: Raster‚Äôs in R\n Tech Talk 04: STAC\nLab 04: Lab 4: Remote Sensing for Flooding\n\n\n\nComponent 3: Machine Learning & Time series\n Week 05: Feature Engineering & Model Workflows\n Tech Talk 05: Live Demo\n Lab 05: Lab 5: CAMELS Data Part 1\n\n Week 06: Models, Evaluation, Tuning\n Tech Talk 06: Quarto Websites\n Lab 06: Personal Website\n\n Week 07: Time Series\n Tech Talk 07: Wrap up\n Lab 07: Poudre River Forecast"
  },
  {
    "objectID": "index.html#component-1-data-science-tools",
    "href": "index.html#component-1-data-science-tools",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Component 1: Data Science Tools",
    "text": "Component 1: Data Science Tools\n Week 01: Level Setting  Lab 01: COVID Trends"
  },
  {
    "objectID": "index.html#component-2-working-with-vector-data",
    "href": "index.html#component-2-working-with-vector-data",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Component 2: Working with Vector Data",
    "text": "Component 2: Working with Vector Data\n Week 02: Projections & Measures  Lab 02: 100 mile border zone\n Week 03: Predicates & Tesselations  Lab 03: Dams in the US"
  },
  {
    "objectID": "index.html#component-3-working-with-raster-data",
    "href": "index.html#component-3-working-with-raster-data",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Component 3: Working with Raster Data",
    "text": "Component 3: Working with Raster Data\n Week 04: Raster‚Äôs in R  Lab 04: Remote Sensing\n Week 05: Terrain Mapping  Lab 05: Terrain & Flood Modeling"
  },
  {
    "objectID": "index.html#component-3-machine-learning",
    "href": "index.html#component-3-machine-learning",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Component 3: Machine Learning",
    "text": "Component 3: Machine Learning\nBuild confidence in wrangling, visualizing, and analyzing data. This section covers importing and cleaning data sets, working with joins, and creating effective visualizations. You‚Äôll also delve into study design, hypothesis testing, and statistical analyses spanning uni-variate, bivariate, and multivariate techniques.\n Week 06: Feature Engineering & Model Setup  Lab 06: \n Week 07:   Lab 07:"
  },
  {
    "objectID": "slides/week-1.html#getting-started-with-r-for-data-science",
    "href": "slides/week-1.html#getting-started-with-r-for-data-science",
    "title": "Week 1",
    "section": "üöÄ Getting Started with R for Data Science",
    "text": "üöÄ Getting Started with R for Data Science\n\nWelcome to 523C: Environmental Data Science Applications: Water Resources!\nThis first lecture will introduce essential, high-level topics to help you build a strong foundation in R for environmental data science.\nThroughout the lecture, you will be asked to assess your comfort level with various topics via a Google survey.\nThe survey results will help tailor the course focus, ensuring that we reinforce challenging concepts while avoiding unnecessary review of familiar topics."
  },
  {
    "objectID": "slides/week-1.html#google-survey",
    "href": "slides/week-1.html#google-survey",
    "title": "Week 1",
    "section": "Google Survey",
    "text": "Google Survey\n\nPlease open this survey and answer the questions as we work through this lecture.\nYour responses will provide valuable insights into areas where additional explanations or hands-on exercises may be beneficial.\n\nGoogle Survey"
  },
  {
    "objectID": "slides/week-1.html#data-types",
    "href": "slides/week-1.html#data-types",
    "title": "Week 1",
    "section": "Data Types",
    "text": "Data Types\nR has five principal data types (excluding raw and complex):\n\nCharacter: A string of text, represented with quotes (e.g., ‚Äúhello‚Äù).\n\nUsed to store words, phrases, and categorical data.\n\nInteger: A whole number, explicitly defined with an L suffix (e.g., 42L).\n\nStored more efficiently than numeric values when decimals are not needed.\n\nNumeric: A floating-point number, used for decimal values (e.g., 3.1415).\n\nThis is the default type for numbers in R.\n\nBoolean (Logical): A logical value that represents TRUE or FALSE.\n\nCommonly used in logical operations and conditional statements.\n\n\n\ncharacter &lt;- \"a\"\ninteger &lt;- 1L\nnumeric &lt;- 3.3\nboolean &lt;- TRUE"
  },
  {
    "objectID": "slides/week-1.html#data-structures",
    "href": "slides/week-1.html#data-structures",
    "title": "Week 1",
    "section": "Data Structures",
    "text": "Data Structures\n\nWhen working with multiple values, we need data structures to store and manipulate data efficiently.\nR provides several types of data structures, each suited for different use cases.\n\nVector\n\nA vector is the most basic data structure in R and contains elements of the same type.\nVectors are created using the c() function.\n\n\nchar.vec &lt;- c(\"a\", \"b\", \"c\")\nboolean.vec &lt;- c(TRUE, FALSE, TRUE)\n\n\nLists allow for heterogeneous data types.\n\n\nlist &lt;- list(a = c(1,2,3),\n            b = c(TRUE, FALSE),\n            c = \"test\")"
  },
  {
    "objectID": "slides/week-1.html#installing-packages",
    "href": "slides/week-1.html#installing-packages",
    "title": "Week 1",
    "section": "üì¶ Installing Packages",
    "text": "üì¶ Installing Packages\n\nR has a vast ecosystem of packages that extend its capabilities both on CRAN and github\nTo install a package from CRAN, use install.packages().\nTo install a package from Github, use remotes::install_github()`.\nWe‚Äôll start by installing palmerpenguins, which contains a dataset on penguins.\n\n\ninstall.packages('palmerpenguins')"
  },
  {
    "objectID": "slides/week-1.html#attachingloading-packages",
    "href": "slides/week-1.html#attachingloading-packages",
    "title": "Week 1",
    "section": "Attaching/Loading Packages",
    "text": "Attaching/Loading Packages\n\nTo use an installed package, you need to load it in your current working session using library().\nHere, we load palmerpenguins for dataset exploration and tidyverse for data science workflows.\n\n\nlibrary(palmerpenguins) # üêß Fun dataset about penguins!\nlibrary(tidyverse)      # üõ† Essential for data science in R"
  },
  {
    "objectID": "slides/week-1.html#help-documentation",
    "href": "slides/week-1.html#help-documentation",
    "title": "Week 1",
    "section": "Help & Documentation",
    "text": "Help & Documentation\n\nR has built-in documentation that provides information about functions and datasets.\nTo access documentation, use ?function_name.\nExample: Viewing the help page for the penguins dataset.\n\n\n?penguins\n\n\nYou can also use help.search(\"keyword\") to look up topics of interest.\nFor vignettes (detailed guides), use vignette(\"package_name\")."
  },
  {
    "objectID": "slides/week-1.html#quarto-communication",
    "href": "slides/week-1.html#quarto-communication",
    "title": "Week 1",
    "section": "Quarto: Communication",
    "text": "Quarto: Communication\n\nIn this class we will use Quarto, a more modern, cross langauge version of Rmarkdown\nIf you are comfortable with Rmd, you‚Äôll quickly be able to transition to Qmd\nIf you are new to Rmd, you‚Äôll be able to learn the latest and greatest"
  },
  {
    "objectID": "slides/week-1.html#tidyverse-a-swiss-army-knife-for-data-science-r",
    "href": "slides/week-1.html#tidyverse-a-swiss-army-knife-for-data-science-r",
    "title": "Week 1",
    "section": "üåü Tidyverse: A Swiss Army Knife for Data Science R ",
    "text": "üåü Tidyverse: A Swiss Army Knife for Data Science R \n\nThe tidyverse is a collection of packages designed for data science.\nWe can see what it includes using the tidyverse_packages function:\n\n\ntidyverse_packages()\n#&gt;  [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n#&gt;  [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n#&gt;  [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n#&gt; [13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n#&gt; [17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n#&gt; [21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n#&gt; [25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n#&gt; [29] \"tidyr\"         \"xml2\"          \"tidyverse\""
  },
  {
    "objectID": "slides/week-1.html#glimpse",
    "href": "slides/week-1.html#glimpse",
    "title": "Week 1",
    "section": "glimpse ",
    "text": "glimpse \n\nThe glimpse() function provides a concise summary of a dataset.\n\n\nglimpse(penguins)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ‚Ä¶\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ‚Ä¶\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186‚Ä¶\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶"
  },
  {
    "objectID": "slides/week-1.html#readr",
    "href": "slides/week-1.html#readr",
    "title": "Week 1",
    "section": "readr ",
    "text": "readr \n\nThe readr package provides functions for reading data into R.\nThe read_csv() function reads comma-separated files.\nThe read_tsv() function reads tab-separated files.\nThe read_delim() function reads files with custom delimiters.\nIn all cases, more intellegent parsing is done than with base R equivalents.\n\nread_csv \n\npath = 'https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv'\n\n# base R\nread.csv(path) |&gt; \n  head()\n#&gt;    fips        LON      LAT\n#&gt; 1  1061  -85.83575 31.09404\n#&gt; 2  8125 -102.42587 40.00307\n#&gt; 3 17177  -89.66239 42.35138\n#&gt; 4 28153  -88.69577 31.64132\n#&gt; 5 34041  -74.99570 40.85940\n#&gt; 6 46051  -96.76981 45.17255\n\n# More inutitive readr\nread_csv(path) |&gt; \n  head()\n#&gt; # A tibble: 6 √ó 3\n#&gt;   fips     LON   LAT\n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 01061  -85.8  31.1\n#&gt; 2 08125 -102.   40.0\n#&gt; 3 17177  -89.7  42.4\n#&gt; 4 28153  -88.7  31.6\n#&gt; 5 34041  -75.0  40.9\n#&gt; 6 46051  -96.8  45.2"
  },
  {
    "objectID": "slides/week-1.html#dplyr",
    "href": "slides/week-1.html#dplyr",
    "title": "Week 1",
    "section": "dplyr ",
    "text": "dplyr \n\nThe dplyr package provides functions for data manipulation throuhg ‚Äòa grammar for data manipulation‚Äô.\nIt provides capabilities similar to SQL for data manipulation.\nIt includes functions for viewing, filtering, selecting, mutating, summarizing, and joining data."
  },
  {
    "objectID": "slides/week-1.html#left_join",
    "href": "slides/week-1.html#left_join",
    "title": "Week 1",
    "section": "left_join ",
    "text": "left_join \n\nselect(penguins, species, contains('bill')) |&gt; \n  left_join(species, by = \"species\")\n#&gt; # A tibble: 344 √ó 4\n#&gt;    species bill_length_mm bill_depth_mm species_id\n#&gt;    &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 Adelie            39.1          18.7          1\n#&gt;  2 Adelie            39.5          17.4          1\n#&gt;  3 Adelie            40.3          18            1\n#&gt;  4 Adelie            NA            NA            1\n#&gt;  5 Adelie            36.7          19.3          1\n#&gt;  6 Adelie            39.3          20.6          1\n#&gt;  7 Adelie            38.9          17.8          1\n#&gt;  8 Adelie            39.2          19.6          1\n#&gt;  9 Adelie            34.1          18.1          1\n#&gt; 10 Adelie            42            20.2          1\n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#right_join",
    "href": "slides/week-1.html#right_join",
    "title": "Week 1",
    "section": "right_join ",
    "text": "right_join \n\nselect(penguins, species, contains('bill')) |&gt; \n  right_join(species, by = \"species\")\n#&gt; # A tibble: 344 √ó 4\n#&gt;    species bill_length_mm bill_depth_mm species_id\n#&gt;    &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 Adelie            39.1          18.7          1\n#&gt;  2 Adelie            39.5          17.4          1\n#&gt;  3 Adelie            40.3          18            1\n#&gt;  4 Adelie            NA            NA            1\n#&gt;  5 Adelie            36.7          19.3          1\n#&gt;  6 Adelie            39.3          20.6          1\n#&gt;  7 Adelie            38.9          17.8          1\n#&gt;  8 Adelie            39.2          19.6          1\n#&gt;  9 Adelie            34.1          18.1          1\n#&gt; 10 Adelie            42            20.2          1\n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#inner_join",
    "href": "slides/week-1.html#inner_join",
    "title": "Week 1",
    "section": "inner_join ",
    "text": "inner_join \n\nselect(penguins, species, contains('bill')) |&gt; \n  right_join(species, by = \"species\")\n#&gt; # A tibble: 344 √ó 4\n#&gt;    species bill_length_mm bill_depth_mm species_id\n#&gt;    &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 Adelie            39.1          18.7          1\n#&gt;  2 Adelie            39.5          17.4          1\n#&gt;  3 Adelie            40.3          18            1\n#&gt;  4 Adelie            NA            NA            1\n#&gt;  5 Adelie            36.7          19.3          1\n#&gt;  6 Adelie            39.3          20.6          1\n#&gt;  7 Adelie            38.9          17.8          1\n#&gt;  8 Adelie            39.2          19.6          1\n#&gt;  9 Adelie            34.1          18.1          1\n#&gt; 10 Adelie            42            20.2          1\n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#full_join",
    "href": "slides/week-1.html#full_join",
    "title": "Week 1",
    "section": "full_join ",
    "text": "full_join \n\nselect(penguins, species, contains('bill')) |&gt; \n  right_join(species, by = \"species\")\n#&gt; # A tibble: 344 √ó 4\n#&gt;    species bill_length_mm bill_depth_mm species_id\n#&gt;    &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 Adelie            39.1          18.7          1\n#&gt;  2 Adelie            39.5          17.4          1\n#&gt;  3 Adelie            40.3          18            1\n#&gt;  4 Adelie            NA            NA            1\n#&gt;  5 Adelie            36.7          19.3          1\n#&gt;  6 Adelie            39.3          20.6          1\n#&gt;  7 Adelie            38.9          17.8          1\n#&gt;  8 Adelie            39.2          19.6          1\n#&gt;  9 Adelie            34.1          18.1          1\n#&gt; 10 Adelie            42            20.2          1\n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#ggplot2-visualization",
    "href": "slides/week-1.html#ggplot2-visualization",
    "title": "Week 1",
    "section": "ggplot2: Visualization ",
    "text": "ggplot2: Visualization \n\nThe ggplot2 package is used for data visualization.\nIt is based on the ‚Äúgrammar of graphics‚Äù, which allows for a high level of customization.\nggplot2 is built on the concept of layers, where each layer adds a different element to the plot."
  },
  {
    "objectID": "slides/week-1.html#tidyr",
    "href": "slides/week-1.html#tidyr",
    "title": "Week 1",
    "section": "tidyr ",
    "text": "tidyr \n\nThe tidyr package provides functions for data reshaping.\nIt includes functions for pivoting and nesting data."
  },
  {
    "objectID": "slides/week-1.html#drop_na",
    "href": "slides/week-1.html#drop_na",
    "title": "Week 1",
    "section": "drop_na",
    "text": "drop_na\n\nThe drop_na() function is used to remove rows with missing values.\n\n\npenguins |&gt; \n  drop_na()\n#&gt; # A tibble: 333 √ó 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  5 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  6 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  7 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  8 Adelie  Torgersen           41.1          17.6               182        3200\n#&gt;  9 Adelie  Torgersen           38.6          21.2               191        3800\n#&gt; 10 Adelie  Torgersen           34.6          21.1               198        4400\n#&gt; # ‚Ñπ 323 more rows\n#&gt; # ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nnest / unnest\n\nThe nest() function is used to nest data into a list-column.\nIt is useful when you want to group data together.\nExample: Nesting the penguins dataset by species.\n\n\npenguins |&gt; \n  nest(data = -species)\n#&gt; # A tibble: 3 √ó 2\n#&gt;   species   data              \n#&gt;   &lt;fct&gt;     &lt;list&gt;            \n#&gt; 1 Adelie    &lt;tibble [152 √ó 7]&gt;\n#&gt; 2 Gentoo    &lt;tibble [124 √ó 7]&gt;\n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;"
  },
  {
    "objectID": "slides/week-1.html#broom",
    "href": "slides/week-1.html#broom",
    "title": "Week 1",
    "section": "broom ",
    "text": "broom \n\nThe broom package is used to tidy model outputs.\nIt provides functions to convert model outputs into tidy data frames.\nExample: Tidying the model output."
  },
  {
    "objectID": "slides/week-1.html#purr",
    "href": "slides/week-1.html#purr",
    "title": "Week 1",
    "section": "purr",
    "text": "purr\n\nThe purrr package is used for functional programming.\nIt provides functions for working with lists and vectors.\n\nmap\n\nThe map() function is used to apply a function to each element of a list.\nIt is useful when you want to iterate over a list.\nExample: Fitting a linear model to each species in the penguins dataset.\n\n\npenguins |&gt; \n  nest(data = -species) |&gt; \n  mutate(lm = map(data, ~lm(body_mass_g ~ flipper_length_mm, data = .x)))\n#&gt; # A tibble: 3 √ó 3\n#&gt;   species   data               lm    \n#&gt;   &lt;fct&gt;     &lt;list&gt;             &lt;list&gt;\n#&gt; 1 Adelie    &lt;tibble [152 √ó 7]&gt; &lt;lm&gt;  \n#&gt; 2 Gentoo    &lt;tibble [124 √ó 7]&gt; &lt;lm&gt;  \n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;  &lt;lm&gt;\n\nmap_*\n\nThe map_*() functions are used to extract specific outputs from a list.\nThey are useful when you want to extract specific outputs from a list.\nExample: Extracting the R-squared values (doubles) from the linear models.\n\n\npenguins |&gt; \n  nest(data = -species) |&gt; \n  mutate(lm = map(data, ~lm(body_mass_g ~ flipper_length_mm, data = .x)),\n         r2 = map_dbl(lm, ~summary(.x)$r.squared))\n#&gt; # A tibble: 3 √ó 4\n#&gt;   species   data               lm        r2\n#&gt;   &lt;fct&gt;     &lt;list&gt;             &lt;list&gt; &lt;dbl&gt;\n#&gt; 1 Adelie    &lt;tibble [152 √ó 7]&gt; &lt;lm&gt;   0.219\n#&gt; 2 Gentoo    &lt;tibble [124 √ó 7]&gt; &lt;lm&gt;   0.494\n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;  &lt;lm&gt;   0.412\n\nmap2\n\nThe map2() function is used to iterate over two lists in parallel.\nIt is useful when you want to apply a function to two lists simultaneously.\nExample: Augmenting the linear models with the original data.\n\n\npenguins |&gt; \n  drop_na() |&gt; \n  nest(data = -species) |&gt; \n  mutate(lm_mod = map(data, ~lm(body_mass_g ~ flipper_length_mm, data = .x)),\n         r2 = map_dbl(lm_mod, ~summary(.x)$r.squared),\n         a  = map2(lm_mod, data, ~broom::augment(.x, .y))) \n#&gt; # A tibble: 3 √ó 5\n#&gt;   species   data               lm_mod    r2 a                  \n#&gt;   &lt;fct&gt;     &lt;list&gt;             &lt;list&gt; &lt;dbl&gt; &lt;list&gt;             \n#&gt; 1 Adelie    &lt;tibble [146 √ó 7]&gt; &lt;lm&gt;   0.216 &lt;tibble [146 √ó 13]&gt;\n#&gt; 2 Gentoo    &lt;tibble [119 √ó 7]&gt; &lt;lm&gt;   0.506 &lt;tibble [119 √ó 13]&gt;\n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;  &lt;lm&gt;   0.412 &lt;tibble [68 √ó 13]&gt;"
  },
  {
    "objectID": "slides/week-1.html#unit-3-machine-learning",
    "href": "slides/week-1.html#unit-3-machine-learning",
    "title": "Week 1",
    "section": "Unit 3: Machine Learning",
    "text": "Unit 3: Machine Learning\n\nlibrary(tidymodels)\ntidymodels_packages()\n#&gt;  [1] \"broom\"        \"cli\"          \"conflicted\"   \"dials\"        \"dplyr\"       \n#&gt;  [6] \"ggplot2\"      \"hardhat\"      \"infer\"        \"modeldata\"    \"parsnip\"     \n#&gt; [11] \"purrr\"        \"recipes\"      \"rlang\"        \"rsample\"      \"rstudioapi\"  \n#&gt; [16] \"tibble\"       \"tidyr\"        \"tune\"         \"workflows\"    \"workflowsets\"\n#&gt; [21] \"yardstick\"    \"tidymodels\""
  },
  {
    "objectID": "slides/week-1.html#seeds-for-reproducability",
    "href": "slides/week-1.html#seeds-for-reproducability",
    "title": "Week 1",
    "section": "Seeds for reproducability",
    "text": "Seeds for reproducability"
  },
  {
    "objectID": "slides/week-1.html#rsamples-for-resampling-and-cross-validation",
    "href": "slides/week-1.html#rsamples-for-resampling-and-cross-validation",
    "title": "Week 1",
    "section": "rsamples for resampling and cross-validation",
    "text": "rsamples for resampling and cross-validation\n\nThe rsample package is used for resampling and cross-validation.\nIt provides functions for creating resamples and cross-validation folds.\nExample: Creating a 5-fold cross-validation object for the penguins dataset.\n\n\nset.seed(123)\n\n(penguins_split &lt;- initial_split(drop_na(penguins), prop = 0.8, strata = species))\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;265/68/333&gt;\npenguins_train  &lt;- training(penguins_split)\npenguins_test   &lt;- testing(penguins_split)\n\npenguin_folds &lt;- vfold_cv(penguins_train, v = 5)"
  },
  {
    "objectID": "slides/week-1.html#recipes-for-feature-engineering",
    "href": "slides/week-1.html#recipes-for-feature-engineering",
    "title": "Week 1",
    "section": "recipes for feature engineering ",
    "text": "recipes for feature engineering \n\nThe recipes package is used for feature engineering.\nIt provides functions for preprocessing data before modeling.\nExample: Defining a recipe for feature engineering the penguins dataset.\n\n\n# Define recipe for feature engineering\npenguin_recipe &lt;- recipe(species ~ ., data = penguins_train) |&gt;\n  step_impute_knn(all_predictors()) |&gt;         # Impute missing values\n  step_normalize(all_numeric_predictors())     # Normalize numeric features"
  },
  {
    "objectID": "slides/week-1.html#parsnip-for-model-selection",
    "href": "slides/week-1.html#parsnip-for-model-selection",
    "title": "Week 1",
    "section": "Parsnip for model selection ",
    "text": "Parsnip for model selection \n\nThe parsnip package is used for model implementation\nIt provides functions for defining models types, engines, and modes.\nExample: Defining models for logistic regression, random forest, and decision tree.\n\n\n# Define models\nlog_reg_model &lt;- multinom_reg() |&gt; \n  set_engine(\"nnet\")  |&gt; \n  set_mode(\"classification\")\n\nrf_model &lt;- rand_forest(trees = 500) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"classification\")\n\ndt_model &lt;- decision_tree() |&gt; \n  set_mode(\"classification\")"
  },
  {
    "objectID": "slides/week-1.html#workflows-for-model-execution",
    "href": "slides/week-1.html#workflows-for-model-execution",
    "title": "Week 1",
    "section": "Workflows for model execution ",
    "text": "Workflows for model execution \n\nThe workflows package is used for model execution.\nIt provides functions for defining and executing workflows.\nExample: Creating a workflow for logistic regression.\n\n\n# Create workflow\nlog_reg_workflow &lt;- workflow() |&gt;\n  add_model(log_reg_model) |&gt;\n  add_recipe(penguin_recipe) |&gt; \n  fit_resamples(resamples = penguin_folds, \n                metrics = metric_set(roc_auc, accuracy))"
  },
  {
    "objectID": "slides/week-1.html#yardstick-for-model-evaluation",
    "href": "slides/week-1.html#yardstick-for-model-evaluation",
    "title": "Week 1",
    "section": "yardstick for model evaluation ",
    "text": "yardstick for model evaluation \n\ncollect_metrics(log_reg_workflow)\n#&gt; # A tibble: 2 √ó 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy multiclass     1     5       0 Preprocessor1_Model1\n#&gt; 2 roc_auc  hand_till      1     5       0 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-1.html#workflowsets-for-model-comparison",
    "href": "slides/week-1.html#workflowsets-for-model-comparison",
    "title": "Week 1",
    "section": "workflowsets for model comparison ",
    "text": "workflowsets for model comparison \n\nThe workflowsets package is used for model comparison.\nIt provides functions for comparing multiple models usingthe purrr mapping paradigm\nExample: Comparing logistic regression, random forest, and decision tree models.\n\n\n(workflowset &lt;- workflow_set(list(penguin_recipe), \n                             list(log_reg_model, rf_model, dt_model)) |&gt; \n  workflow_map(\"fit_resamples\", \n               resamples = penguin_folds, \n               metrics = metric_set(roc_auc, accuracy)))\n#&gt; # A workflow set/tibble: 3 √ó 4\n#&gt;   wflow_id             info             option    result   \n#&gt;   &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 recipe_multinom_reg  &lt;tibble [1 √ó 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 recipe_rand_forest   &lt;tibble [1 √ó 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n#&gt; 3 recipe_decision_tree &lt;tibble [1 √ó 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "slides/week-1.html#model-validation",
    "href": "slides/week-1.html#model-validation",
    "title": "Week 1",
    "section": "Model Validation  ",
    "text": "Model Validation  \n\nFinally, we can validate the model on the test set\nThe augment() function is used to add model predictions and residuals to the dataset.\nThe conf_mat() function is used to create a confusion matrix.\nExample: Validating the logistic regression model on the test set.\n\n\nworkflow() |&gt; \n  # Add model and recipe\n  add_model(log_reg_model) |&gt;\n  add_recipe(penguin_recipe) |&gt;\n  # Train model\n  fit(data = penguins_train) |&gt; \n  # Fit trained model to test set\n  fit(data = penguins_test) |&gt;  \n  # Extract Predictions\n  augment(penguins_test) |&gt; \n  conf_mat(truth = species, estimate = .pred_class) \n#&gt;            Truth\n#&gt; Prediction  Adelie Chinstrap Gentoo\n#&gt;   Adelie        30         0      0\n#&gt;   Chinstrap      0        14      0\n#&gt;   Gentoo         0         0     24"
  },
  {
    "objectID": "slides/week-1.html#io",
    "href": "slides/week-1.html#io",
    "title": "Week 1",
    "section": "I/O ",
    "text": "I/O \n\nThe st_read() function is used to read spatial data.\nIt is useful when you want to import spatial data into R for local or remote files.\nExample: Reading a Major Global Rivers."
  },
  {
    "objectID": "slides/week-1.html#geometries",
    "href": "slides/week-1.html#geometries",
    "title": "Week 1",
    "section": "Geometries ",
    "text": "Geometries \n\nThe geometry column contains the spatial information.\nIt is stored as a list-column of sfc objects.\nExample: Accessing the first geometry in the rivers dataset.\n\n\nrivers$geometry[1]\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 144.8258 ymin: 61.40833 xmax: 160.7636 ymax: 68.8008\n#&gt; Geodetic CRS:  WGS 84"
  },
  {
    "objectID": "slides/week-1.html#data-manipulation",
    "href": "slides/week-1.html#data-manipulation",
    "title": "Week 1",
    "section": "Data Manipulation ",
    "text": "Data Manipulation \n\nAll dplyr verbs work with sf objects.\nExample: Filtering the rivers dataset to include only the Mississippi River.\n\n\nmississippi &lt;- filter(rivers, SYSTEM == \"Mississippi\")\nlarimer     &lt;- filter(counties, name == \"Larimer\")"
  },
  {
    "objectID": "slides/week-1.html#measures",
    "href": "slides/week-1.html#measures",
    "title": "Week 1",
    "section": "Measures ",
    "text": "Measures \n\nThe st_length() function is used to calculate the length of a geometry.\nThe st_area() function is used to calculate the area of a geometry.\nThe st_distance() function is used to calculate the distance between two geometries.\nExample: Calculating the length of the Mississippi River and the area of Larimer County.\n\n\nst_length(mississippi)\n#&gt; Units: [m]\n#&gt; [1] 1912869 3147943 3331900 1785519\n\nst_area(larimer)\n#&gt; 6813621254 [m^2]\n\nst_distance(larimer, mississippi)\n#&gt; Units: [m]\n#&gt;          [,1]    [,2]   [,3]    [,4]\n#&gt; [1,] 116016.6 1009375 526454 1413983"
  },
  {
    "objectID": "slides/week-1.html#predicates",
    "href": "slides/week-1.html#predicates",
    "title": "Week 1",
    "section": "Predicates ",
    "text": "Predicates \n\nSpatial predicates are used to check relationships between geometries using the DE-9IM model.\nThe st_intersects() function is used to check if geometries intersect.\nThe st_filter() function is used to filter geometries based on a predicate.\n\n\n\n\nst_intersects(counties, mississippi)\n#&gt; Sparse geometry binary predicate list of length 3108, where the\n#&gt; predicate was `intersects'\n#&gt; first 10 elements:\n#&gt;  1: (empty)\n#&gt;  2: (empty)\n#&gt;  3: (empty)\n#&gt;  4: (empty)\n#&gt;  5: (empty)\n#&gt;  6: (empty)\n#&gt;  7: (empty)\n#&gt;  8: (empty)\n#&gt;  9: (empty)\n#&gt;  10: (empty)\n\n\n\nints &lt;- st_filter(counties, mississippi, .predicate = st_intersects)\n\nggplot() + \n  geom_sf(data = ints) +\n  geom_sf(data = mississippi, col = \"blue\") + \n  theme_bw()"
  },
  {
    "objectID": "slides/week-1.html#io-1",
    "href": "slides/week-1.html#io-1",
    "title": "Week 1",
    "section": "I/O ",
    "text": "I/O \n\nAny raster format that GDAL can read, can be read with rast().\nThe package loads the native GDAL src library (like sf)\nrast reads data headers, not data itself, until needed.\nExample: Reading a GeoTIF of Colorado elevation.\n\n\n(elev = terra::rast('data/colorado_elevation.tif'))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 16893, 21395, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 30, 30  (x, y)\n#&gt; extent      : -1146465, -504615, 1566915, 2073705  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source      : colorado_elevation.tif \n#&gt; name        : CONUS_dem \n#&gt; min value   :     98679 \n#&gt; max value   :    439481"
  },
  {
    "objectID": "slides/week-1.html#cropmask",
    "href": "slides/week-1.html#cropmask",
    "title": "Week 1",
    "section": "Crop/Mask ",
    "text": "Crop/Mask \n\nThe crop() function is used to crop a raster to a specific extent.\nIt is useful when you want to work with a subset of the data.\ncrop extracts data (whether from a remote or local source)\nThe mask() function is used to mask a raster using a vector or other extent, keeping only the data within the mask.\nInput extents must match the CRS of the raster data\nExample: Cropping then masking the elevation raster to Larimer County.\n\n\n\n\nlarimer_5070 &lt;- st_transform(larimer, crs(elev))\n\nlarimer_elev = crop(elev, larimer_5070)\n\nplot(larimer_elev)\n\n\n\n\n\n\n\n\n\n\nlarimer_mask &lt;- mask(larimer_elev, larimer_5070)\nplot(larimer_mask)"
  },
  {
    "objectID": "slides/week-1.html#summary-algebra",
    "href": "slides/week-1.html#summary-algebra",
    "title": "Week 1",
    "section": "Summary / Algebra ",
    "text": "Summary / Algebra \n\nRasters can be added, subtracted, multiplied, and divided\nAny form of map algebra can be done with rasters\nFor example, multiplying the Larimer mask by 2\n\n\n\nraw\n\nlarimer_mask\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3054, 3469, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 30, 30  (x, y)\n#&gt; extent      : -849255, -745185, 1952655, 2044275  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; varname     : colorado_elevation \n#&gt; name        : CONUS_dem \n#&gt; min value   :    145787 \n#&gt; max value   :    412773\n\n\nData Operation\n\nelev2 &lt;- larimer_mask^2\n\n\nrast modified by rast\n\nlarimer_mask / elev2\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3054, 3469, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 30, 30  (x, y)\n#&gt; extent      : -849255, -745185, 1952655, 2044275  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; varname     : colorado_elevation \n#&gt; name        :    CONUS_dem \n#&gt; min value   : 2.422639e-06 \n#&gt; max value   : 6.859322e-06\n\n\nstatistical methods\n\n(scaled = scale(larimer_mask))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3054, 3469, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 30, 30  (x, y)\n#&gt; extent      : -849255, -745185, 1952655, 2044275  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; varname     : colorado_elevation \n#&gt; name        : CONUS_dem \n#&gt; min value   : -1.562331 \n#&gt; max value   :  3.053412"
  },
  {
    "objectID": "slides/week-1.html#raster-data-store",
    "href": "slides/week-1.html#raster-data-store",
    "title": "Week 1",
    "section": "Raster data store",
    "text": "Raster data store\n\nRasters are matrices or arrays of values, and can be manipulated as such\nFor example, setting 35% of the raster to NA\n\n\nlarimer_elev[sample(ncell(larimer_elev), .35*ncell(larimer_elev))] &lt;-  NA\n\nplot(larimer_elev)"
  },
  {
    "objectID": "slides/week-1.html#focal",
    "href": "slides/week-1.html#focal",
    "title": "Week 1",
    "section": "Focal ",
    "text": "Focal \n\nThe focal() function is used to calculate focal statistics.\nIt is useful when you want to calculate statistics for each cell based on its neighbors.\nExample: Calculating the mean elevation within a 30-cell window to remove the NAs we just created\n\n\nxx = terra::focal(larimer_elev, win = 30, fun  = \"mean\", na.policy=\"only\")\nplot(xx)"
  },
  {
    "objectID": "slides/week-1.html#drop_na-na.omit",
    "href": "slides/week-1.html#drop_na-na.omit",
    "title": "Week 1",
    "section": "drop_na / na.omit ",
    "text": "drop_na / na.omit \n\nThe drop_na() function is used to remove rows with missing values.\n\n\npenguins |&gt; \n  drop_na()\n#&gt; # A tibble: 333 √ó 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  5 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  6 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  7 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  8 Adelie  Torgersen           41.1          17.6               182        3200\n#&gt;  9 Adelie  Torgersen           38.6          21.2               191        3800\n#&gt; 10 Adelie  Torgersen           34.6          21.1               198        4400\n#&gt; # ‚Ñπ 323 more rows\n#&gt; # ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nnest / unnest \n\nThe nest() function is used to nest data into a list-column.\nIt is useful when you want to group data together.\nExample: Nesting the penguins dataset by species.\n\n\npenguins |&gt; \n  nest(data = -species)\n#&gt; # A tibble: 3 √ó 2\n#&gt;   species   data              \n#&gt;   &lt;fct&gt;     &lt;list&gt;            \n#&gt; 1 Adelie    &lt;tibble [152 √ó 7]&gt;\n#&gt; 2 Gentoo    &lt;tibble [124 √ó 7]&gt;\n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;"
  },
  {
    "objectID": "slides/week-1.html#unions-combines",
    "href": "slides/week-1.html#unions-combines",
    "title": "Week 1",
    "section": "Unions / Combines ",
    "text": "Unions / Combines \n\nThe st_union() function is used to combine geometries.\nIt is useful when you want to merge geometries.\n\n\nmississippi\n#&gt; Simple feature collection with 4 features and 4 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -112 ymin: 28.92983 xmax: -77.86168 ymax: 48.16158\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 4 √ó 5\n#&gt;   NAME        SYSTEM      MILES KILOMETERS                              geometry\n#&gt; * &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;                 &lt;MULTILINESTRING [¬∞]&gt;\n#&gt; 1 Arkansas    Mississippi 1446.      2327. ((-106.3789 39.36165, -106.3295 39.3‚Ä¶\n#&gt; 2 Mississippi Mississippi 2385.      3838. ((-95.02364 47.15609, -94.98973 47.3‚Ä¶\n#&gt; 3 Missouri    Mississippi 2739.      4408. ((-110.5545 44.76081, -110.6122 44.7‚Ä¶\n#&gt; 4 Ohio        Mississippi 1368.      2202. ((-89.12166 36.97756, -89.17502 37.0‚Ä¶\n\nst_union(mississippi)\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -112 ymin: 28.92983 xmax: -77.86168 ymax: 48.16158\n#&gt; Geodetic CRS:  WGS 84"
  },
  {
    "objectID": "slides/week-1.html#terra",
    "href": "slides/week-1.html#terra",
    "title": "Week 1",
    "section": "terra ",
    "text": "terra \n\nThe terra package is used for working with raster data.\nIt provides functions for reading, writing, and manipulating raster data.\n\n\nlibrary(terra)\ngdal()\n#&gt; [1] \"3.10.1\""
  },
  {
    "objectID": "slides/week-1.html#dplyr-1",
    "href": "slides/week-1.html#dplyr-1",
    "title": "Week 1",
    "section": "dplyr ",
    "text": "dplyr \n\nThe dplyr package provides functions for data manipulation.\nIt includes functions for filtering, selecting, mutating, summarizing, and joining data.\n\nselect \n\nThe select() function is used to select columns from a dataset.\nIt is useful when you want to work with specific columns.\nExample: Selecting the species column from the penguins dataset.\n\n\nselect(penguins, species)\n#&gt; # A tibble: 344 √ó 1\n#&gt;    species\n#&gt;    &lt;fct&gt;  \n#&gt;  1 Adelie \n#&gt;  2 Adelie \n#&gt;  3 Adelie \n#&gt;  4 Adelie \n#&gt;  5 Adelie \n#&gt;  6 Adelie \n#&gt;  7 Adelie \n#&gt;  8 Adelie \n#&gt;  9 Adelie \n#&gt; 10 Adelie \n#&gt; # ‚Ñπ 334 more rows\n\nfilter \n\nThe filter() function is used to filter rows based on a condition.\nIt is useful when you want to work with specific rows.\nExample: Filtering the penguins dataset to include only Adelie penguins.\n\n\nfilter(penguins, species == \"Adelie\")\n#&gt; # A tibble: 152 √ó 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ‚Ñπ 142 more rows\n#&gt; # ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nmutate \n\nThe mutate() function is used to create new columns or modify existing ones.\nIt is useful when you want to add new information to your dataset.\nExample: Creating a new column bill_length_cm from bill_length_mm.\n\nNote the use of the tidy_select helper starts_with\n\nmutate(penguins, bill_length_cm = bill_length_mm / 100) |&gt; \n  select(starts_with(\"bill\"))\n#&gt; # A tibble: 344 √ó 3\n#&gt;    bill_length_mm bill_depth_mm bill_length_cm\n#&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n#&gt;  1           39.1          18.7          0.391\n#&gt;  2           39.5          17.4          0.395\n#&gt;  3           40.3          18            0.403\n#&gt;  4           NA            NA           NA    \n#&gt;  5           36.7          19.3          0.367\n#&gt;  6           39.3          20.6          0.393\n#&gt;  7           38.9          17.8          0.389\n#&gt;  8           39.2          19.6          0.392\n#&gt;  9           34.1          18.1          0.341\n#&gt; 10           42            20.2          0.42 \n#&gt; # ‚Ñπ 334 more rows\n\nsummarize \n\nThe summarize() function is used to aggregate data.\nIt is useful when you want to calculate summary statistics.\nIt always produces a one-row output.\nExample: Calculating the mean bill_length_mm for all penguins\n\n\nsummarize(penguins, bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n#&gt; # A tibble: 1 √ó 1\n#&gt;   bill_length_mm\n#&gt;            &lt;dbl&gt;\n#&gt; 1           43.9\n\ngroup_by / ungroup \n\nThe group_by() function is used to group data by one or more columns.\nIt is useful when you want to perform operations on groups.\nIt does this by adding a grouped_df class to the dataset.\nThe ungroup() function removes grouping from a dataset.\n\n\ngroups &lt;- group_by(penguins, species)\n\ndplyr::group_keys(groups)\n#&gt; # A tibble: 3 √ó 1\n#&gt;   species  \n#&gt;   &lt;fct&gt;    \n#&gt; 1 Adelie   \n#&gt; 2 Chinstrap\n#&gt; 3 Gentoo\ndplyr::group_indices(groups)[1:5]\n#&gt; [1] 1 1 1 1 1\n\nGroup operations\n\nExample: Grouping the penguins dataset by species and calculating the mean bill_length_mm.\n\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(bill_length_mm = mean(bill_length_mm, na.rm = TRUE)) |&gt; \n  ungroup()\n#&gt; # A tibble: 3 √ó 2\n#&gt;   species   bill_length_mm\n#&gt;   &lt;fct&gt;              &lt;dbl&gt;\n#&gt; 1 Adelie              38.8\n#&gt; 2 Chinstrap           48.8\n#&gt; 3 Gentoo              47.5\n\nJoins \n\nThe dplyr package provides functions for joining datasets.\nCommon join functions include inner_join(), left_join(), right_join(), and full_join().\nJoins are used to combine datasets based on shared keys (primary and foreign).\n\nMutating joins \n\nMutating joins add columns from one dataset to another based on a shared key.\nExample: Adding species information to the penguins dataset based on the species_id.\n\n\nspecies &lt;- tribble(\n  ~species_id, ~species,\n  1, \"Adelie\",\n  2, \"Chinstrap\",\n  3, \"Gentoo\"\n)"
  },
  {
    "objectID": "slides/week-1.html#section",
    "href": "slides/week-1.html#section",
    "title": "Week 1",
    "section": "%>% / |> ",
    "text": "%&gt;% / |&gt; \n\nThe pipe operator %&gt;% is used to chain operations in R.\nThe pipe operator |&gt; is a base R version of %&gt;% introduced in R 4.1.\nThe pipe passes what on the ‚Äúleft hand‚Äù side to the function on the ‚Äúright hand‚Äù side as the first argument.\n\n\npenguins |&gt; \n  glimpse()\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ‚Ä¶\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ‚Ä¶\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186‚Ä¶\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶"
  },
  {
    "objectID": "slides/week-1.html#select",
    "href": "slides/week-1.html#select",
    "title": "Week 1",
    "section": "select ",
    "text": "select \n\nThe select() function is used to select columns from a dataset.\nIt is useful when you want to work with specific columns.\nExample: Selecting the species column from the penguins dataset.\n\n\nselect(penguins, species)\n#&gt; # A tibble: 344 √ó 1\n#&gt;    species\n#&gt;    &lt;fct&gt;  \n#&gt;  1 Adelie \n#&gt;  2 Adelie \n#&gt;  3 Adelie \n#&gt;  4 Adelie \n#&gt;  5 Adelie \n#&gt;  6 Adelie \n#&gt;  7 Adelie \n#&gt;  8 Adelie \n#&gt;  9 Adelie \n#&gt; 10 Adelie \n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#filter",
    "href": "slides/week-1.html#filter",
    "title": "Week 1",
    "section": "filter ",
    "text": "filter \n\nThe filter() function is used to filter rows based on a condition.\nIt is useful when you want to work with specific rows.\nExample: Filtering the penguins dataset to include only Adelie penguins.\n\n\nfilter(penguins, species == \"Adelie\")\n#&gt; # A tibble: 152 √ó 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ‚Ñπ 142 more rows\n#&gt; # ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/week-1.html#mutate",
    "href": "slides/week-1.html#mutate",
    "title": "Week 1",
    "section": "mutate ",
    "text": "mutate \n\nThe mutate() function is used to create new columns or modify existing ones.\nIt is useful when you want to add new information to your dataset.\nExample: Creating a new column bill_length_cm from bill_length_mm.\n\nNote the use of the tidy_select helper starts_with\n\nmutate(penguins, bill_length_cm = bill_length_mm / 100) |&gt; \n  select(starts_with(\"bill\"))\n#&gt; # A tibble: 344 √ó 3\n#&gt;    bill_length_mm bill_depth_mm bill_length_cm\n#&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n#&gt;  1           39.1          18.7          0.391\n#&gt;  2           39.5          17.4          0.395\n#&gt;  3           40.3          18            0.403\n#&gt;  4           NA            NA           NA    \n#&gt;  5           36.7          19.3          0.367\n#&gt;  6           39.3          20.6          0.393\n#&gt;  7           38.9          17.8          0.389\n#&gt;  8           39.2          19.6          0.392\n#&gt;  9           34.1          18.1          0.341\n#&gt; 10           42            20.2          0.42 \n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#summarize",
    "href": "slides/week-1.html#summarize",
    "title": "Week 1",
    "section": "summarize ",
    "text": "summarize \n\nThe summarize() function is used to aggregate data.\nIt is useful when you want to calculate summary statistics.\nIt always produces a one-row output.\nExample: Calculating the mean bill_length_mm for all penguins\n\n\nsummarize(penguins, bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n#&gt; # A tibble: 1 √ó 1\n#&gt;   bill_length_mm\n#&gt;            &lt;dbl&gt;\n#&gt; 1           43.9"
  },
  {
    "objectID": "slides/week-1.html#group_by-ungroup",
    "href": "slides/week-1.html#group_by-ungroup",
    "title": "Week 1",
    "section": "group_by / ungroup ",
    "text": "group_by / ungroup \n\nThe group_by() function is used to group data by one or more columns.\nIt is useful when you want to perform operations on groups.\nIt does this by adding a grouped_df class to the dataset.\nThe ungroup() function removes grouping from a dataset.\n\n\ngroups &lt;- group_by(penguins, species)\n\ndplyr::group_keys(groups)\n#&gt; # A tibble: 3 √ó 1\n#&gt;   species  \n#&gt;   &lt;fct&gt;    \n#&gt; 1 Adelie   \n#&gt; 2 Chinstrap\n#&gt; 3 Gentoo\ndplyr::group_indices(groups)[1:5]\n#&gt; [1] 1 1 1 1 1"
  },
  {
    "objectID": "slides/week-1.html#group-operations",
    "href": "slides/week-1.html#group-operations",
    "title": "Week 1",
    "section": "Group operations ",
    "text": "Group operations \n\nExample: Grouping the penguins dataset by species and calculating the mean bill_length_mm.\n\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(bill_length_mm = mean(bill_length_mm, na.rm = TRUE)) |&gt; \n  ungroup()\n#&gt; # A tibble: 3 √ó 2\n#&gt;   species   bill_length_mm\n#&gt;   &lt;fct&gt;              &lt;dbl&gt;\n#&gt; 1 Adelie              38.8\n#&gt; 2 Chinstrap           48.8\n#&gt; 3 Gentoo              47.5"
  },
  {
    "objectID": "slides/week-1.html#joins",
    "href": "slides/week-1.html#joins",
    "title": "Week 1",
    "section": "Joins ",
    "text": "Joins \n\nThe dplyr package provides functions for joining datasets.\nCommon join functions include inner_join(), left_join(), right_join(), and full_join().\nJoins are used to combine datasets based on shared keys (primary and foreign)."
  },
  {
    "objectID": "slides/week-1.html#mutating-joins",
    "href": "slides/week-1.html#mutating-joins",
    "title": "Week 1",
    "section": "Mutating joins ",
    "text": "Mutating joins \n\nMutating joins add columns from one dataset to another based on a shared key.\nExample: Adding species information to the penguins dataset based on the species_id.\n\n\nspecies &lt;- tribble(\n  ~species_id, ~species,\n  1, \"Adelie\",\n  2, \"Chinstrap\",\n  3, \"Gentoo\"\n)"
  },
  {
    "objectID": "slides/week-1.html#filtering-joins",
    "href": "slides/week-1.html#filtering-joins",
    "title": "Week 1",
    "section": "Filtering Joins ",
    "text": "Filtering Joins \n\nFiltering joins retain only rows that match between datasets.\nExample: Filtering the penguins dataset to include only rows with matching species_id.\n\n\nselect(penguins, species, contains('bill')) |&gt; \n  semi_join(species, by = \"species\")\n#&gt; # A tibble: 344 √ó 3\n#&gt;    species bill_length_mm bill_depth_mm\n#&gt;    &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1 Adelie            39.1          18.7\n#&gt;  2 Adelie            39.5          17.4\n#&gt;  3 Adelie            40.3          18  \n#&gt;  4 Adelie            NA            NA  \n#&gt;  5 Adelie            36.7          19.3\n#&gt;  6 Adelie            39.3          20.6\n#&gt;  7 Adelie            38.9          17.8\n#&gt;  8 Adelie            39.2          19.6\n#&gt;  9 Adelie            34.1          18.1\n#&gt; 10 Adelie            42            20.2\n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#ggplot",
    "href": "slides/week-1.html#ggplot",
    "title": "Week 1",
    "section": "ggplot ",
    "text": "ggplot \n\nThe ggplot() function initializes a plot.\nIt provides a blank canvas to which layers can be added.\n\n\nggplot()"
  },
  {
    "objectID": "slides/week-1.html#geom_",
    "href": "slides/week-1.html#geom_",
    "title": "Week 1",
    "section": "geom_* ",
    "text": "geom_* \n\nThe geom_*() functions add geometric objects to the plot.\nThey describe how to render the mapping created in aes\nExample: Adding points to the plot.\n\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point()"
  },
  {
    "objectID": "slides/week-1.html#labs",
    "href": "slides/week-1.html#labs",
    "title": "Week 1",
    "section": "labs ",
    "text": "labs \n\nThe labs() function is used to add titles, subtitles, and axis labels to the plot.\nIt is useful for providing context and making the plot more informative.\nExample: Adding titles and axis labels to the plot.\n\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() + \n  facet_wrap(~species) + \n  theme_linedraw() + \n  labs(title = \"Penguins Weight by Bill Size\", \n       x = \"Body Mass\",\n       y = \"Bill Length\", \n       subtitle = \"Made for 523c\")"
  },
  {
    "objectID": "slides/week-1.html#raster-structure",
    "href": "slides/week-1.html#raster-structure",
    "title": "Week 1",
    "section": "Raster Structure ",
    "text": "Raster Structure \nRaster data is stored as an multi-dimensional array of values. - Remember this is atomic vector with diminisions - The same way we looked\n\nv &lt;- values(elev)\nhead(v)\n#&gt;      CONUS_dem\n#&gt; [1,]    242037\n#&gt; [2,]    243793\n#&gt; [3,]    244464\n#&gt; [4,]    244302\n#&gt; [5,]    244060\n#&gt; [6,]    243888\nclass(v[,1])\n#&gt; [1] \"integer\"\n\ndim(v)\n#&gt; [1] 361425735         1\ndim(elev)\n#&gt; [1] 16893 21395     1\nnrow(elev)\n#&gt; [1] 16893"
  },
  {
    "objectID": "slides/week-1.html#purrr",
    "href": "slides/week-1.html#purrr",
    "title": "Week 1",
    "section": "purrr ",
    "text": "purrr \n\nThe purrr package is used for functional programming.\nIt provides functions for working with lists and vectors."
  },
  {
    "objectID": "slides/week-1.html#machine-learning",
    "href": "slides/week-1.html#machine-learning",
    "title": "Week 1",
    "section": "Machine Learning ",
    "text": "Machine Learning \n\nlibrary(tidymodels)\ntidymodels_packages()\n#&gt;  [1] \"broom\"        \"cli\"          \"conflicted\"   \"dials\"        \"dplyr\"       \n#&gt;  [6] \"ggplot2\"      \"hardhat\"      \"infer\"        \"modeldata\"    \"parsnip\"     \n#&gt; [11] \"purrr\"        \"recipes\"      \"rlang\"        \"rsample\"      \"rstudioapi\"  \n#&gt; [16] \"tibble\"       \"tidyr\"        \"tune\"         \"workflows\"    \"workflowsets\"\n#&gt; [21] \"yardstick\"    \"tidymodels\""
  },
  {
    "objectID": "slides/week-1.html#linear-modeling-lm",
    "href": "slides/week-1.html#linear-modeling-lm",
    "title": "Week 1",
    "section": "linear modeling: lm",
    "text": "linear modeling: lm\n\nThe lm() function is used to fit linear models.\nIt is useful when you want to model the relationship between two variables.\nExample: Fitting a linear model to predict body_mass_g from flipper_length_mm.\n\n\nmodel &lt;- lm(body_mass_g ~ flipper_length_mm, data = drop_na(penguins))\n\nsummary(model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm, data = drop_na(penguins))\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1057.33  -259.79   -12.24   242.97  1293.89 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       -5872.09     310.29  -18.93   &lt;2e-16 ***\n#&gt; flipper_length_mm    50.15       1.54   32.56   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 393.3 on 331 degrees of freedom\n#&gt; Multiple R-squared:  0.7621, Adjusted R-squared:  0.7614 \n#&gt; F-statistic:  1060 on 1 and 331 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/week-1.html#tidy",
    "href": "slides/week-1.html#tidy",
    "title": "Week 1",
    "section": "tidy ",
    "text": "tidy \n\nThe tidy() function is used to tidy model coefficients.\nIt is useful when you want to extract model coefficients.\nExample: Tidying the model output.\n\n\ntidy(model)\n#&gt; # A tibble: 2 √ó 5\n#&gt;   term              estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)        -5872.     310.       -18.9 1.18e- 54\n#&gt; 2 flipper_length_mm     50.2      1.54      32.6 3.13e-105"
  },
  {
    "objectID": "slides/week-1.html#glance",
    "href": "slides/week-1.html#glance",
    "title": "Week 1",
    "section": "glance ",
    "text": "glance \n\nThe glance() function is used to provide a summary of model fit.\nIt is useful when you want to assess model performance.\nExample: Glancing at the model output.\n\n\nglance(model)\n#&gt; # A tibble: 1 √ó 12\n#&gt;   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.762         0.761  393.     1060. 3.13e-105     1 -2461. 4928. 4940.\n#&gt; # ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "slides/week-1.html#augment",
    "href": "slides/week-1.html#augment",
    "title": "Week 1",
    "section": "augment ",
    "text": "augment \n\nThe augment() function is used to add model predictions and residuals to the dataset.\nIt is useful when you want to visualize model performance.\nExample: Augmenting the model output.\n\n\n\n\na &lt;- augment(model)\n\nggplot(a, aes(x = .fitted, y = body_mass_g)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\nggplot(a, aes(x = .resid)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/week-1.html#map",
    "href": "slides/week-1.html#map",
    "title": "Week 1",
    "section": "map ",
    "text": "map \n\nThe map() function is used to apply a function to each element of a list.\nIt is useful when you want to iterate over a list.\nExample: Fitting a linear model to each species in the penguins dataset.\n\n\npenguins |&gt; \n  nest(data = -species) |&gt; \n  mutate(lm = map(data, ~lm(body_mass_g ~ flipper_length_mm, data = .x)))\n#&gt; # A tibble: 3 √ó 3\n#&gt;   species   data               lm    \n#&gt;   &lt;fct&gt;     &lt;list&gt;             &lt;list&gt;\n#&gt; 1 Adelie    &lt;tibble [152 √ó 7]&gt; &lt;lm&gt;  \n#&gt; 2 Gentoo    &lt;tibble [124 √ó 7]&gt; &lt;lm&gt;  \n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;  &lt;lm&gt;"
  },
  {
    "objectID": "slides/week-1.html#map_",
    "href": "slides/week-1.html#map_",
    "title": "Week 1",
    "section": "map_* ",
    "text": "map_* \n\nThe map_*() functions are used to extract specific outputs from a list.\nThey are useful when you want to extract specific outputs from a list.\nExample: Extracting the R-squared values (doubles) from the linear models.\n\n\npenguins |&gt; \n  nest(data = -species) |&gt; \n  mutate(lm = map(data, ~lm(body_mass_g ~ flipper_length_mm, data = .x)),\n         r2 = map_dbl(lm, ~summary(.x)$r.squared))\n#&gt; # A tibble: 3 √ó 4\n#&gt;   species   data               lm        r2\n#&gt;   &lt;fct&gt;     &lt;list&gt;             &lt;list&gt; &lt;dbl&gt;\n#&gt; 1 Adelie    &lt;tibble [152 √ó 7]&gt; &lt;lm&gt;   0.219\n#&gt; 2 Gentoo    &lt;tibble [124 √ó 7]&gt; &lt;lm&gt;   0.494\n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;  &lt;lm&gt;   0.412"
  },
  {
    "objectID": "slides/week-1.html#map2",
    "href": "slides/week-1.html#map2",
    "title": "Week 1",
    "section": "map2 ",
    "text": "map2 \n\nThe map2() function is used to iterate over two lists in parallel.\nIt is useful when you want to apply a function to two lists simultaneously.\nExample: Augmenting the linear models with the original data.\n\n\npenguins |&gt; \n  drop_na() |&gt; \n  nest(data = -species) |&gt; \n  mutate(lm_mod = map(data, ~lm(body_mass_g ~ flipper_length_mm, data = .x)),\n         r2 = map_dbl(lm_mod, ~summary(.x)$r.squared),\n         a  = map2(lm_mod, data, ~broom::augment(.x, .y))) \n#&gt; # A tibble: 3 √ó 5\n#&gt;   species   data               lm_mod    r2 a                  \n#&gt;   &lt;fct&gt;     &lt;list&gt;             &lt;list&gt; &lt;dbl&gt; &lt;list&gt;             \n#&gt; 1 Adelie    &lt;tibble [146 √ó 7]&gt; &lt;lm&gt;   0.216 &lt;tibble [146 √ó 13]&gt;\n#&gt; 2 Gentoo    &lt;tibble [119 √ó 7]&gt; &lt;lm&gt;   0.506 &lt;tibble [119 √ó 13]&gt;\n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;  &lt;lm&gt;   0.412 &lt;tibble [68 √ó 13]&gt;"
  },
  {
    "objectID": "slides/week-1.html#geometry-list-columns",
    "href": "slides/week-1.html#geometry-list-columns",
    "title": "Week 1",
    "section": "Geometry list columns ",
    "text": "Geometry list columns \n\nThe geometry column contains the spatial information.\nIt is stored as a list-column of sfc objects.\nExample: Accessing the first geometry in the rivers dataset.\n\n\nrivers$geometry[1]\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 144.8258 ymin: 61.40833 xmax: 160.7636 ymax: 68.8008\n#&gt; Geodetic CRS:  WGS 84"
  },
  {
    "objectID": "slides/week-1.html#projections",
    "href": "slides/week-1.html#projections",
    "title": "Week 1",
    "section": "Projections ",
    "text": "Projections \n\nCRS (Coordinate Reference System) is used to define the spatial reference.\nThe st_crs() function is used to get the CRS of a dataset.\nThe st_transform() function is used to transform the CRS of a dataset.\nExample: Transforming the rivers dataset to EPSG:5070.\n\n\nst_crs(rivers) |&gt; sf::st_is_longlat()\n#&gt; [1] TRUE\nst_crs(rivers)$units\n#&gt; NULL\n\nriv_5070  &lt;- st_transform(rivers, 5070)\n\nst_crs(riv_5070) |&gt; sf::st_is_longlat()\n#&gt; [1] FALSE\n\nst_crs(riv_5070)$units\n#&gt; [1] \"m\""
  },
  {
    "objectID": "slides/week-1.html#data-aesthetics",
    "href": "slides/week-1.html#data-aesthetics",
    "title": "Week 1",
    "section": "data / aesthetics ",
    "text": "data / aesthetics \n\nData must be provided to ggplot()\nThe aes() function is used to map variables to aesthetics (e.g., x and y axes).\naes arguments provided in ggplot are inherited by all layers.\nExample: Creating a plot of body_mass_g vs.¬†bill_length_mm.\n\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm))"
  },
  {
    "objectID": "slides/week-1.html#facet_wrap-facet_grid",
    "href": "slides/week-1.html#facet_wrap-facet_grid",
    "title": "Week 1",
    "section": "facet_wrap / facet_grid ",
    "text": "facet_wrap / facet_grid \n\nThe facet_wrap() function is used to create small multiples of a plot.\nIt is useful when you want to compare subsets of data.\nThe facet_grid() function is used to create a grid of plots.\nExample: Faceting the plot by species.\n\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() + \n  facet_wrap(~species)"
  },
  {
    "objectID": "slides/week-1.html#theme_",
    "href": "slides/week-1.html#theme_",
    "title": "Week 1",
    "section": "theme_* ",
    "text": "theme_* \n\nThe theme_*() functions are used to customize the appearance of the plot.\nThey allow you to modify the plot‚Äôs background, gridlines, and text.\nExample: Applying the theme_linedraw() theme to the plot.\n\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() + \n  facet_wrap(~species) + \n  theme_linedraw()"
  },
  {
    "objectID": "slides/week-1.html#pivot_longer",
    "href": "slides/week-1.html#pivot_longer",
    "title": "Week 1",
    "section": "pivot_longer ",
    "text": "pivot_longer \n\nThe pivot_longer() function is used to convert wide data to long data.\nIt is useful when you want to work with data in a tidy format.\nExample: Converting the penguins dataset from wide to long format.\n\n\n(data.long = penguins |&gt; \n  select(species, bill_length_mm, body_mass_g) |&gt; \n  mutate(penguin_id = 1:n()) |&gt; \n  pivot_longer(-c(penguin_id, species), \n               names_to = \"Measure\", \n               values_to = \"value\"))\n#&gt; # A tibble: 688 √ó 4\n#&gt;    species penguin_id Measure         value\n#&gt;    &lt;fct&gt;        &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 Adelie           1 bill_length_mm   39.1\n#&gt;  2 Adelie           1 body_mass_g    3750  \n#&gt;  3 Adelie           2 bill_length_mm   39.5\n#&gt;  4 Adelie           2 body_mass_g    3800  \n#&gt;  5 Adelie           3 bill_length_mm   40.3\n#&gt;  6 Adelie           3 body_mass_g    3250  \n#&gt;  7 Adelie           4 bill_length_mm   NA  \n#&gt;  8 Adelie           4 body_mass_g      NA  \n#&gt;  9 Adelie           5 bill_length_mm   36.7\n#&gt; 10 Adelie           5 body_mass_g    3450  \n#&gt; # ‚Ñπ 678 more rows"
  },
  {
    "objectID": "slides/week-1.html#pivot_wider",
    "href": "slides/week-1.html#pivot_wider",
    "title": "Week 1",
    "section": "pivot_wider ",
    "text": "pivot_wider \n\nThe pivot_wider() function is used to convert long data to wide data.\nIt is useful when you want to work with data in a wide format.\nExample: Converting the data.long dataset from long to wide format.\n\n\ndata.long |&gt; \n  pivot_wider(names_from = \"Measure\", \n              values_from = \"value\")\n#&gt; # A tibble: 344 √ó 4\n#&gt;    species penguin_id bill_length_mm body_mass_g\n#&gt;    &lt;fct&gt;        &lt;int&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 Adelie           1           39.1        3750\n#&gt;  2 Adelie           2           39.5        3800\n#&gt;  3 Adelie           3           40.3        3250\n#&gt;  4 Adelie           4           NA            NA\n#&gt;  5 Adelie           5           36.7        3450\n#&gt;  6 Adelie           6           39.3        3650\n#&gt;  7 Adelie           7           38.9        3625\n#&gt;  8 Adelie           8           39.2        4675\n#&gt;  9 Adelie           9           34.1        3475\n#&gt; 10 Adelie          10           42          4250\n#&gt; # ‚Ñπ 334 more rows"
  },
  {
    "objectID": "slides/week-1.html#nest-unnest",
    "href": "slides/week-1.html#nest-unnest",
    "title": "Week 1",
    "section": "nest / unnest ",
    "text": "nest / unnest \n\nThe nest() function is used to nest data into a list-column.\nIt is useful when you want to group data together.\nExample: Nesting the penguins dataset by species.\n\n\npenguins |&gt; \n  nest(data = -species)\n#&gt; # A tibble: 3 √ó 2\n#&gt;   species   data              \n#&gt;   &lt;fct&gt;     &lt;list&gt;            \n#&gt; 1 Adelie    &lt;tibble [152 √ó 7]&gt;\n#&gt; 2 Gentoo    &lt;tibble [124 √ó 7]&gt;\n#&gt; 3 Chinstrap &lt;tibble [68 √ó 7]&gt;\n\npenguins |&gt; \n  nest(data = -species) |&gt; \n  unnest(data)\n#&gt; # A tibble: 344 √ó 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ‚Ñπ 334 more rows\n#&gt; # ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "slides/week-1.html#from-package",
    "href": "slides/week-1.html#from-package",
    "title": "Week 1",
    "section": "From package ",
    "text": "From package \n\n# via packages\n(counties &lt;- AOI::aoi_get(state = \"conus\", county = \"all\"))\n#&gt; Simple feature collection with 3108 features and 14 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_region state_division feature_code state_name state_abbr     name\n#&gt; 1             3              6      0161526    Alabama         AL  Autauga\n#&gt; 2             3              6      0161527    Alabama         AL  Baldwin\n#&gt; 3             3              6      0161528    Alabama         AL  Barbour\n#&gt; 4             3              6      0161529    Alabama         AL     Bibb\n#&gt; 5             3              6      0161530    Alabama         AL   Blount\n#&gt; 6             3              6      0161531    Alabama         AL  Bullock\n#&gt; 7             3              6      0161532    Alabama         AL   Butler\n#&gt; 8             3              6      0161533    Alabama         AL  Calhoun\n#&gt; 9             3              6      0161534    Alabama         AL Chambers\n#&gt; 10            3              6      0161535    Alabama         AL Cherokee\n#&gt;    fip_class tiger_class combined_area_code metropolitan_area_code\n#&gt; 1         H1       G4020                388                   &lt;NA&gt;\n#&gt; 2         H1       G4020                380                   &lt;NA&gt;\n#&gt; 3         H1       G4020                 NA                   &lt;NA&gt;\n#&gt; 4         H1       G4020                142                   &lt;NA&gt;\n#&gt; 5         H1       G4020                142                   &lt;NA&gt;\n#&gt; 6         H1       G4020                 NA                   &lt;NA&gt;\n#&gt; 7         H1       G4020                 NA                   &lt;NA&gt;\n#&gt; 8         H1       G4020                 NA                   &lt;NA&gt;\n#&gt; 9         H1       G4020                122                   &lt;NA&gt;\n#&gt; 10        H1       G4020                 NA                   &lt;NA&gt;\n#&gt;    functional_status  land_area water_area fip_code\n#&gt; 1                  A 1539634184   25674812    01001\n#&gt; 2                  A 4117656514 1132955729    01003\n#&gt; 3                  A 2292160149   50523213    01005\n#&gt; 4                  A 1612188717    9572303    01007\n#&gt; 5                  A 1670259090   14860281    01009\n#&gt; 6                  A 1613083467    6030667    01011\n#&gt; 7                  A 2012002546    2701199    01013\n#&gt; 8                  A 1569246126   16536293    01015\n#&gt; 9                  A 1545085601   16971700    01017\n#&gt; 10                 A 1433620850  120310807    01019\n#&gt;                          geometry\n#&gt; 1  MULTIPOLYGON (((-86.81491 3...\n#&gt; 2  MULTIPOLYGON (((-87.59883 3...\n#&gt; 3  MULTIPOLYGON (((-85.41644 3...\n#&gt; 4  MULTIPOLYGON (((-87.01916 3...\n#&gt; 5  MULTIPOLYGON (((-86.5778 33...\n#&gt; 6  MULTIPOLYGON (((-85.65767 3...\n#&gt; 7  MULTIPOLYGON (((-86.4482 31...\n#&gt; 8  MULTIPOLYGON (((-85.79605 3...\n#&gt; 9  MULTIPOLYGON (((-85.59315 3...\n#&gt; 10 MULTIPOLYGON (((-85.51361 3..."
  },
  {
    "objectID": "slides/week-1.html#from-file",
    "href": "slides/week-1.html#from-file",
    "title": "Week 1",
    "section": "From file ",
    "text": "From file \n\n(rivers &lt;- sf::read_sf('data/majorrivers_0_0/MajorRivers.shp'))\n#&gt; Simple feature collection with 98 features and 4 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -164.8874 ymin: -36.96945 xmax: 160.7636 ymax: 71.39249\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 98 √ó 5\n#&gt;    NAME          SYSTEM MILES KILOMETERS                                geometry\n#&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;                   &lt;MULTILINESTRING [¬∞]&gt;\n#&gt;  1 Kolyma        &lt;NA&gt;   2552.      4106. ((144.8419 61.75915, 144.8258 61.8036,‚Ä¶\n#&gt;  2 Parana        Parana 1616.      2601. ((-51.0064 -20.07941, -51.02972 -20.22‚Ä¶\n#&gt;  3 San Francisco &lt;NA&gt;   1494.      2404. ((-46.43639 -20.25807, -46.49835 -20.2‚Ä¶\n#&gt;  4 Japura        Amazon 1223.      1968. ((-76.71056 1.624166, -76.70029 1.6883‚Ä¶\n#&gt;  5 Putumayo      Amazon  890.      1432. ((-76.86806 1.300553, -76.86695 1.295,‚Ä¶\n#&gt;  6 Rio Maranon   Amazon  889.      1431. ((-73.5079 -4.459834, -73.79197 -4.621‚Ä¶\n#&gt;  7 Ucayali       Amazon 1298.      2089. ((-73.5079 -4.459834, -73.51585 -4.506‚Ä¶\n#&gt;  8 Guapore       Amazon  394.       634. ((-65.39585 -10.39333, -65.39578 -10.3‚Ä¶\n#&gt;  9 Madre de Dios Amazon  568.       914. ((-65.39585 -10.39333, -65.45279 -10.4‚Ä¶\n#&gt; 10 Amazon        Amazon 1890.      3042. ((-73.5079 -4.459834, -73.45141 -4.427‚Ä¶\n#&gt; # ‚Ñπ 88 more rows"
  },
  {
    "objectID": "slides/week-1.html#via-url",
    "href": "slides/week-1.html#via-url",
    "title": "Week 1",
    "section": "via url ",
    "text": "via url \n\n# via url\n(gage &lt;- sf::read_sf(\"https://reference.geoconnex.us/collections/gages/items/1000001\"))\n#&gt; Simple feature collection with 1 feature and 17 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -107.2826 ymin: 35.94568 xmax: -107.2826 ymax: 35.94568\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 1 √ó 18\n#&gt;   nhdpv2_reachcode mainstem_uri           fid nhdpv2_reach_measure cluster uri  \n#&gt;   &lt;chr&gt;            &lt;chr&gt;                &lt;int&gt;                &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;\n#&gt; 1 13020205000216   https://geoconnex.u‚Ä¶     1                 80.3 https:‚Ä¶ http‚Ä¶\n#&gt; # ‚Ñπ 12 more variables: nhdpv2_comid &lt;dbl&gt;, name &lt;chr&gt;, nhdpv2_totdasqkm &lt;dbl&gt;,\n#&gt; #   description &lt;chr&gt;, nhdpv2_link_source &lt;chr&gt;, subjectof &lt;chr&gt;,\n#&gt; #   nhdpv2_offset_m &lt;dbl&gt;, provider &lt;chr&gt;, gage_totdasqkm &lt;dbl&gt;,\n#&gt; #   provider_id &lt;chr&gt;, dasqkm_diff &lt;dbl&gt;, geometry &lt;POINT [¬∞]&gt;\n\n# write out data\n# write_sf(counties, \"data/counties.shp\")"
  },
  {
    "objectID": "slides/week-1.html#additonal-structure",
    "href": "slides/week-1.html#additonal-structure",
    "title": "Week 1",
    "section": "Additonal Structure",
    "text": "Additonal Structure\nIn addition to the values and diminsions, rasters have: - Extent: The spatial extent of the raster. - Resolution: The spatial resolution of the raster pixels. - CRS: The coordinate reference system of the raster.\n\ncrs(elev)\n#&gt; [1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\next(elev)\n#&gt; SpatExtent : -1146465, -504615, 1566915, 2073705 (xmin, xmax, ymin, ymax)\nres(elev)\n#&gt; [1] 30 30"
  },
  {
    "objectID": "slides/week-1.html#raster-structure-1",
    "href": "slides/week-1.html#raster-structure-1",
    "title": "Week 1",
    "section": "Raster Structure ",
    "text": "Raster Structure \n\nRasters are matrices or arrays of values, and can be manipulated as such\nFor example, setting 35% of the raster to NA\n\n\nlarimer_elev[sample(ncell(larimer_elev), .35*ncell(larimer_elev))] &lt;-  NA\n\nplot(larimer_elev)"
  },
  {
    "objectID": "slides/week-1.html#value-supersetting",
    "href": "slides/week-1.html#value-supersetting",
    "title": "Week 1",
    "section": "Value Supersetting ",
    "text": "Value Supersetting \n\nRasters are matrices or arrays of values, and can be manipulated as such\nFor example, setting 35% of the raster to NA\n\n\nlarimer_elev[sample(ncell(larimer_elev), .35*ncell(larimer_elev))] &lt;-  NA\n\nplot(larimer_elev)"
  },
  {
    "objectID": "slides/week-1.html#autoplot-rank_results",
    "href": "slides/week-1.html#autoplot-rank_results",
    "title": "Week 1",
    "section": "autoplot / rank_results  ",
    "text": "autoplot / rank_results  \n\nThe autoplot() function is used to visualize model performance.\nThe rank_results() function is used to rank models based on a metric.\nExample: Visualizing and ranking the model results based on the roc_auc (area under the curve) metric.\n\n\n\n\nautoplot(workflowset)\n\n\n\n\n\n\n\n\n\n\nrank_results(workflowset, rank_metric = \"roc_auc\")\n#&gt; # A tibble: 6 √ó 9\n#&gt;   wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 recipe_multinom_‚Ä¶ Prepro‚Ä¶ accura‚Ä¶ 1     0           5 recipe       mult‚Ä¶     1\n#&gt; 2 recipe_multinom_‚Ä¶ Prepro‚Ä¶ roc_auc 1     0           5 recipe       mult‚Ä¶     1\n#&gt; 3 recipe_rand_fore‚Ä¶ Prepro‚Ä¶ accura‚Ä¶ 0.981 5.97e-3     5 recipe       rand‚Ä¶     2\n#&gt; 4 recipe_rand_fore‚Ä¶ Prepro‚Ä¶ roc_auc 1.00  3.60e-4     5 recipe       rand‚Ä¶     2\n#&gt; 5 recipe_decision_‚Ä¶ Prepro‚Ä¶ accura‚Ä¶ 0.955 1.28e-2     5 recipe       deci‚Ä¶     3\n#&gt; 6 recipe_decision_‚Ä¶ Prepro‚Ä¶ roc_auc 0.953 1.39e-2     5 recipe       deci‚Ä¶     3"
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Structure",
    "text": "Structure\nIn general ‚Ä¶\n\nMondays will be a lecture,with a mix of slides and discussion.\nWednesdays will be a lab with a introductory ~30 min technical demo, followed by a hands-on lab due the following week.\nGroup work is encouraged, but all assignments should be submitted individually."
  },
  {
    "objectID": "index.html#grades",
    "href": "index.html#grades",
    "title": "Ecosystem Science and Sustainability 523c",
    "section": "Grades",
    "text": "Grades\n\n6 labs will be worth 150 points each.\nThey will be assigned on Wednesdays and due the following Wednesdays before class.\nA final project will be optional and worth 150 extra credit points. It will build on your personal website built in ESS 523a.\n\nThe total points possible is 1050, with the percentage being taken out of 900 using the traditional 90/80/70/60 scales"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1: Data Science Tools",
    "section": "",
    "text": "In this lab you will practice data wrangling and visualization skills using COVID-19 data curated by the New York Times. This data is a large dataset measuring the cases and deaths per US county across the lifespan of COVID from its early beginnings to just past the peak. The data stored in daily cummulative counts, is a great example of data that needs to be wrangled and cleaned before any analysis can be done."
  },
  {
    "objectID": "labs/lab1.html#libraries",
    "href": "labs/lab1.html#libraries",
    "title": "Lab 1: Data Science Tools",
    "section": "Libraries",
    "text": "Libraries\nYou will need a few libraries for this lab. Make sure they are installed and loaded in your Qmd:\n\ntidyverse (data wrangling and visualization)\nflextable (make nice tables)\nzoo (rolling averages)"
  },
  {
    "objectID": "labs/lab1.html#data",
    "href": "labs/lab1.html#data",
    "title": "Lab 1: Data Science Tools",
    "section": "Data",
    "text": "Data\nWe are going to practice some data wrangling skills using a real-world dataset about COVID cases curated and maintained by the New York Times. The data was used in the peak of the pandemic to create reports and data visualizations like this, and are archived on a GitHub repo here. A history of the importance can be found here.\nLets pretend it in Feb 1st, 2022. You are a data scientist for the state of Colorado Department of Public Health (this is actually a task I did in California!). You‚Äôve been tasked with giving a report to Governor Polis each morning about the most current COVID-19 conditions at the county level.\nAs it stands, the Colorado Department of Public Health maintains a watch list of counties that are being monitored for worsening corona virus trends. There are six criteria used to place counties on the watch list:\n\nDoing fewer than 150 tests per 100,000 residents daily (over a 7-day average)\nMore than 100 new cases per 100,000 residents over the past 14 days‚Ä¶\n25 new cases per 100,000 residents and an 8% test positivity rate\n10% or greater increase in COVID-19 hospitalized patients over the past 3 days\nFewer than 20% of ICU beds available\nFewer than 25% ventilators available\n\nOf these 6 conditions, you are in charge of monitoring condition number 2."
  },
  {
    "objectID": "labs/lab1.html#steps",
    "href": "labs/lab1.html#steps",
    "title": "Lab 1: Data Science Tools",
    "section": "Steps:",
    "text": "Steps:\n\nStart by reading in the data from the NY-Times URL with read_csv (make sure to attach the tidyverse). The data read from Github is considered our ‚Äúraw data‚Äù. Remember to always leave ‚Äúraw-data-raw‚Äù and to generate meaningful subsets as you go.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(flextable)\ndata &lt;- read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')\n\n\n\nCreate an object called my.date and set it as ‚Äú2022-02-01‚Äù - ensure this is a date object.\nCreate a object called my.state and set it to ‚ÄúColorado‚Äù.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIn R, as.Date() is a function used to convert character strings, numeric values, or other date-related objects into Date objects. It ensures that dates are stored in the correct format for date-based calculations and manipulations.\n\n\nCode\ntxt &lt;- \"2025-02-15\"\nclass(txt)\n\n\n[1] \"character\"\n\n\nCode\ndate_example &lt;- as.Date(txt)\nclass(date_example)\n\n\n[1] \"Date\"\n\n\n\n\n\n\n\nCode\nmy.date  &lt;- as.Date(\"2022-02-01\")\nmy.state &lt;- \"Colorado\"\n\n\n\nStart by making a subset that limits the data to Colorado (filter), and add a new column (mutate) with the daily new cases using diff/lag by county (group_by). Do the same for new deaths. If lag is new to you, lag is a function that shifts a vector by a specified number of positions. The help file can be found with ?lag.\n\n(Hint: you will need some combination of filter, group_by, arrange, mutate, diff/lag, and ungroup)\n\nUsing your subset, generate (2) tables. The first should show the 5 counties with the most CUMULATIVE cases on you date of interest, and the second should show the 5 counties with the most NEW cases on that same date. Remember to use your my.date object as a proxy for today‚Äôs date:\n\nYour tables should have clear column names and descriptive captions.\n(Hint: Use flextable::flextable() and flextable::set_caption())"
  },
  {
    "objectID": "labs/lab1.html#steps-1",
    "href": "labs/lab1.html#steps-1",
    "title": "Lab 1: Data Science Tools",
    "section": "Steps:",
    "text": "Steps:\n\nGiven the above URL, and guidelines on string concatenation, read in the population data and (1) create a five digit FIP variable and only keep columns that contain ‚ÄúNAME‚Äù or ‚Äú2021‚Äù (remember the tidyselect option found with ?dplyr::select). Additionally, remove all state level rows (e.g.¬†COUNTY FIP == ‚Äú000‚Äù)\n\n\nNow, explore the data ‚Ä¶ what attributes does it have, what are the names of the columns? Do any match the COVID data we have? What are the dimensions‚Ä¶ In a few sentences describe the data obtained after modification:\n\n(Hint: names(), dim(), nrow(), str(), glimpse(), skimr,‚Ä¶))"
  },
  {
    "objectID": "labs/lab1.html#steps-2",
    "href": "labs/lab1.html#steps-2",
    "title": "Lab 1: Data Science Tools",
    "section": "Steps:",
    "text": "Steps:\n\nFirst, we need to group/summarize our county level data to the state level, filter it to the four states of interest, and calculate the number of daily new cases (diff/lag) and the 7-day rolling mean.\n\n\n\n\n\n\n\nRolling Averages\n\n\n\n\n\nThe rollmean function from the zoo package in R is used to compute the rolling (moving) mean of a numeric vector, matrix, or zoo/ts object.\nrollmean(x, k, fill = NA, align = \"center\", na.pad = FALSE)\n- x: Numeric vector, matrix, or time series.\n- k: Window size (number of observations).\n- fill: Values to pad missing results (default NA).\n- align: Position of the rolling window (‚Äúcenter‚Äù, ‚Äúleft‚Äù, ‚Äúright‚Äù).\n- na.pad: If TRUE, pads missing values with NA.\n\n\nExamples\n\nRolling Mean on a Numeric Vector Since align = \"center\" by default, values at the start and end are dropped.\n\n\n\nCode\nlibrary(zoo)\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Rolling mean with a window size of 3\nrollmean(x, k = 3)\n\n\n[1] 2 3 4 5 6 7 8 9\n\n\n\nRolling Mean with Padding Missing values are filled at the start and end.\n\n\n\nCode\nrollmean(x, k = 3, fill = NA)\n\n\n [1] NA  2  3  4  5  6  7  8  9 NA\n\n\n\nAligning Left or Right The rolling mean is calculated with values aligned to the left or right\n\n\n\nCode\nrollmean(x, k = 3, fill = NA, align = \"left\")\n\n\n [1]  2  3  4  5  6  7  8  9 NA NA\n\n\nCode\nrollmean(x, k = 3, fill = NA, align = \"right\")\n\n\n [1] NA NA  2  3  4  5  6  7  8  9\n\n\n\n\n\n\nHint: You will need two group_by calls and the zoo::rollmean function.\n\nUsing the modified data, make a facet plot of the daily new cases and the 7-day rolling mean. Your plot should use compelling geoms, labels, colors, and themes.\n\n\nThe story of raw case counts can be misleading. To understand why, lets explore the cases per capita of each state. To do this, join the state COVID data to the population estimates and calculate the \\(new cases / total population\\). Additionally, calculate the 7-day rolling mean of the new cases per capita counts. This is a tricky task and will take some thought, time, and modification to existing code (most likely)!\n\nHint: You may need to modify the columns you kept in your original population data. Be creative with how you join data (inner vs outer vs full)!\n\nUsing the per capita data, plot the 7-day rolling averages overlying each other (one plot) with compelling labels, colors, and theme.\n\n\nBriefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?\n\n\n‚Ä¶"
  },
  {
    "objectID": "labs/lab1.html#data-preparation",
    "href": "labs/lab1.html#data-preparation",
    "title": "Lab 1: Data Science Tools",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLet‚Äôs start with the raw COVID dataset, and compute county level daily new cases and deaths (lag). Then, join it to the census data in order to use population data in the model.\nWe are aware there was a strong seasonal component to the spread of COVID-19. To account for this, lets add a new column to the data for year (lubridate::year()), month (lubridate::month()), and season (dplyr::case_when()) which will be one of four values: ‚ÄúSpring‚Äù (Mar-May), ‚ÄúSummer‚Äù (Jun-Aug), ‚ÄúFall‚Äù (Sep-Nov), or ‚ÄúWinter‚Äù (Dec - Jan) based on the computed Month.\nNext, lets group the data by state, year, and season and summarize the total population, new cases, and new deaths per grouping.\nGiven the case/death counts are not scaled by population, we expect that each will exhibit a right skew behavior (you can confirm this with density plots, shapiro.test, or histrograms). Given an assumption of linear models is normality in the data, let‚Äôs apply a log transformation to cases, deaths, and population to normalize them.\n\n\n\n\n\n\n\nNote\n\n\n\nWe know there are 0‚Äôs in the data (cases/deaths), so we can add 1 to the data before taking the log. As the log of 0 is undefined, adding 1 ensures that the log of 0 is -Inf.\n\n\nCode\nlog(0)\n\n\n[1] -Inf"
  },
  {
    "objectID": "labs/lab1.html#model-building",
    "href": "labs/lab1.html#model-building",
    "title": "Lab 1: Data Science Tools",
    "section": "Model Building",
    "text": "Model Building\n\nOnce the data has been prepared, build a linear model (lm) to predict the log of cases using the log of deaths the log of population, and the season. Be sure to add an interaction term for population and deaths since they per capita realtionship is significant!\nOnce the model is built, summarize it (summary) and report the R-squared value and the p-value of the model. What does this mean for the value of its application?"
  },
  {
    "objectID": "slides/week-1.html",
    "href": "slides/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Welcome to 523C: Environmental Data Science Applications: Water Resources!\nThis first lecture will introduce essential, high-level topics to help you build a strong foundation in R for environmental data science.\nThroughout the lecture, you will be asked to assess your comfort level with various topics via a Google survey.\nThe survey results will help tailor the course focus, ensuring that we reinforce challenging concepts while avoiding unnecessary review of familiar topics."
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2: Distances and Projections",
    "section": "",
    "text": "In this lab we will explore the properties of sf, sfc, and sfg features & objects; how they are stored; and issues related to distance calculation and coordinate transformation.\nWe will continue to build on our data wrangling and data visualization skills; as well as document preparation via Quarto and GitHub.\n\n\nSet-up\n\nNavigage to your csu-523c repository\nCreate a new Quarto (.qmd) file called lab-02.qmd\nPopulate its YML with a title, author, subtitle, output type and theme. For example:\n\n\n---\ntitle: \"Lab 02: Distances and the Border Zone\"\nsubtitle: 'Ecosystem Science and Sustainability 523c'\nauthor:\n  - name: ...\n    email: ...\nformat: html\n---\n\n\n\n\nLibraries\n\n# spatial data science\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(units)\n\n# Data\nlibrary(USAboundaries)\nlibrary(rnaturalearth)\n\n# Visualization\nlibrary(gghighlight)\nlibrary(ggrepel)\nlibrary(knitr)\n\n\n\n\nBackground\nIn this lab, 4 main skills are covered:\n\nIngesting / building sf objects from R packages and CSVs. (Q1)\nManipulating geometries and coordinate systems (Q2)\nCalculating distances (Q2)\nBuilding maps using ggplot (Q3)\n\nHints and Tricks for this lab are available here\n\n\n\nQuestion 1:\nFor this lab we need three (3) datasets.\n\nSpatial boundaries of continental USA states (1.1)\nBoundaries of Canada, Mexico and the United States (1.2)\nAll USA cites (1.3)\n\n\n1.1 Define a Projection\nFor this lab we want to calculate distances between features, therefore we need a projection that preserves distance at the scale of CONUS. For this, we will use the North America Equidistant Conic:\n\neqdc &lt;- '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\nThis PROJ.4 string defines an Equidistant Conic projection with the following parameters:\n\n+proj=eqdc ‚Üí Equidistant Conic projection\n+lat_0=40 ‚Üí Latitude of the projection‚Äôs center (40¬∞N)\n+lon_0=-96 ‚Üí Central meridian (96¬∞W)\n+lat_1=20 ‚Üí First standard parallel (20¬∞N)\n+lat_2=60 ‚Üí Second standard parallel (60¬∞N)\n+x_0=0 ‚Üí False easting (0 meters)\n+y_0=0 ‚Üí False northing (0 meters)\n+datum=NAD83 ‚Üí Uses the North American Datum 1983 (NAD83)\n+units=m ‚Üí Units are in meters\n+no_defs ‚Üí No additional default parameters from PROJ‚Äôs database\n\nThis projection is commonly used for mapping large areas with an east-west extent, especially in North America, as it balances distortion well between the two standard parallels.\n\n\n1.2 - Get USA state boundaries\nIn R, USA boundaries are stored in the USAboundaries package. In case this package and data are not installed:\n\nremotes::install_github(\"ropensci/USAboundaries\")\nremotes::install_github(\"ropensci/USAboundariesData\")\n\nOnce installed:\n\nUSA state boundaries can be accessed with USAboundaries::us_states(resolution = \"low\"). Given the precision needed for this analysis we are ok with the low resolution.\nMake sure you only have the states in the continental United States (CONUS) (Hint use filter)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\n\n\n1.3 - Get country boundaries for Mexico, the United States of America, and Canada\nIn R, country boundaries are stored in the rnaturalearth package. In case this package is not installed:\n\nremotes::install_github(\"ropenscilabs/rnaturalearthdata\")\n\nOnce installed:\n\nWorld boundaries can be accessed with rnaturalearth::countries110.\nMake sure the data is in simple features (sf) format (Hint use the st_as_sf variable).\nMake sure you only have the countries you want (Hint filter on the admin variable)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\n\n\n1.4 - Get city locations from the CSV file\nThe process of finding, downloading and accessing data is the first step of every analysis. Here we will go through these steps (minus finding the data).\nFirst go to this site and download the appropriate (free) dataset into the data directory of this project.\nOnce downloaded, read it into your working session using readr::read_csv() and explore the dataset until you are comfortable with the information it contains.\nWhile this data has everything we want, it is not yet spatial. Convert the data.frame to a spatial object using st_as_sf and prescribing the coordinate variables and CRS (Hint what projection are the raw coordinates in?)\nFinally, remove cities in states not wanted and make sure the data is in a projected coordinate system suitable for distance measurements at the national scale:\nCongratulations! You now have three real-world, large datasets ready for analysis.\n\n\n\nQuestion 2:\nHere we will focus on calculating the distance of each USA city to (1) the national border (2) the nearest state border (3) the Mexican border and (4) the Canadian border. You will need to manipulate you existing spatial geometries to do this using either st_union or st_combine depending on the situation. In all cases, since we are after distances to borders, we will need to cast (st_cast) our MULTIPPOLYGON geometries to MULTILINESTRING geometries. To perform these distance calculations we will use st_distance().\n\n2.1 - Distance to USA Border (coastline or national) (km)\nFor 2.2 we are interested in calculating the distance of each USA city to the USA border (coastline or national border). To do this we need all states to act as single unit. Convert the USA state boundaries to a MULTILINESTRING geometry in which the state boundaries are resolved. Please do this starting with the states object and NOT with a filtered country object. In addition to storing this distance data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\n2.2 - Distance to States (km)\nFor 2.1 we are interested in calculating the distance of each city to the nearest state boundary. To do this we need all states to act as single unit. Convert the USA state boundaries to a MULTILINESTRING geometry in which the state boundaries are preserved (not resolved). In addition to storing this distance data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\n2.3 - Distance to Mexico (km)\nFor 2.3 we are interested in calculating the distance of each city to the Mexican border. To do this we need to isolate Mexico from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\n2.4 - Distance to Canada (km)\nFor 2.4 we are interested in calculating the distance of each city to the Canadian border. To do this we need to isolate Canada from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\n\nQuestion 3:\nIn this section we will focus on visualizing the distance data you calculated above. You will be using ggplot to make your maps, ggrepl to label significant features, and gghighlight to emphasize important criteria.\n\n3.1 Data\nShow the 3 continents, CONUS outline, state boundaries, and 10 largest USA cities (by population) on a single map\n\nUse geom_sf to plot your layers\nUse lty to change the line type and size to change line width\nUse ggrepel::geom_label_repel to label your cities\n\n\n\n3.2 City Distance from the Border\nCreate a map that colors USA cities by their distance from the national border. In addition, re-draw and label the 5 cities that are farthest from the border.\n\n\n3.3 City Distance from Nearest State\nCreate a map that colors USA cities by their distance from the nearest state border. In addition, re-draw and label the 5 cities that are farthest from any border.\n\n\n3.4 Equidistance boundary from Mexico and Canada\nHere we provide a little more challenge. Use gghighlight to identify the cities that are equal distance from the Canadian AND Mexican border \\(\\pm\\) 100 km.\nIn addition, label the five (5) most populous cites in this zone.\nHint: (create a new variable that finds the absolute difference between the distance to Mexico and the distance to Canada)\n\n\n\nQuestion 4:\n\nReal World Application\nRecently, Federal Agencies have claimed basic constitutional rights protected by the Fourth Amendment (protecting Americans from random and arbitrary stops and searches) do not apply fully at our borders (see Portland). For example, federal authorities do not need a warrant or suspicion of wrongdoing to justify conducting what courts have called a ‚Äúroutine search,‚Äù such as searching luggage or a vehicle. Specifically, federal regulations give U.S. Customs and Border Protection (CBP) authority to operate within 100 miles of any U.S. ‚Äúexternal boundary‚Äù. Further information can be found at this ACLU article.\n\n\n4.1 Quantifing Border Zone\n\nHow many cities are in this 100 mile zone? (100 miles ~ 160 kilometers)\nHow many people live in a city within 100 miles of the border?\nWhat percentage of the total population is in this zone?\nDoes it match the ACLU estimate in the link above?\n\nReport this information as a table.\n\n\n4.2 Mapping Border Zone\n\nMake a map highlighting the cites within the 100 mile zone using gghighlight.\nUse a color gradient from ‚Äòorange‚Äô to ‚Äòdarkred‚Äô.\nLabel the 10 most populous cities in the Danger Zone\n\n\n\n4.3 : Instead of labeling the 10 most populous cites, label the most populous city in each state within the Danger Zone.\n\n\n\n\nRubric\n\nQuestion 1 (35)\nQuestion 2 (35)\nQuestion 3 (35)\nQuestion 4 (35)\nWell Structured and appealing Qmd deployed as web page (10)\n\nTotal: 150 points\n\n\nSubmission\nFor this lab you will submit a URL to a webpage deployed with GitHub pages.\nTo do this:\n\nRender your lab document\nStage/commit/push your files\nIf you followed the naming conventions in the ‚ÄúSet Up‚Äù, your lab 2 link will be available at:\n\nhttps://USERNAME.github.io/csu-523c/lab-02.html\nSubmit this URL in the appropriate Canvas dropbox. Also take a moment to update your personal webpage with this link and some bullet points of what you learned. While not graded as part of this lab, it will be your final!"
  },
  {
    "objectID": "slides/week-2.html",
    "href": "slides/week-2.html",
    "title": "week-2",
    "section": "",
    "text": "Projections & Measures\nLecture 10, Lecture 11, Lecture 13 (centroids/buffers forward)\n\n\nPredicates & Tesselations\nLecture 12, 15"
  },
  {
    "objectID": "labs/lab2-hints.html",
    "href": "labs/lab2-hints.html",
    "title": "Lab 2: Distances and Projections",
    "section": "",
    "text": "Question 1:\n\nMaking Spatial Objects & Coordinate Transformation\nSpatial objects (sf) can be built from a vector of X and Y values in addition to a coordinate reference system (CRS). For example:\n\ndf &lt;- data.frame(name = state.name, \n                X = state.center$x, \n                Y = state.center$y)\nhead(df)\n\n        name         X       Y\n1    Alabama  -86.7509 32.5901\n2     Alaska -127.2500 49.2500\n3    Arizona -111.6250 34.2192\n4   Arkansas  -92.2992 34.7336\n5 California -119.7730 36.5341\n6   Colorado -105.5130 38.6777\n\n# Geographic Coordinate System (GCS)\n(df_sf_gcs = st_as_sf(df, \n                      coords = c(\"X\", \"Y\"), \n                      crs = 4269))\n\nSimple feature collection with 50 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -127.25 ymin: 27.8744 xmax: -68.9801 ymax: 49.25\nGeodetic CRS:  NAD83\nFirst 10 features:\n          name                 geometry\n1      Alabama POINT (-86.7509 32.5901)\n2       Alaska    POINT (-127.25 49.25)\n3      Arizona POINT (-111.625 34.2192)\n4     Arkansas POINT (-92.2992 34.7336)\n5   California POINT (-119.773 36.5341)\n6     Colorado POINT (-105.513 38.6777)\n7  Connecticut POINT (-72.3573 41.5928)\n8     Delaware POINT (-74.9841 38.6777)\n9      Florida  POINT (-81.685 27.8744)\n10     Georgia POINT (-83.3736 32.3329)\n\nggplot() + \n  geom_sf(data = df_sf_gcs) + \n  coord_sf(datum = st_crs(df_sf_gcs)) +\n  theme_linedraw()\n\n\n\n\n\n\n\n# Projected Coordinate System (PCS)\n# st_transforms converts from one reference system to another\n(df_sf_pcs = st_transform(df_sf_gcs, 5070))\n\nSimple feature collection with 50 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2805703 ymin: 640477 xmax: 2079664 ymax: 3291437\nProjected CRS: NAD83 / Conus Albers\nFirst 10 features:\n          name                  geometry\n1      Alabama  POINT (862043.5 1099545)\n2       Alaska  POINT (-2264853 3291437)\n3      Arizona  POINT (-1422260 1356663)\n4     Arkansas  POINT (336061.5 1303543)\n5   California  POINT (-2086972 1760961)\n6     Colorado POINT (-818480.9 1779785)\n7  Connecticut   POINT (1936213 2307450)\n8     Delaware   POINT (1796466 1938236)\n9      Florida    POINT (1409814 640477)\n10     Georgia   POINT (1179012 1107322)\n\nggplot() + \n  geom_sf(data = df_sf_pcs) + \n  coord_sf(datum = st_crs(df_sf_pcs)) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2:\n\nst_distance review\n\n# Three most populous cities in the USA\n(big3 = cities |&gt; \n   select(city, population) |&gt; \n   slice_max(population, n = 3))\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2032604 ymin: 1468468 xmax: 1833394 ymax: 2178657\nProjected CRS: NAD83 / Conus Albers\n# A tibble: 3 √ó 3\n  city        population           geometry\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;POINT [m]&gt;\n1 New York      18832416  (1833394 2178657)\n2 Los Angeles   11885717 (-2032604 1468468)\n3 Chicago        8489066 (684628.5 2122697)\n\n# Fort Collins\n(foco = filter(cities, city == \"Fort Collins\") |&gt; \n    select(city, population))\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -760147.5 ymin: 1984621 xmax: -760147.5 ymax: 1984621\nProjected CRS: NAD83 / Conus Albers\n# A tibble: 1 √ó 3\n  city         population            geometry\n  &lt;chr&gt;             &lt;dbl&gt;         &lt;POINT [m]&gt;\n1 Fort Collins     339256 (-760147.5 1984621)\n\n# Distance from foco to population centers\nst_distance(big3, foco)\n\nUnits: [m]\n        [,1]\n[1,] 2600790\n[2,] 1373156\n[3,] 1451359\n\n\nThere are two notable things about this result:\n\nIt has units\nIt is returned as a matrix, even though foco only had one row\n\nThis second point highlights a useful feature of st_distance, namley, its ability to return distance matrices between all combinations of features in x and y.\n\n\nunits review\nWhile units are useful, they are not always the preferred units. By default, the units measurement is defined by the projection. For example:\n\nst_crs(big3)$units\n\n[1] \"m\"\n\n\nUnits can be converted using units::set_units. For example, ‚Äòm‚Äô can be converted to ‚Äòkm‚Äô:\n\nbig3 = mutate(big3, \n              dist_to_foco = st_distance(big3, foco),\n              dist_to_foco = set_units(dist_to_foco, \"km\")) \n\n(big3$dist_to_foco)\n\nUnits: [km]\n         [,1]\n[1,] 2600.790\n[2,] 1373.156\n[3,] 1451.359\n\n\nYou might have noticed the data type of the st_distance objects are an S3 class of units. Sometimes, this class can cause problems when trying to using it with other classes or methods:\n\nbig3$dist_to_foco + 4\n\nError in Ops.units(big3$dist_to_foco, 4): both operands of the expression should be \"units\" objects\n\nggplot(data = big3) + \n  geom_col(aes(x = city, y = dist_to_foco)) + \n  theme_linedraw()\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\n\n\n\n\n\n\n\nIn these cases, the units class can be dropped with units::drop_units\n\nbig3 = mutate(big3, \n              dist_to_foco = st_distance(big3, foco),\n              dist_to_foco = set_units(dist_to_foco, \"km\"),\n              dist_to_foco = drop_units(dist_to_foco))\n\nbig3$dist_to_foco + 4\n\n         [,1]\n[1,] 2604.790\n[2,] 1377.156\n[3,] 1455.359\n\nggplot(data = big3) + \n  geom_col(aes(x = reorder(city, -dist_to_foco), y = dist_to_foco)) + \n    labs(title = \"Distance to Fort Collins (km)\") + \n  ggthemes::theme_fivethirtyeight() + \n  theme( axis.text.x = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\nAs with all functions, these steps can be nested:\n\nbig3 = mutate(big3, \n              dist_to_foco = drop_units(set_units(st_distance(big3, foco), \"km\")))\n\n\n\n\nGeometry review\nThere are a few ways to manipulate existing geometries, here we discuss st_union, st_combine and st_cast\n\nst_combine returns a single, combined geometry, with no resolved boundaries.\nst_union() returns a single geometry with resolved boundaries\nst_cast() casts one geometry type to another\n\n\n(rockies = USAboundaries::us_states() |&gt; \n  filter(name %in% c('Montana', 'Wyoming', 'Colorado', \"New Mexico\")) |&gt; \n  select(name, geometry))\n\nSimple feature collection with 4 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.0492 ymin: 31.33232 xmax: -102.0419 ymax: 49.00139\nGeodetic CRS:  WGS 84\n        name                       geometry\n1    Montana MULTIPOLYGON (((-116.0492 4...\n2   Colorado MULTIPOLYGON (((-109.06 38....\n3    Wyoming MULTIPOLYGON (((-111.0569 4...\n4 New Mexico MULTIPOLYGON (((-109.0492 3...\n\nplot(rockies['name'], key.pos = 1)\n\n\n\n\n\n\n\n# Combine Geometries\n(combined_wc = st_combine(rockies))\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.0492 ymin: 31.33232 xmax: -102.0419 ymax: 49.00139\nGeodetic CRS:  WGS 84\n\n\nMULTIPOLYGON (((-116.0492 49.00091, -115.501 49...\n\nplot(combined_wc, col = \"red\")\n\n\n\n\n\n\n\n# Unioned Geometries\n(unioned_wc = st_union(rockies))\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -116.0492 ymin: 31.33232 xmax: -102.0419 ymax: 49.00139\nGeodetic CRS:  WGS 84\n\n\nPOLYGON ((-109.0601 38.27549, -109.0418 38.1646...\n\nplot(unioned_wc, col = \"red\")\n\n\n\n\n\n\n\n# Combine Geometries\nline_wc = st_cast(unioned_wc, \"MULTILINESTRING\")\nplot(line_wc, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3:\nIn this section you will extend your growing ggplot skills to handle spatial data using ggrepl to label significant features; gghighlight to emphasize important criteria; and scaled color/fill to create chloropleth represnetations of variables. Below is some example code to provide an example of these tools in action:\n\nGet some data (review)\n\n# Define a state/region classifier and select the southern states\nstate.of.interest &lt;- data.frame(state = state.name, region = state.region) |&gt; \n  filter(region == \"South\") |&gt; \n  pull(state)\n\n# Get USA states in the southern region and transform to EPSG:5070\nstate = USAboundaries::us_states() |&gt; \n  filter(name %in% state.of.interest) |&gt; \n  st_transform(5070)\n\n# Get USA congressional districts in the southern region and transform to EPSG:5070\ndistricts =  USAboundaries::us_congressional() |&gt; \n  filter(state_name %in% state.of.interest) |&gt; \n  st_transform(5070)\n\n# Get the 10 most populous cities in the southern region and transform to EPSG:5070\nsub_cities = cities |&gt; \n  filter(state_name %in% state.of.interest) |&gt; \n  slice_max(population, n = 10) |&gt; \n  st_transform(5070)\n\n\n\nMap\n\nggplot() + \n  # Add districts with a dashed line (lty = 3), \n  # a color gradient from blue to red based on aland, \n  # and a fill aplha of 0.5\n  geom_sf(data = districts, aes(fill = aland), lty = 3, alpha = .5) + \n  scale_fill_gradient(low = 'blue', high = \"red\") +\n  # Highlight (keep blue) only those districts witn a land area &gt; 5e10\n  gghighlight(aland &gt; 5e10) +\n  # Add the state borders with a thicker line and no fill\n  geom_sf(data = state, size = 1, fill = \"NA\") +\n  # Add the cities\n  geom_sf(data = sub_cities, size= 2, color = \"red\") +\n  # Add labels to the cities\n  ggrepel::geom_label_repel(\n    data = sub_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  labs(fill = \"Area Land\") + \n  ggthemes::theme_map()"
  },
  {
    "objectID": "slides/week-2.html#simple-features",
    "href": "slides/week-2.html#simple-features",
    "title": "Week 2",
    "section": "Simple Features",
    "text": "Simple Features"
  },
  {
    "objectID": "slides/week-2.html#todays-data",
    "href": "slides/week-2.html#todays-data",
    "title": "Week 2",
    "section": "Todays Data:",
    "text": "Todays Data:\n\nSimple feature collection with 64 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   geoid       name      aland state_nm                       geometry\n1  08001      Adams 3021840487 Colorado MULTIPOLYGON (((-105.0532 3...\n2  08003    Alamosa 1871643028 Colorado MULTIPOLYGON (((-105.4855 3...\n3  08005   Arapahoe 2066438714 Colorado MULTIPOLYGON (((-103.7065 3...\n4  08007  Archuleta 3496712164 Colorado MULTIPOLYGON (((-107.1287 3...\n5  08009       Baca 6617400567 Colorado MULTIPOLYGON (((-102.0416 3...\n6  08011       Bent 3918255148 Colorado MULTIPOLYGON (((-102.7476 3...\n7  08013    Boulder 1881325109 Colorado MULTIPOLYGON (((-105.3978 3...\n8  08014 Broomfield   85386685 Colorado MULTIPOLYGON (((-105.1092 3...\n9  08015    Chaffee 2624715692 Colorado MULTIPOLYGON (((-105.9698 3...\n10 08017   Cheyenne 4605713960 Colorado MULTIPOLYGON (((-103.1729 3..."
  },
  {
    "objectID": "slides/week-2.html#simple-features-1",
    "href": "slides/week-2.html#simple-features-1",
    "title": "Week 2",
    "section": "Simple Features",
    "text": "Simple Features\n\nSimple feature geometries describe the geometries of features.\nThe main application of simple feature geometries is to describe 2D geometries as points, lines, or polygons.\n‚Äúsimple‚Äù refers to the fact that line or polygon geometries are represented by set of points connected with straight lines.\nSimple features access is a standard (Herring 2011, Herring (2010), ISO (2004)) for describing simple feature geometries via:\n\na class hierarchy\na set of operations\nbinary and text encodings"
  },
  {
    "objectID": "slides/week-2.html#simple-features-access",
    "href": "slides/week-2.html#simple-features-access",
    "title": "Week 2",
    "section": "Simple Features Access",
    "text": "Simple Features Access\n\nSimple features or simple feature access refers to the formal standard (ISO 19125-1:2004) describing how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects.\nIt also describes how objects can be stored in and retrieved from databases, and which geometrical operations should/can be defined for them.\nThe standard is widely implemented in spatial databases (such as PostGIS), commercial GIS (e.g., ESRI ArcGIS) and forms the vector data basis for libraries such as GDAL.\nA subset of simple features (e.g.¬†the big 7) forms the GeoJSON specification.\nR has well-supported classes for storing spatial data (sp) and interfacing to the above mentioned environments (rgdal, rgeos), but has so far lacked a complete implementation of simple features, making conversions at times convoluted, inefficient or incomplete.\nsf is seeking to fill this gap and has/will succeed sp"
  },
  {
    "objectID": "slides/week-2.html#so-what-is-a-feature",
    "href": "slides/week-2.html#so-what-is-a-feature",
    "title": "Week 2",
    "section": "So what is a feature?",
    "text": "So what is a feature?\n\nA feature is a thing (object) in the real world, such as a building or a river\nThey often consist of other objects.\n\nA river system can be a feature, a river can be a feature, a river outlet can be a feature.\nA image pixel can be a feature, and the image can be a feature‚Ä¶"
  },
  {
    "objectID": "slides/week-2.html#spatial-features",
    "href": "slides/week-2.html#spatial-features",
    "title": "Week 2",
    "section": "Spatial Features",
    "text": "Spatial Features\n\nThe standard says: ‚ÄúA simple feature is defined by the OpenGIS Abstract specification to have both spatial and non-spatial attributes. Spatial attributes are geometry valued, and simple features are based on 2D geometry with linear interpolation between vertices.‚Äù - standard.\nSpatial Features have a geometry describing where the feature is located and how it is represented.\nThe geometry of a river can be its watershed, of its mainstem, or the point it drains to (see the OGC HY_Feature standard)\nFeatures can have attributes describing other properties of the feature\n\n\n\nstr(co$geometry)\n#&gt; sfc_MULTIPOLYGON of length 64; first list element: List of 1\n#&gt;  $ :List of 1\n#&gt;   ..$ : num [1:132, 1:2] -105 -105 -105 -105 -105 ...\n#&gt;  - attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n\n\nOther properties may include its length, slope, stream order or average flowrate"
  },
  {
    "objectID": "slides/week-2.html#geometry-types",
    "href": "slides/week-2.html#geometry-types",
    "title": "Week 2",
    "section": "Geometry types",
    "text": "Geometry types\nThe following 7 simple feature types are the most common, and are the only ones used for GeoJSON:\n\n\n\n\n\n\n\nSINGLE\nDescription\n\n\n\n\nPOINT\nzero-dimensional geometry containing a single point\n\n\nLINESTRING\nsequence of points connected by straight, non-self intersecting line pieces; one-dimensional geometry\n\n\nPOLYGON\ngeometry with a positive area (two-dimensional); sequence of points form a closed, non-self intersecting ring; the first ring denotes the exterior ring, zero or more subsequent rings denote holes in this exterior ring\n\n\n\n\n\n\n\n\n\n\nMULTI (same typed)\nDescription\n\n\n\n\nMULTIPOINT\nset of points; a MULTIPOINT is simple if no two Points in the MULTIPOINT are equal\n\n\nMULTILINESTRING\nset of linestrings\n\n\nMULTIPOLYGON\nset of polygons\n\n\n\n\n\n\n\n\n\n\nMulti-Typed\nDescription\n\n\n\n\nGEOMETRYCOLLECTION\nset of geometries of any type except GEOMETRYCOLLECTION\n\n\n\n\nThe descriptions above were copied from the PostGIS manual."
  },
  {
    "objectID": "slides/week-2.html#dimensions",
    "href": "slides/week-2.html#dimensions",
    "title": "Week 2",
    "section": "Dimensions",
    "text": "Dimensions\nAll geometries are composed of points\n\nPoints are defined by coordinates in a 2-, 3- or 4-D space.\nIn addition to XY coordinates, there are two optional dimensions:\na Z coordinate, denoting altitude\nan M coordinate (rarely used), denoting some measure\nThe M describes a property of the vertex that is independent of the feature.\nIt sounds attractive to encode a time as M, however these quickly become invalid once the path self-intersects.\nBoth Z and M are found relatively rarely, and software support to do something useful with them is rarer still."
  },
  {
    "objectID": "slides/week-2.html#valid-geometries",
    "href": "slides/week-2.html#valid-geometries",
    "title": "Week 2",
    "section": "Valid geometries",
    "text": "Valid geometries\n\nValid geometries obey the following properties:\n\nLINESTRINGS shall not self-intersect\nPOLYGON rings shall be closed (last point = first point)\nPOLYGON holes (inner rings) shall be inside their exterior ring\nPOLYGON inner rings shall maximally touch the exterior ring in single points, not over a line\nPOLYGON rings shall not repeat their own path\n\nIf any of the above is not the case, the geometry is not valid."
  },
  {
    "objectID": "slides/week-2.html#non-simple-and-non-valid-geometries",
    "href": "slides/week-2.html#non-simple-and-non-valid-geometries",
    "title": "Week 2",
    "section": "Non-simple and non-valid geometries",
    "text": "Non-simple and non-valid geometries\n\nst_is_simple and st_is_valid provide methods to help detect non-simple and non-valid geometries:\nAn example of a non-simple geometries is a self-intersecting lines\nAn example of a non-valid geometry are would be a polygon with slivers or self-intersections.\n\n\n\n\n(x1 &lt;- st_linestring(cbind(c(0,1,0,1),c(0,1,1,0))))\n#&gt; LINESTRING (0 0, 1 1, 0 1, 1 0)\nst_is_simple(x1)\n#&gt; [1] FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n(x2 &lt;- st_polygon(list(cbind(c(0,1,1,1,0,0),c(0,0,1,0.6,1,0)))))\n#&gt; POLYGON ((0 0, 1 0, 1 1, 1 0.6, 0 1, 0 0))\n(x3 &lt;- st_polygon(list(cbind(c(0,1,0,1,0),c(0,1,1,0,0)))))\n#&gt; POLYGON ((0 0, 1 1, 0 1, 1 0, 0 0))\n\nst_is_valid(c(x2,x3))\n#&gt; [1] FALSE"
  },
  {
    "objectID": "slides/week-2.html#empty-geometries",
    "href": "slides/week-2.html#empty-geometries",
    "title": "Week 2",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nAn important concept in the feature geometry framework is the empty geometry.\nempty geometries serve similar purposes as NA values in vectors (placeholder)\nEmpty geometries arise naturally from geometrical operations, for instance:\n\n\n\n(e = st_intersection(st_point(c(0,0)), st_point(c(1,1))))\n#&gt; GEOMETRYCOLLECTION EMPTY\n\n\nIt is not entirely clear what the benefit is of having typed empty geometries, but according to the simple feature standard they are type so the sf package abides by that.\nEmpty geometries can be detected by:\n\n\n\n\nst_is_empty(e)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "slides/week-2.html#so",
    "href": "slides/week-2.html#so",
    "title": "Week 2",
    "section": "So:",
    "text": "So:\n\nThere are 17 typed geometries supported by the simple feature standard\nAll geometries are made up of points\npoints can exist in 2,3,4 Dinimsonal space\nLINESTRING and POLYGON geometries have rules that define validity\nGeometries can be empty (but are still typed)"
  },
  {
    "objectID": "slides/week-2.html#wkt-and-wkb",
    "href": "slides/week-2.html#wkt-and-wkb",
    "title": "Week 2",
    "section": "WKT and WKB",
    "text": "WKT and WKB\n\nThe simple feature standard includes two encodings: Well-known text (WKT) & well-known binary (WKB)\nWell Known Text is human-readable:\n\n\n\nx &lt;- st_linestring(matrix(10:1,5))\nst_as_text(x)\n#&gt; [1] \"LINESTRING (10 5, 9 4, 8 3, 7 2, 6 1)\"\n\n\nIn this example, the word LINESTRING provides the geometry type which is followed by a parentheses, inside the parentheses are the points that make up the geometry.\nSeparate points are separated by a ‚Äúcomma‚Äù, while the point coordinates are separated by a ‚Äúspace.‚Äù\nCoordinates are usually floating point numbers, and moving large amounts of information as text is slow and imprecise."
  },
  {
    "objectID": "slides/week-2.html#how-simple-features-are-organized-in-r",
    "href": "slides/week-2.html#how-simple-features-are-organized-in-r",
    "title": "Week 2",
    "section": "How simple features are organized in R?",
    "text": "How simple features are organized in R?\n\nSimple Features is a standard that is implemented in R (not limited to R)\nSo far we have discusses simple features the standard, rather then simple features the implementation\nIn R, simple features are implemented using standard data structures (S3 classes, lists, matrix, vector).\nAttributes are stored in data.frames (or tbl_df)\nFeature geometries are stored in a data.frame column.\nSince geometries are not single-valued, they are put in a list-column\nThis means each observation (element) is a list itself!\n\n\nRemember our nested lists?\n\nlist(list(c(1:5)))\n#&gt; [[1]]\n#&gt; [[1]][[1]]\n#&gt; [1] 1 2 3 4 5"
  },
  {
    "objectID": "slides/week-2.html#sf-sfc-sfg",
    "href": "slides/week-2.html#sf-sfc-sfg",
    "title": "Week 2",
    "section": "sf, sfc, sfg",
    "text": "sf, sfc, sfg\nThe three classes are used to represent simple feature obejcts are:\nsf: data.frame with feature attributes and geometries\n\nwhich is contains an:\n\nsfc: the list-column with the geometries for each feature\n\n\nwhich is composed of:\n\nsfg, individual simple feature geometries"
  },
  {
    "objectID": "slides/week-2.html#sf-sfc-sfg-1",
    "href": "slides/week-2.html#sf-sfc-sfg-1",
    "title": "Week 2",
    "section": "sf, sfc, sfg",
    "text": "sf, sfc, sfg\n\nIn the output we see:\n\nin green a simple feature: a single record (row, consisting of attributes and geometry\nin blue a single simple feature geometry (an object of class sfg)\nin red a simple feature list-column (an object of class sfc, which is a column in the data.frame)\nEven though geometries are native R objects, they are printed as well-known text"
  },
  {
    "objectID": "slides/week-2.html#sfg-simple-feature-geometry-blue",
    "href": "slides/week-2.html#sfg-simple-feature-geometry-blue",
    "title": "Week 2",
    "section": "sfg: simple feature geometry (blue)",
    "text": "sfg: simple feature geometry (blue)\n\n\nSimple feature geometry (sfg) objects carry the geometry for a single feature\nSimple feature geometries are implemented as R native data, using the following rules\n\na single POINT is a numeric vector\na set of points (e.g.¬†in a LINESTRING or ring of a POLYGON) is a matrix, each row containing a point\nany other set is a list\n\n\nlist of numeric matrices for MULTILINESTRING and POLYGON\nlist of lists of numeric matrices for MULTIPOLYGON\nlist of (typed) geometries for GEOMETRYCOLLECTION"
  },
  {
    "objectID": "slides/week-2.html#sfg-simple-feature-geometry",
    "href": "slides/week-2.html#sfg-simple-feature-geometry",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\nCreator functions are rarely used in practice, since we typically read existing spatial data. But, they are useful for illustration:\n\n(x &lt;- st_point(c(1,2)))\n#&gt; POINT (1 2)\nstr(x)\n#&gt;  'XY' num [1:2] 1 2\n(x &lt;- st_linestring(matrix(c(1,2,3,4), ncol=2)))\n#&gt; LINESTRING (1 3, 2 4)\nstr(x)\n#&gt;  'XY' num [1:2, 1:2] 1 2 3 4"
  },
  {
    "objectID": "slides/week-2.html#sfg-simple-feature-geometry-1",
    "href": "slides/week-2.html#sfg-simple-feature-geometry-1",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\nAll geometry objects have a S3 class indicating their (1) dimension, (2) type, and (3) superclass\n\n(pt = st_point(c(0,1)))\n#&gt; POINT (0 1)\nattributes(pt)\n#&gt; $class\n#&gt; [1] \"XY\"    \"POINT\" \"sfg\"\n\n(pt2 = st_point(c(0,1,4)))\n#&gt; POINT Z (0 1 4)\nattributes(pt2)\n#&gt; $class\n#&gt; [1] \"XYZ\"   \"POINT\" \"sfg\""
  },
  {
    "objectID": "slides/week-2.html#sfg-simple-feature-geometry-2",
    "href": "slides/week-2.html#sfg-simple-feature-geometry-2",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\n\n(m1 = rbind(c(8, 1), c(2, 5), c(3, 2)))\n#&gt;      [,1] [,2]\n#&gt; [1,]    8    1\n#&gt; [2,]    2    5\n#&gt; [3,]    3    2\n\n(mp = st_multipoint(m1))\n#&gt; MULTIPOINT ((8 1), (2 5), (3 2))\nattributes(mp)\n#&gt; $dim\n#&gt; [1] 3 2\n#&gt; \n#&gt; $class\n#&gt; [1] \"XY\"         \"MULTIPOINT\" \"sfg\"\n\n(ls = st_linestring(m1))\n#&gt; LINESTRING (8 1, 2 5, 3 2)\nattributes(ls)\n#&gt; $dim\n#&gt; [1] 3 2\n#&gt; \n#&gt; $class\n#&gt; [1] \"XY\"         \"LINESTRING\" \"sfg\""
  },
  {
    "objectID": "slides/week-2.html#sfg-simple-feature-geometry-3",
    "href": "slides/week-2.html#sfg-simple-feature-geometry-3",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\nAlthough these geometries contain the same points (m1), they have entirely different meaning: the point set is a zero-dimensional, the line a one-dimensional geometry:\nHere, dimensions is not the XY vs XYZ, but rather whether the geometry has length (1D) or area (2D) or greater‚Ä¶\n\nst_dimension(mp)\n#&gt; [1] 0\n\nst_length(mp)\n#&gt; [1] 0\n\nst_dimension(ls)\n#&gt; [1] 1\n\nst_length(ls)\n#&gt; [1] 10.37338"
  },
  {
    "objectID": "slides/week-2.html#geometrycollection",
    "href": "slides/week-2.html#geometrycollection",
    "title": "Week 2",
    "section": "GEOMETRYCOLLECTION",
    "text": "GEOMETRYCOLLECTION\n\nSingle features (1 geometry per row) can have a single geometry, that consists of several geometries of different types.\nSuch cases arise rather naturally when looking for intersections. For instance, the intersection of two LINESTRING geometries may be the combination of a LINESTRING and a POINT.\nPutting this intersection into a single feature geometry needs a GEOMETRYCOLLECTION\n\n\n\npt &lt;- st_point(c(1, 0))\nls &lt;- st_linestring(matrix(c(4, 3, 0, 0), ncol = 2))\npoly1 &lt;- st_polygon(list(matrix(c(5.5, 7, 7, 6, 5.5, 0, 0, -0.5, -0.5, 0), ncol = 2)))\npoly2 &lt;- st_polygon(list(matrix(c(6.6, 8, 8, 7, 6.6, 1, 1, 1.5, 1.5, 1), ncol = 2)))\nmultipoly &lt;- st_multipolygon(list(poly1, poly2))\n\nst_sfc(pt, ls, poly1, poly2, multipoly)\n#&gt; Geometry set for 5 features \n#&gt; Geometry type: GEOMETRY\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1 ymin: -0.5 xmax: 8 ymax: 1.5\n#&gt; CRS:           NA\n#&gt; POINT (1 0)\n#&gt; LINESTRING (4 0, 3 0)\n#&gt; POLYGON ((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5 0))\n#&gt; POLYGON ((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1))\n#&gt; MULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5...\n\n(j &lt;- st_geometrycollection(list(pt, ls, poly1, poly2, multipoly)))\n#&gt; GEOMETRYCOLLECTION (POINT (1 0), LINESTRING (4 0, 3 0), POLYGON ((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5 0)), POLYGON ((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1)), MULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5 0)), ((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1))))"
  },
  {
    "objectID": "slides/week-2.html#sfc-sets-of-geometries",
    "href": "slides/week-2.html#sfc-sets-of-geometries",
    "title": "Week 2",
    "section": "sfc: sets of geometries",
    "text": "sfc: sets of geometries\n\nsf provides a dedicated class for handeling geometry sets, called sfc (simple feature geometry list column).\nWe can create such a list column with constructor function st_sfc:\n\n\n\n(sfc = st_sfc(st_point(c(0,1)), st_point(c(-3,2))))\n#&gt; Geometry set for 2 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -3 ymin: 1 xmax: 0 ymax: 2\n#&gt; CRS:           NA\n#&gt; POINT (0 1)\n#&gt; POINT (-3 2)"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1))\n\n\n\n#&gt;      [,1] [,2]\n#&gt; [1,]    0    0\n#&gt; [2,]    1    1\n#&gt; [3,]    1    0\n#&gt; [4,]    0    1"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-1",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-1",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring()\n\n\n\n#&gt; LINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-2",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-2",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc()\n\n\n\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; LINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-3",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-3",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\")\n\n\n\n#&gt; Geometry set for 4 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; POINT (0 0)\n#&gt; POINT (1 1)\n#&gt; POINT (1 0)\n#&gt; POINT (0 1)"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-4",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-4",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p\n\n\n\n\n\nOn the last slide, st_sfc creates a set of one LINESTRING (p), with a size of 4.\nGoing the other way around (from set to feature), we need to combine geometries:\n\n\n\n\n\np\n#&gt; Geometry set for 4 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; POINT (0 0)\n#&gt; POINT (1 1)\n#&gt; POINT (1 0)\n#&gt; POINT (0 1)\n\n\n\nst_combine(p)\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTIPOINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; MULTIPOINT ((0 0), (1 1), (1 0), (0 1))"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-5",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-5",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1))\n\n\n\n#&gt;      [,1] [,2]\n#&gt; [1,]    0    0\n#&gt; [2,]    1    1\n#&gt; [3,]    1    0\n#&gt; [4,]    0    1"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-6",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-6",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring()\n\n\n\n#&gt; LINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-7",
    "href": "slides/week-2.html#sets-of-geometries-arise-when-we-separate-compound-geometries-7",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_cast(\"POINT\")\n\n\n\n#&gt; POINT (0 0)\n\n\n\nOn the last slide, st_sfc creates a set of one LINESTRING (p), with a size of 4.\nGoing the other way around (from set to feature), we need to combine geometries:\n\n\n\n\n\np\n#&gt; Geometry set for 4 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; POINT (0 0)\n#&gt; POINT (1 1)\n#&gt; POINT (1 0)\n#&gt; POINT (0 1)\n\n\n\nst_combine(p)\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTIPOINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; MULTIPOINT ((0 0), (1 1), (1 0), (0 1))"
  },
  {
    "objectID": "slides/week-2.html#casting-must-be-done-the-level-of-the-feature",
    "href": "slides/week-2.html#casting-must-be-done-the-level-of-the-feature",
    "title": "Week 2",
    "section": "Casting must be done the level of the feature",
    "text": "Casting must be done the level of the feature\nIf we want to go from the 4 feature (p) object to a 1 feature LINESTRING, we must combine before casting ‚Ä¶\n\nst_combine(p) |&gt; \n  st_cast(\"LINESTRING\")\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; LINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2.html#mixed-geometries",
    "href": "slides/week-2.html#mixed-geometries",
    "title": "Week 2",
    "section": "Mixed geometries",
    "text": "Mixed geometries\nSets of simple features also consist of features with heterogeneous geometries. In this case, the geometry type of the set is GEOMETRY:\n\n\n\n(g = st_sfc(st_point(c(0,0)), \n            st_linestring(rbind(c(0,0), c(1,1)))))\n#&gt; Geometry set for 2 features \n#&gt; Geometry type: GEOMETRY\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; POINT (0 0)\n#&gt; LINESTRING (0 0, 1 1)\n\n\nThese set can be filtered by using st_is\n\ng |&gt; st_is(\"LINESTRING\")\n#&gt; [1] FALSE  TRUE\n\nor, when working with sf objects,\n\n# Note need of %&gt;%\nst_sf(g) %&gt;%\n  filter(st_is(., \"LINESTRING\"))\n#&gt; Simple feature collection with 1 feature and 0 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt;                       g\n#&gt; 1 LINESTRING (0 0, 1 1)"
  },
  {
    "objectID": "slides/week-2.html#sf-objects-with-simple-features",
    "href": "slides/week-2.html#sf-objects-with-simple-features",
    "title": "Week 2",
    "section": "sf: objects with simple features",
    "text": "sf: objects with simple features\nSimple features geometries and feature attributes are put together in sf (simple feature) objects.\n\nco\n#&gt; Simple feature collection with 64 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    geoid       name      aland state_nm                       geometry\n#&gt; 1  08001      Adams 3021840487 Colorado MULTIPOLYGON (((-105.0532 3...\n#&gt; 2  08003    Alamosa 1871643028 Colorado MULTIPOLYGON (((-105.4855 3...\n#&gt; 3  08005   Arapahoe 2066438714 Colorado MULTIPOLYGON (((-103.7065 3...\n#&gt; 4  08007  Archuleta 3496712164 Colorado MULTIPOLYGON (((-107.1287 3...\n#&gt; 5  08009       Baca 6617400567 Colorado MULTIPOLYGON (((-102.0416 3...\n#&gt; 6  08011       Bent 3918255148 Colorado MULTIPOLYGON (((-102.7476 3...\n#&gt; 7  08013    Boulder 1881325109 Colorado MULTIPOLYGON (((-105.3978 3...\n#&gt; 8  08014 Broomfield   85386685 Colorado MULTIPOLYGON (((-105.1092 3...\n#&gt; 9  08015    Chaffee 2624715692 Colorado MULTIPOLYGON (((-105.9698 3...\n#&gt; 10 08017   Cheyenne 4605713960 Colorado MULTIPOLYGON (((-103.1729 3...\n\nThis sf object is of class\n\nclass(co)\n#&gt; [1] \"sf\"         \"data.frame\"\n\nmeaning it extends data.frame, but with a single list-column with geometries, which is held in the column named:\n\nattr(co, \"sf_column\")\n#&gt; [1] \"geometry\""
  },
  {
    "objectID": "slides/week-2.html#sfc-simple-feature-geometry-list-column",
    "href": "slides/week-2.html#sfc-simple-feature-geometry-list-column",
    "title": "Week 2",
    "section": "sfc: simple feature geometry list-column",
    "text": "sfc: simple feature geometry list-column\nThe column in the sf data.frame that contains the geometries is a list, of class sfc.\nWe can retrieve the geometry list-column as we would any data.frame column (e.g.¬†co$geometry), or more generally with st_geometry:\n\n(co_geom &lt;- st_geometry(co))\n#&gt; Geometry set for 64 features \n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 5 geometries:\n#&gt; MULTIPOLYGON (((-105.0532 39.79106, -104.976 39...\n#&gt; MULTIPOLYGON (((-105.4855 37.5779, -105.4859 37...\n#&gt; MULTIPOLYGON (((-103.7065 39.73989, -103.7239 3...\n#&gt; MULTIPOLYGON (((-107.1287 37.42294, -107.2803 3...\n#&gt; MULTIPOLYGON (((-102.0416 37.64428, -102.0558 3...\n\nGeometries are printed in abbreviated form, but we can view a complete geometry by selecting it:\n\nco_geom[[1]]\n#&gt; MULTIPOLYGON (((-105.0532 39.79106, -104.976 39.79104, -104.9731 39.79242, -104.9716 39.79829, -104.9687 39.7984, -104.9689 39.79104, -104.9602 39.79102, -104.9554 39.79463, -104.9405 39.7946, -104.9405 39.791, -104.927 39.79105, -104.927 39.78378, -104.9034 39.78381, -104.9036 39.79839, -104.8845 39.79832, -104.8844 39.81282, -104.8661 39.81285, -104.866 39.79839, -104.8291 39.79806, -104.7909 39.79825, -104.7909 39.8418, -104.7623 39.84179, -104.7623 39.84539, -104.731 39.84519, -104.7304 39.89613, -104.7033 39.89595, -104.7032 39.90693, -104.6921 39.90685, -104.6921 39.91418, -104.6796 39.91402, -104.6797 39.90701, -104.6309 39.90664, -104.6309 39.89929, -104.5996 39.89904, -104.5998 39.88131, -104.6052 39.88135, -104.6053 39.87311, -104.6192 39.87322, -104.6198 39.82242, -104.6554 39.82261, -104.6554 39.813, -104.6662 39.81307, -104.6661 39.82279, -104.7625 39.82344, -104.7626 39.79843, -104.7344 39.79844, -104.7346 39.76918, -104.7646 39.76919, -104.7646 39.77157, -104.7722 39.7715, -104.7722 39.77641, -104.7816 39.77648, -104.7816 39.7728, -104.8282 39.77278, -104.8282 39.76916, -104.833 39.76918, -104.8376 39.76717, -104.8564 39.76858, -104.8563 39.75813, -104.8482 39.75642, -104.8469 39.75469, -104.8799 39.75473, -104.88 39.74744, -104.8847 39.74747, -104.8846 39.74016, -104.8217 39.74029, -104.7256 39.74027, -104.6783 39.74, -104.6603 39.74048, -104.6528 39.73978, -104.6304 39.7395, -104.6246 39.74008, -104.5589 39.73933, -104.5074 39.73825, -104.3919 39.73804, -104.3401 39.73825, -104.265 39.73888, -104.2095 39.73902, -104.1151 39.73977, -104.0407 39.73998, -103.9776 39.74027, -103.9655 39.74052, -103.9075 39.74065, -103.8661 39.74024, -103.8002 39.7402, -103.794 39.74005, -103.7239 39.73978, -103.7065 39.73989, -103.7066 39.76555, -103.7063 39.82855, -103.7063 39.889, -103.7061 39.90854, -103.7062 39.95888, -103.7057 39.98511, -103.7057 40.00137, -103.8677 40.0012, -103.9546 40.00113, -104.0371 40.00113, -104.1503 40.00086, -104.1693 40.00078, -104.2674 40.00092, -104.3015 40.00077, -104.4523 40.00062, -104.6019 40.00053, -104.6406 40.00057, -104.7885 40.00041, -104.9048 40.00032, -104.9614 40.00034, -104.9809 40.00032, -104.9877 39.98648, -104.9878 39.97575, -104.9944 39.9758, -104.9971 39.97215, -104.9881 39.97218, -104.9881 39.96847, -104.9972 39.96853, -104.9974 39.98121, -105.0158 39.98119, -105.0156 39.95519, -105.0171 39.95281, -105.0122 39.95045, -105.0063 39.95044, -105.0063 39.9468, -104.9972 39.94677, -104.9971 39.94324, -105.0157 39.94313, -105.0155 39.9214, -105.0344 39.9213, -105.0343 39.91418, -105.0529 39.91422, -105.0532 39.86362, -105.0532 39.79106)))"
  },
  {
    "objectID": "slides/week-2.html#reading-and-writing",
    "href": "slides/week-2.html#reading-and-writing",
    "title": "Week 2",
    "section": "Reading and writing",
    "text": "Reading and writing\nAs we‚Äôve seen above, reading spatial data from an external file can be done via sf - reading data requires the ‚Äúparser function‚Äù and the file path\n\n#&gt; Reading layer `co' from data source \n#&gt;   `/Users/mikejohnson/github/csu-ess-523c/slides/data/co.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 64 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n\nwe can suppress the output by adding argument quiet=TRUE or by using the otherwise nearly identical but more quiet\n\nco &lt;- read_sf(\"data/co.shp\")\n\nWriting takes place in the same fashion, using st_write:\n\nst_write(co, \"data/co.shp\")\n\nor its quiet alternative that silently overwrites existing files by default,\n\nwrite_sf(co, \"co.shp\") # silently overwrites"
  },
  {
    "objectID": "slides/week-2.html#from-tables-e.g.-csv",
    "href": "slides/week-2.html#from-tables-e.g.-csv",
    "title": "Week 2",
    "section": "From Tables (e.g.¬†CSV)",
    "text": "From Tables (e.g.¬†CSV)\nSpatial data can also be created from CSV and other flat files once it is in R:\n\n(cities = readr::read_csv(\"../labs/data/uscities.csv\") |&gt; \n  select(city, state_name, county_name, population, lat, lng) )\n#&gt; # A tibble: 31,254 √ó 6\n#&gt;    city         state_name           county_name         population   lat    lng\n#&gt;    &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 New York     New York             Queens                18832416  40.7  -73.9\n#&gt;  2 Los Angeles  California           Los Angeles           11885717  34.1 -118. \n#&gt;  3 Chicago      Illinois             Cook                   8489066  41.8  -87.7\n#&gt;  4 Miami        Florida              Miami-Dade             6113982  25.8  -80.2\n#&gt;  5 Houston      Texas                Harris                 6046392  29.8  -95.4\n#&gt;  6 Dallas       Texas                Dallas                 5843632  32.8  -96.8\n#&gt;  7 Philadelphia Pennsylvania         Philadelphia           5696588  40.0  -75.1\n#&gt;  8 Atlanta      Georgia              Fulton                 5211164  33.8  -84.4\n#&gt;  9 Washington   District of Columbia District of Columb‚Ä¶    5146120  38.9  -77.0\n#&gt; 10 Boston       Massachusetts        Suffolk                4355184  42.3  -71.1\n#&gt; # ‚Ñπ 31,244 more rows"
  },
  {
    "objectID": "slides/week-2.html#data-manipulation",
    "href": "slides/week-2.html#data-manipulation",
    "title": "Week 2",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nSince sf objects are data.frames, our dplyr verbs work!\nLets find the most populous city in each Colorado county‚Ä¶"
  },
  {
    "objectID": "slides/week-2.html#sf-and-dplyr",
    "href": "slides/week-2.html#sf-and-dplyr",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf\n\n\n\n#&gt; Simple feature collection with 31254 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -176.6295 ymin: 17.9559 xmax: 174.111 ymax: 71.2727\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 31,254 √ó 5\n#&gt;    city         state_name      county_name population            geometry\n#&gt;  * &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n#&gt;  1 New York     New York        Queens        18832416  (-73.9249 40.6943)\n#&gt;  2 Los Angeles  California      Los Angeles   11885717 (-118.4068 34.1141)\n#&gt;  3 Chicago      Illinois        Cook           8489066  (-87.6866 41.8375)\n#&gt;  4 Miami        Florida         Miami-Dade     6113982   (-80.2101 25.784)\n#&gt;  5 Houston      Texas           Harris         6046392   (-95.3885 29.786)\n#&gt;  6 Dallas       Texas           Dallas         5843632  (-96.7667 32.7935)\n#&gt;  7 Philadelphia Pennsylvania    Philadelph‚Ä¶    5696588  (-75.1339 40.0077)\n#&gt;  8 Atlanta      Georgia         Fulton         5211164   (-84.422 33.7628)\n#&gt;  9 Washington   District of Co‚Ä¶ District o‚Ä¶    5146120  (-77.0163 38.9047)\n#&gt; 10 Boston       Massachusetts   Suffolk        4355184  (-71.0852 42.3188)\n#&gt; # ‚Ñπ 31,244 more rows"
  },
  {
    "objectID": "slides/week-2.html#sf-and-dplyr-1",
    "href": "slides/week-2.html#sf-and-dplyr-1",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\")\n\n\n\n#&gt; Simple feature collection with 477 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0066 ymin: 37.0155 xmax: -102.0804 ymax: 40.9849\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 477 √ó 5\n#&gt;    city             state_name county_name population            geometry\n#&gt;  * &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n#&gt;  1 Denver           Colorado   Denver         2691349  (-104.8758 39.762)\n#&gt;  2 Colorado Springs Colorado   El Paso         638421 (-104.7605 38.8674)\n#&gt;  3 Aurora           Colorado   Arapahoe        390201 (-104.7237 39.7083)\n#&gt;  4 Fort Collins     Colorado   Larimer         339256 (-105.0656 40.5477)\n#&gt;  5 Lakewood         Colorado   Jefferson       156309 (-105.1172 39.6977)\n#&gt;  6 Greeley          Colorado   Weld            143554 (-104.7706 40.4152)\n#&gt;  7 Thornton         Colorado   Adams           142878 (-104.9438 39.9197)\n#&gt;  8 Grand Junction   Colorado   Mesa            141008 (-108.5673 39.0877)\n#&gt;  9 Arvada           Colorado   Jefferson       122835   (-105.151 39.832)\n#&gt; 10 Boulder          Colorado   Boulder         120121 (-105.2524 40.0248)\n#&gt; # ‚Ñπ 467 more rows"
  },
  {
    "objectID": "slides/week-2.html#sf-and-dplyr-2",
    "href": "slides/week-2.html#sf-and-dplyr-2",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\") |&gt;\n  group_by(county_name)\n\n\n\n#&gt; Simple feature collection with 477 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0066 ymin: 37.0155 xmax: -102.0804 ymax: 40.9849\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 477 √ó 5\n#&gt; # Groups:   county_name [64]\n#&gt;    city             state_name county_name population            geometry\n#&gt;    &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n#&gt;  1 Denver           Colorado   Denver         2691349  (-104.8758 39.762)\n#&gt;  2 Colorado Springs Colorado   El Paso         638421 (-104.7605 38.8674)\n#&gt;  3 Aurora           Colorado   Arapahoe        390201 (-104.7237 39.7083)\n#&gt;  4 Fort Collins     Colorado   Larimer         339256 (-105.0656 40.5477)\n#&gt;  5 Lakewood         Colorado   Jefferson       156309 (-105.1172 39.6977)\n#&gt;  6 Greeley          Colorado   Weld            143554 (-104.7706 40.4152)\n#&gt;  7 Thornton         Colorado   Adams           142878 (-104.9438 39.9197)\n#&gt;  8 Grand Junction   Colorado   Mesa            141008 (-108.5673 39.0877)\n#&gt;  9 Arvada           Colorado   Jefferson       122835   (-105.151 39.832)\n#&gt; 10 Boulder          Colorado   Boulder         120121 (-105.2524 40.0248)\n#&gt; # ‚Ñπ 467 more rows"
  },
  {
    "objectID": "slides/week-2.html#sf-and-dplyr-3",
    "href": "slides/week-2.html#sf-and-dplyr-3",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\") |&gt;\n  group_by(county_name) |&gt;\n  slice_max(population, n = 1)\n\n\n\n#&gt; Simple feature collection with 64 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -108.9071 ymin: 37.1751 xmax: -102.2627 ymax: 40.9849\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 64 √ó 5\n#&gt; # Groups:   county_name [64]\n#&gt;    city           state_name county_name population            geometry\n#&gt;    &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n#&gt;  1 Thornton       Colorado   Adams           142878 (-104.9438 39.9197)\n#&gt;  2 Alamosa        Colorado   Alamosa           9847  (-105.877 37.4752)\n#&gt;  3 Aurora         Colorado   Arapahoe        390201 (-104.7237 39.7083)\n#&gt;  4 Pagosa Springs Colorado   Archuleta         1718 (-107.0307 37.2675)\n#&gt;  5 Springfield    Colorado   Baca              1482  (-102.6189 37.405)\n#&gt;  6 Las Animas     Colorado   Bent              2480 (-103.2236 38.0695)\n#&gt;  7 Boulder        Colorado   Boulder         120121 (-105.2524 40.0248)\n#&gt;  8 Broomfield     Colorado   Broomfield       75110 (-105.0526 39.9542)\n#&gt;  9 Salida         Colorado   Chaffee           5786 (-105.9979 38.5298)\n#&gt; 10 Cheyenne Wells Colorado   Cheyenne           949 (-102.3521 38.8192)\n#&gt; # ‚Ñπ 54 more rows"
  },
  {
    "objectID": "slides/week-2.html#sf-and-dplyr-4",
    "href": "slides/week-2.html#sf-and-dplyr-4",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\") |&gt;\n  group_by(county_name) |&gt;\n  slice_max(population, n = 1) -&gt;\n  co_cities"
  },
  {
    "objectID": "slides/week-2.html#plotting",
    "href": "slides/week-2.html#plotting",
    "title": "Week 2",
    "section": "Plotting",
    "text": "Plotting\n\nWe‚Äôve already seen that ggplot() is a powerful visualization tool:\nThe 5 steps we described for building a ggplot are:\n\ncanvas\nlayers (geoms)\nlabels\nfacets\nthemes\n\nspatial work in R is becoming so common that ggplot() comes with a sf geom (geom_sf)"
  },
  {
    "objectID": "slides/week-2.html#sf-an-ggplot",
    "href": "slides/week-2.html#sf-an-ggplot",
    "title": "Week 2",
    "section": "sf an ggplot",
    "text": "sf an ggplot\n\n\n\nggplot()"
  },
  {
    "objectID": "slides/week-2.html#sf-an-ggplot-1",
    "href": "slides/week-2.html#sf-an-ggplot-1",
    "title": "Week 2",
    "section": "sf an ggplot",
    "text": "sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10))"
  },
  {
    "objectID": "slides/week-2.html#sf-an-ggplot-2",
    "href": "slides/week-2.html#sf-an-ggplot-2",
    "title": "Week 2",
    "section": "sf an ggplot",
    "text": "sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10)) +\n  geom_sf(data = co_cities, aes(size = population/1e5), col = \"red\")"
  },
  {
    "objectID": "slides/week-2.html#sf-an-ggplot-3",
    "href": "slides/week-2.html#sf-an-ggplot-3",
    "title": "Week 2",
    "section": "sf an ggplot",
    "text": "sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10)) +\n  geom_sf(data = co_cities, aes(size = population/1e5), col = \"red\") +\n  theme_linedraw()"
  },
  {
    "objectID": "slides/week-2.html#sf-an-ggplot-4",
    "href": "slides/week-2.html#sf-an-ggplot-4",
    "title": "Week 2",
    "section": "sf an ggplot",
    "text": "sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10)) +\n  geom_sf(data = co_cities, aes(size = population/1e5), col = \"red\") +\n  theme_linedraw() +\n  labs(title = \"Colorado Counties: Land Area\",\n       size = \"Population \\n(100,000)\",\n       fill = \"Acres \\n(billions)\")"
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3: Tesselations, Point-in-Polygon",
    "section": "",
    "text": "Background\nIn this lab we will an explore the impacts of tessellated surfaces and the modifiable areal unit problem (MAUP) using the National Dam Inventory maintained by the United States Army Corps of Engineers. Doing this will require repetitive tasks that we will write as functions and careful consideration of feature aggregation/simplification, spatial joins, and data visualization. The end goal is to visualize the distribution of dams and there purposes across the country.\nDISCLAIMER: This lab will be crunching a TON of data, in some cases 562,590,604 values for a single process! Therefore, I encourage you to run your code chuck-by-chunk rather then regularly knitting. Your final knit may take a couple of minutes to process. I know this is painful but be proud that, all said, your report will be analyzing billions of meaningful data and geometric relations.\n\nThis labs covers 4 main skills:\n\nTessellating Surfaces to discritized space\nGeometry Simplification: to expedite expensive intersections\nWriting functions to expedite repetitious reporting and mapping tasks\nPoint-in-polygon counts to aggregate point data\n\n\nLibraries\n\n\n\n\nQuestion 1:\nHere we will prepare five tessellated surfaces from CONUS and write a function to plot them in a descriptive way.\n\nStep 1.1\nFirst, we need a spatial file of CONUS counties. For future area calculations we want these in an equal area projection (EPSG:5070).\nTo achieve this:\n\nget an sf object of US counties (AOI::aoi_get(state = \"conus\", county = \"all\"))\ntransform the data to EPSG:5070\n\n\n\nStep 1.2\nFor triangle based tessellations we need point locations to serve as our ‚Äúanchors‚Äù.\nTo achieve this:\n\ngenerate county centroids using st_centroid\nSince, we can only tessellate over a feature we need to combine or union the resulting 3,108 POINT features into a single MULTIPOINT feature\nSince these are point objects, the difference between union/combine is mute\n\n\n\nStep 1.3\nTessellations/Coverage‚Äôs describe the extent of a region with geometric shapes, called tiles, with no overlaps or gaps.\nTiles can range in size, shape, area and have different methods for being created.\nSome methods generate triangular tiles across a set of defined points (e.g.¬†voroni and delauny triangulation)\nOthers generate equal area tiles over a known extent (st_make_grid)\nFor this lab, we will create surfaces of CONUS using using 4 methods, 2 based on an extent and 2 based on point anchors:\nTessellations :\n\nst_voroni: creates voroni tessellation\nst_traingulate: triangulates set of points (not constrained)\n\nCoverage‚Äôs:\n\nst_make_grid: Creates a square grid covering the geometry of an sf or sfc object\nst_make_grid(square = FALSE): Create a hexagonal grid covering the geometry of an sf or sfc object\nThe side of coverage tiles can be defined by a cell resolution or a specified number of cell in the X and Y direction\n\n\nFor this step:\n\nMake a voroni tessellation over your county centroids (MULTIPOINT)\nMake a triangulated tessellation over your county centroids (MULTIPOINT)\nMake a gridded coverage with n = 70, over your counties object\nMake a hexagonal coverage with n = 70, over your counties object\n\nIn addition to creating these 4 coverage‚Äôs we need to add an ID to each tile.\nTo do this:\n\nadd a new column to each tessellation that spans from 1:n().\nRemember that ALL tessellation methods return an sfc GEOMETRYCOLLECTION, and to add attribute information - like our ID - you will have to coerce the sfc list into an sf object (st_sf or st_as_sf)\n\nLast, we want to ensure that our surfaces are topologically valid/simple.\n\nTo ensure this, we can pass our surfaces through st_cast.\nRemember that casting an object explicitly (e.g.¬†st_cast(x, \"POINT\")) changes a geometry\nIf no output type is specified (e.g.¬†st_cast(x)) then the cast attempts to simplify the geometry.\nIf you don‚Äôt do this you might get unexpected ‚ÄúTopologyException‚Äù errors.\n\n\n\nStep 1.4\nIf you plot the above tessellations you‚Äôll see the triangulated surfaces produce regions far beyond the boundaries of CONUS.\nWe need to cut these boundaries to CONUS border.\nTo do this, we will call on st_intersection, but will first need a geometry of CONUS to serve as our differencing feature. We can get this by unioning our existing county boundaries.\n\n\nStep 1.5\nWith a single feature boundary, we must carefully consider the complexity of the geometry. Remember, the more points our geometry contains, the more computations needed for spatial predicates our differencing. For a task like ours, we do not need a finely resolved coastal boarder.\nTo achieve this:\n\nSimplify your unioned border using the Visvalingam algorithm provided by rmapshaper::ms_simplify.\nChoose what percentage of vertices to retain using the keep argument and work to find the highest number that provides a shape you are comfortable with for the analysis:\n\n\nOnce you are happy with your simplification, use the mapview::npts function to report the number of points in your original object, and the number of points in your simplified object.\nHow many points were you able to remove? What are the consequences of doing this computationally?\n\n\nFinally, use your simplified object to crop the two triangulated tessellations with st_intersection:\n\n\n\nStep 1.6\nThe last step is to plot your tessellations. We don‚Äôt want to write out 5 ggplots (or mindlessly copy and paste üòÑ)\nInstead, lets make a function that takes an sf object as arg1 and a character string as arg2 and returns a ggplot object showing arg1 titled with arg2.\n\nThe form of a function is:\n\nname = function(arg1, arg2) {\n  \n  ... code goes here ...\n  \n}\n\n\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nIn your function, the code should follow our standard ggplot practice where your data is arg1, and your title is arg2\nThe function should also enforce the following:\n\na white fill\na navy border\na size of 0.2\n`theme_void``\na caption that reports the number of features in arg1\n\nYou will need to paste character stings and variables together.\n\n\n\n\n\nStep 1.7\nUse your new function to plot each of your tessellated surfaces and the original county data (5 plots in total):\n\n\n\nQuestion 2:\nIn this question, we will write out a function to summarize our tessellated surfaces. Most of this should have been done in your daily assignments.\n\nStep 2.1\nFirst, we need a function that takes a sf object and a character string and returns a data.frame.\nFor this function:\n\nThe function name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string describing the object\nIn your function, calculate the area of arg1; convert the units to km2; and then drop the units\nNext, create a data.frame containing the following:\n\ntext from arg2\nthe number of features in arg1\nthe mean area of the features in arg1 (km2)\nthe standard deviation of the features in arg1\nthe total area (km2) of arg1\n\nReturn this data.frame\n\n\n\nStep 2.2\nUse your new function to summarize each of your tessellations and the original counties.\n\n\nStep 2.3\nMultiple data.frame objects can bound row-wise with bind_rows into a single data.frame\nFor example, if your function is called sum_tess, the following would bind your summaries of the triangulation and voroni object.\n\n\nStep 2.4\nOnce your 5 summaries are bound (2 tessellations, 2 coverage‚Äôs, and the raw counties) print the data.frame as a nice table using knitr/kableExtra.\n\n\nStep 2.5\nComment on the traits of each tessellation. Be specific about how these traits might impact the results of a point-in-polygon analysis in the contexts of the modifiable areal unit problem and with respect computational requirements.\n\n\n\n\nQuestion 3:\nThe data we are going to analysis in this lab is from US Army Corp of Engineers National Dam Inventory (NID). This dataset documents ~91,000 dams in the United States and a variety of attribute information including design specifications, risk level, age, and purpose.\nFor the remainder of this lab we will analysis the distributions of these dams (Q3) and their purpose (Q4) through using a point-in-polygon analysis.\n\nStep 3.1\nIn the tradition of this class - and true to data science/GIS work - you need to find, download, and manage raw data. While the raw NID data is no longer easy to get with the transition of the USACE services to ESRI Features Services, I staged the data in the resources directory of this class. To get it, navigate to that location and download the raw file into you lab data directory.\n\nReturn to your RStudio Project and read the data in using the readr::read_csv\n\nAfter reading the data in, be sure to remove rows that don‚Äôt have location values (!is.na())\nConvert the data.frame to a sf object by defining the coordinates and CRS\nTransform the data to a CONUS AEA (EPSG:5070) projection - matching your tessellation\nFilter to include only those within your CONUS boundary\n\n\n\ndams = readr::read_csv('../labs/data/NID2019_U.csv') \n\nusa &lt;- AOI::aoi_get(state = \"conus\") %&gt;% \n  st_union() %&gt;% \n  st_transform(5070)\n\ndams2 = dams %&gt;% \n  filter(!is.na(LATITUDE) ) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4236) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(usa)\n\n\n\nStep 3.2\nFollowing the in-class examples develop an efficient point-in-polygon function that takes:\n\npoints as arg1,\npolygons as arg2,\nThe name of the id column as arg3\n\nThe function should make use of spatial and non-spatial joins, sf coercion and dplyr::count. The returned object should be input sf object with a column - n - counting the number of points in each tile.\n\n\nStep 3.3\nApply your point-in-polygon function to each of your five tessellated surfaces where:\n\nYour points are the dams\nYour polygons are the respective tessellation\nThe id column is the name of the id columns you defined.\n\n\n\nStep 3.4\nLets continue the trend of automating our repetitive tasks through function creation. This time make a new function that extends your previous plotting function.\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nThe function should also enforce the following:\n\nthe fill aesthetic is driven by the count column n\nthe col is NA\nthe fill is scaled to a continuous viridis color ramp\ntheme_void\na caption that reports the number of dams in arg1 (e.g.¬†sum(n))\n\nYou will need to paste character stings and variables together.\n\n\n\n\n\nStep 3.5\nApply your plotting function to each of the 5 tessellated surfaces with Point-in-Polygon counts:\n\n\nStep 3.6\nComment on the influence of the tessellated surface in the visualization of point counts. How does this related to the MAUP problem. Moving forward you will only use one tessellation, which will you chose and why?\nWhile there is not ‚Äúright‚Äù answer, justify your selection here.\n\n\n\nQuestion 4\nThe NID provides a comprehensive data dictionary here. In it we find that dam purposes are designated by a character code.\nThese are shown below for convenience (built using knitr on a data.frame called nid_classifier):\n\n\n\nNID 2019: Dam Purposes\n\n\nabbr\npurpose\n\n\n\n\nI\nIrrigation\n\n\nH\nHydroelectric\n\n\nC\nFlood Control\n\n\nN\nNavigation\n\n\nS\nWater Supply\n\n\nR\nRecreation\n\n\nP\nFire Protection\n\n\nF\nFish and Wildlife\n\n\nD\nDebris Control\n\n\nT\nTailings\n\n\nG\nGrade Stabilization\n\n\nO\nOther\n\n\n\n\n\n\n\n\nIn the data dictionary, we see a dam can have multiple purposes.\nIn these cases, the purpose codes are concatenated in order of decreasing importance. For example, SCR would indicate the primary purposes are Water Supply, then Flood Control, then Recreation.\nA standard summary indicates there are over 400 unique combinations of dam purposes:\n\n\nunique(dams2$PURPOSES) %&gt;% length()\n\n[1] 494\n\n\n\nBy storing dam codes as a concatenated string, there is no easy way to identify how many dams serve any one purpose‚Ä¶ for example where are the hydro electric dams?\n\n\nTo overcome this data structure limitation, we can identify how many dams serve each purpose by splitting the PURPOSES values (strsplit) and tabulating the unlisted results as a data.frame. Effectively this is double/triple/quadruple counting dams bases on how many purposes they serve:\n\n\nJoining with `by = join_by(abbr)`\n\n\nThe result of this would indicate:\n\n\n\n\n\n\n\n\n\n\nStep 4.1\n\nYour task is to create point-in-polygon counts for at least 4 of the above dam purposes:\nYou will use grepl to filter the complete dataset to those with your chosen purpose\nRemember that grepl returns a boolean if a given pattern is matched in a string\ngrepl is vectorized so can be used in dplyr::filter\n\n\nFor example:\n\n# Find flood control dams in the first 5 records:\ndams2$PURPOSES[1:5]\n\n[1] \"FR\" \"R\"  \"C\"  \"FR\" \"R\" \n\ngrepl(\"F\", dams2$PURPOSES[1:5])\n\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\n\n\nFor your analysis, choose at least four of the above codes, and describe why you chose them. Then for each of them, create a subset of dams that serve that purpose using dplyr::filter and grepl\nFinally, use your point-in-polygon function to count each subset across your elected tessellation\n\n\nStep 4.2\n\nNow use your plotting function from Q3 to map these counts.\nBut! you will use gghighlight to only color those tiles where the count (n) is greater then the (mean + 1 standard deviation) of the set\nSince your plotting function returns a ggplot object already, the gghighlight call can be added ‚Äú+‚Äù directly to the function.\nThe result of this exploration is to highlight the areas of the country with the most\n\n\n\nStep 4.3\nComment of geographic distribution of dams you found. Does it make sense? How might the tessellation you chose impact your findings? How does the distribution of dams coincide with other geographic factors such as river systems, climate, ect?\n\n\n\nQuestion 5:\nYou have also been asked to identify the largest, at risk, flood control dams in the country\nYou must also map the Mississippi River System - This data is available here - Download the shapefile and unzip it into your data directory. - Use read_sf to import this data and filter it to only include the Mississippi SYSTEM\nTo achieve this:\nCreate an interactive map using leaflet to show the largest (NID_STORAGE); high-hazard (HAZARD == ‚ÄúH‚Äù) dam in each state\n\nThe markers should be drawn as opaque, circle markers, filled red with no border, and a radius set equal to the (NID_Storage / 1,500,000)\nThe map tiles should be selected from any of the tile providers\nA popup table should be added using leafem::popup and should only include the dam name, storage, purposes, and year completed.\nThe Mississippi system should be added at a Polyline feature.\n\n\n\n\nRubric\n\nQuestion 1: Tessellations (30)\nQuestion 2: Tessellation Comparison (30)\nQuestion 3: PIP (30)\nQuestion 4: Conditional PIP (30)\nQuestion 5: Dam Age (20)\nWell Structured and appealing Qmd deployed as web page (10)\n\nTotal: 150\n\n\nSubmission\nYou will submit a URL to your web page deployed with GitHub pages.\nTo do this:\n\nRender your lab document\nStage/commit/push your files\nIf you followed the naming conventions in the ‚ÄúSet Up‚Äù of lab 1, your lab 3 link will be available at:\n\nhttps://USERNAME.github.io/csu-523c/lab-03.html\nSubmit this URL in the appropriate Canvas dropbox. Also take a moment to update your personal webpage with this link and some bullet points of what you learned. While not graded as part of this lab, it will be extra credit!"
  },
  {
    "objectID": "slides/week-2-1.html#simple-features",
    "href": "slides/week-2-1.html#simple-features",
    "title": "Week 2",
    "section": "Simple Features",
    "text": "Simple Features"
  },
  {
    "objectID": "slides/week-2-1.html#todays-data",
    "href": "slides/week-2-1.html#todays-data",
    "title": "Week 2",
    "section": "Todays Data:",
    "text": "Todays Data:\n\nSimple feature collection with 64 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   geoid       name      aland state_nm                       geometry\n1  08001      Adams 3021840487 Colorado MULTIPOLYGON (((-105.0532 3...\n2  08003    Alamosa 1871643028 Colorado MULTIPOLYGON (((-105.4855 3...\n3  08005   Arapahoe 2066438714 Colorado MULTIPOLYGON (((-103.7065 3...\n4  08007  Archuleta 3496712164 Colorado MULTIPOLYGON (((-107.1287 3...\n5  08009       Baca 6617400567 Colorado MULTIPOLYGON (((-102.0416 3...\n6  08011       Bent 3918255148 Colorado MULTIPOLYGON (((-102.7476 3...\n7  08013    Boulder 1881325109 Colorado MULTIPOLYGON (((-105.3978 3...\n8  08014 Broomfield   85386685 Colorado MULTIPOLYGON (((-105.1092 3...\n9  08015    Chaffee 2624715692 Colorado MULTIPOLYGON (((-105.9698 3...\n10 08017   Cheyenne 4605713960 Colorado MULTIPOLYGON (((-103.1729 3..."
  },
  {
    "objectID": "slides/week-2-1.html#simple-features-1",
    "href": "slides/week-2-1.html#simple-features-1",
    "title": "Week 2",
    "section": "Simple Features",
    "text": "Simple Features\n\nSimple feature geometries describe the geometries of features.\nThe main application of simple feature geometries is to describe 2D geometries as points, lines, or polygons.\n‚Äúsimple‚Äù refers to the fact that line or polygon geometries are represented by set of points connected with straight lines.\nSimple features access is a standard (Herring 2011, Herring (2010), ISO (2004)) for describing simple feature geometries via:\n\na class hierarchy\na set of operations\nbinary and text encodings"
  },
  {
    "objectID": "slides/week-2-1.html#simple-features-access",
    "href": "slides/week-2-1.html#simple-features-access",
    "title": "Week 2",
    "section": "Simple Features Access",
    "text": "Simple Features Access\n\nSimple features or simple feature access refers to the formal standard (ISO 19125-1:2004) describing how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects.\nIt also describes how objects can be stored in and retrieved from databases, and which geometrical operations should/can be defined for them.\nThe standard is widely implemented in spatial databases (such as PostGIS), commercial GIS (e.g., ESRI ArcGIS) and forms the vector data basis for libraries such as GDAL.\nA subset of simple features (e.g.¬†the big 7) forms the GeoJSON specification.\nR has well-supported classes for storing spatial data (sp) and interfacing to the above mentioned environments (rgdal, rgeos), but has so far lacked a complete implementation of simple features, making conversions at times convoluted, inefficient or incomplete.\nsf is seeking to fill this gap and has/will succeed sp"
  },
  {
    "objectID": "slides/week-2-1.html#so-what-is-a-feature",
    "href": "slides/week-2-1.html#so-what-is-a-feature",
    "title": "Week 2",
    "section": "So what is a feature?",
    "text": "So what is a feature?\n\nA feature is a thing (object) in the real world, such as a building or a river\nThey often consist of other objects.\n\nA river system can be a feature, a river can be a feature, a river outlet can be a feature.\nA image pixel can be a feature, and the image can be a feature‚Ä¶"
  },
  {
    "objectID": "slides/week-2-1.html#spatial-features",
    "href": "slides/week-2-1.html#spatial-features",
    "title": "Week 2",
    "section": "Spatial Features",
    "text": "Spatial Features\n\nThe standard says: ‚ÄúA simple feature is defined by the OpenGIS Abstract specification to have both spatial and non-spatial attributes. Spatial attributes are geometry valued, and simple features are based on 2D geometry with linear interpolation between vertices.‚Äù - standard.\nSpatial Features have a geometry describing where the feature is located and how it is represented.\n\n\nstr(co$geometry)\nsfc_MULTIPOLYGON of length 64; first list element: List of 1\n $ :List of 1\n  ..$ : num [1:132, 1:2] -105 -105 -105 -105 -105 ...\n - attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n\n\nThe geometry of a river can be its watershed, of its mainstem, or the point it drains to (see the OGC HY_Feature standard)\nFeatures can have attributes describing other properties of the feature\nOther properties may include its length, slope, stream order or average flowrate"
  },
  {
    "objectID": "slides/week-2-1.html#geometry-types",
    "href": "slides/week-2-1.html#geometry-types",
    "title": "Week 2",
    "section": "Geometry types",
    "text": "Geometry types\nThe following 7 simple feature types are the most common, and are the only ones used for GeoJSON:\n\n\n\n\n\n\n\nSINGLE\nDescription\n\n\n\n\nPOINT\nzero-dimensional geometry containing a single point\n\n\nLINESTRING\nsequence of points connected by straight, non-self intersecting line pieces; one-dimensional geometry\n\n\nPOLYGON\ngeometry with a positive area (two-dimensional); sequence of points form a closed, non-self intersecting ring; the first ring denotes the exterior ring, zero or more subsequent rings denote holes in this exterior ring\n\n\n\n\n\n\n\n\n\n\nMULTI (same typed)\nDescription\n\n\n\n\nMULTIPOINT\nset of points; a MULTIPOINT is simple if no two Points in the MULTIPOINT are equal\n\n\nMULTILINESTRING\nset of linestrings\n\n\nMULTIPOLYGON\nset of polygons\n\n\n\n\n\n\n\n\n\n\nMulti-Typed\nDescription\n\n\n\n\nGEOMETRYCOLLECTION\nset of geometries of any type except GEOMETRYCOLLECTION\n\n\n\n\nThe descriptions above were copied from the PostGIS manual."
  },
  {
    "objectID": "slides/week-2-1.html#dimensions",
    "href": "slides/week-2-1.html#dimensions",
    "title": "Week 2",
    "section": "Dimensions",
    "text": "Dimensions\nAll geometries are composed of points\n\nPoints are defined by coordinates in a 2-, 3- or 4-D space.\nIn addition to XY coordinates, there are two optional dimensions:\na Z coordinate, denoting altitude\nan M coordinate (rarely used), denoting some measure\nThe M describes a property of the vertex that is independent of the feature.\nIt sounds attractive to encode a time as M, however these quickly become invalid once the path self-intersects.\nBoth Z and M are found relatively rarely, and software support to do something useful with them is rarer still."
  },
  {
    "objectID": "slides/week-2-1.html#valid-geometries",
    "href": "slides/week-2-1.html#valid-geometries",
    "title": "Week 2",
    "section": "Valid geometries",
    "text": "Valid geometries\nValid geometries obey the following properties:\n\nLINESTRINGS shall not self-intersect\nPOLYGON rings shall be closed (last point = first point)\nPOLYGON holes (inner rings) shall be inside their exterior ring\nPOLYGON inner rings shall maximally touch the exterior ring in single points, not over a line\nPOLYGON rings shall not repeat their own path\n\nIf any of the above is not the case, the geometry is not valid."
  },
  {
    "objectID": "slides/week-2-1.html#non-simple-and-non-valid-geometries",
    "href": "slides/week-2-1.html#non-simple-and-non-valid-geometries",
    "title": "Week 2",
    "section": "Non-simple and non-valid geometries",
    "text": "Non-simple and non-valid geometries\nst_is_simple and st_is_valid provide methods to help detect non-simple and non-valid geometries:\n\nAn example of a non-simple geometries is a self-intersecting lines;\n\n\n(x1 &lt;- st_linestring(cbind(c(0,1,0,1),c(0,1,1,0))))\nLINESTRING (0 0, 1 1, 0 1, 1 0)\nst_is_simple(x1)\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\nAn example of a non-valid geometry are would be a polygon with slivers or self-intersections.\n\n\n(x2 &lt;- st_polygon(list(cbind(c(0,1,1,1,0,0),c(0,0,1,0.6,1,0)))))\nPOLYGON ((0 0, 1 0, 1 1, 1 0.6, 0 1, 0 0))\n(x3 &lt;- st_polygon(list(cbind(c(0,1,0,1,0),c(0,1,1,0,0)))))\nPOLYGON ((0 0, 1 1, 0 1, 1 0, 0 0))\n\nst_is_valid(c(x2,x3))\n[1] FALSE"
  },
  {
    "objectID": "slides/week-2-1.html#empty-geometries",
    "href": "slides/week-2-1.html#empty-geometries",
    "title": "Week 2",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nAn important concept in the feature geometry framework is the empty geometry.\nempty geometries serve similar purposes as NA values in vectors (placeholder)\nEmpty geometries arise naturally from geometrical operations, for instance:\n\n\n(e = st_intersection(st_point(c(0,0)), st_point(c(1,1))))\nGEOMETRYCOLLECTION EMPTY\n\n\nIt is not entirely clear what the benefit is of having typed empty geometries, but according to the simple feature standard they are type so the sf package abides by that.\nEmpty geometries can be detected by:\n\n\nst_is_empty(e)\n[1] TRUE"
  },
  {
    "objectID": "slides/week-2-1.html#so",
    "href": "slides/week-2-1.html#so",
    "title": "Week 2",
    "section": "So:",
    "text": "So:\n\nThere are 17 typed geometries supported by the simple feature standard\nAll geometries are made up of points\npoints can exist in 2,3,4 Dinimsonal space\nLINESTRING and POLYGON geometries have rules that define validity\nGeometries can be empty (but are still typed)"
  },
  {
    "objectID": "slides/week-2-1.html#wkt-and-wkb",
    "href": "slides/week-2-1.html#wkt-and-wkb",
    "title": "Week 2",
    "section": "WKT and WKB",
    "text": "WKT and WKB\nThe simple feature standard includes two encodings:\nWell-known text (WKT) & well-known binary (WKB)\nWell Known Text is human-readable:\n\nx &lt;- st_linestring(matrix(10:1,5))\nst_as_text(x)\n[1] \"LINESTRING (10 5, 9 4, 8 3, 7 2, 6 1)\"\n\nIn this example,\nThe word LINESTRING provides the geometry type which is followed by a parentheses, inside the parentheses are the points that make up the geometry.\nSeparate points are separated by a ‚Äúcomma‚Äù, while the point coordinates are separated by a ‚Äúspace.‚Äù\nCoordinates are usually floating point numbers, and moving large amounts of information as text is slow and imprecise.\nFor that reason, we use well-known binary (WKB) encoding\n\nx\nLINESTRING (10 5, 9 4, 8 3, 7 2, 6 1)\nst_as_binary(x)\n [1] 01 02 00 00 00 05 00 00 00 00 00 00 00 00 00 24 40 00 00 00 00 00 00 14 40\n[26] 00 00 00 00 00 00 22 40 00 00 00 00 00 00 10 40 00 00 00 00 00 00 20 40 00\n[51] 00 00 00 00 00 08 40 00 00 00 00 00 00 1c 40 00 00 00 00 00 00 00 40 00 00\n[76] 00 00 00 00 18 40 00 00 00 00 00 00 f0 3f\n\n\nBinary conversion is used to communicate geometries to external libraries (GDAL, GEOS, liblwgeom) and spatial databases because it is fast and lossless.\nWKT and WKB can both be transformed back into R native objects by\n\n\nst_as_sfc(\"LINESTRING(10 5, 9 4, 8 3, 7 2, 6 1)\")[[1]]\nLINESTRING (10 5, 9 4, 8 3, 7 2, 6 1)\nst_as_sfc(structure(list(st_as_binary(x)), class = \"WKB\"))[[1]]\nLINESTRING (10 5, 9 4, 8 3, 7 2, 6 1)\n\nConversion between R native objects and WKB is done by package sf in compiled (C++/Rcpp) code, making this a reusable and fast route for I/O of simple feature geometries in R."
  },
  {
    "objectID": "slides/week-2-1.html#how-simple-features-are-organized-in-r",
    "href": "slides/week-2-1.html#how-simple-features-are-organized-in-r",
    "title": "Week 2",
    "section": "How simple features are organized in R?",
    "text": "How simple features are organized in R?\n\nSimple Features is a standard that is implemented in R (not limited to R)\nSo far we have discusses simple features the standard, rather then simple features the implementation\nIn R, simple features are implemented using standard data structures (S3 classes, lists, matrix, vector).\nAttributes are stored in data.frames (or tbl_df)\nFeature geometries are stored in a data.frame column.\nSince geometries are not single-valued, they are put in a list-column\nThis means each observation (element) is a list itself!\n\nRemember our nested lists?\n\nlist(list(c(1:5)))\n[[1]]\n[[1]][[1]]\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "slides/week-2-1.html#sf-sfc-sfg",
    "href": "slides/week-2-1.html#sf-sfc-sfg",
    "title": "Week 2",
    "section": "sf, sfc, sfg",
    "text": "sf, sfc, sfg\nThe three classes are used to represent simple feature obejcts are:\n\nsf: data.frame with feature attributes and geometries\n\nwhich is composed of\n\nsfc: the list-column with the geometries for each feature\n\nwhich is composed of\n\nsfg, individual simple feature geometries"
  },
  {
    "objectID": "slides/week-2-1.html#sf-sfc-sfg-1",
    "href": "slides/week-2-1.html#sf-sfc-sfg-1",
    "title": "Week 2",
    "section": "sf, sfc, sfg",
    "text": "sf, sfc, sfg\n\nIn the output we see:\n\nin green a simple feature: a single record (row, consisting of attributes and geometry\nin blue a single simple feature geometry (an object of class sfg)\nin red a simple feature list-column (an object of class sfc, which is a column in the data.frame)\n\nEven though geometries are native R objects, they are printed as well-known text"
  },
  {
    "objectID": "slides/week-2-1.html#sfg-simple-feature-geometry-blue",
    "href": "slides/week-2-1.html#sfg-simple-feature-geometry-blue",
    "title": "Week 2",
    "section": "sfg: simple feature geometry (blue)",
    "text": "sfg: simple feature geometry (blue)\n\n\nSimple feature geometry (sfg) objects carry the geometry for a single feature\nSimple feature geometries are implemented as R native data, using the following rules\n\na single POINT is a numeric vector\na set of points (e.g.¬†in a LINESTRING or ring of a POLYGON) is a matrix, each row containing a point\nany other set is a list\n\n\nlist of numeric matrices for MULTILINESTRING and POLYGON\nlist of lists of numeric matrices for MULTIPOLYGON\nlist of (typed) geometries for GEOMETRYCOLLECTION"
  },
  {
    "objectID": "slides/week-2-1.html#sfg-simple-feature-geometry",
    "href": "slides/week-2-1.html#sfg-simple-feature-geometry",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\nCreator functions are rarely used in practice, since we typically read existing spatial data. But, they are useful for illustration:\n\n(x &lt;- st_point(c(1,2)))\nPOINT (1 2)\nstr(x)\n 'XY' num [1:2] 1 2\n(x &lt;- st_linestring(matrix(c(1,2,3,4), ncol=2)))\nLINESTRING (1 3, 2 4)\nstr(x)\n 'XY' num [1:2, 1:2] 1 2 3 4"
  },
  {
    "objectID": "slides/week-2-1.html#sfg-simple-feature-geometry-1",
    "href": "slides/week-2-1.html#sfg-simple-feature-geometry-1",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\nAll geometry objects have a S3 class indicating their (1) dimension, (2) type, and (3) superclass\n\n(pt = st_point(c(0,1)))\nPOINT (0 1)\nattributes(pt)\n$class\n[1] \"XY\"    \"POINT\" \"sfg\"  \n\n(pt2 = st_point(c(0,1,4)))\nPOINT Z (0 1 4)\nattributes(pt2)\n$class\n[1] \"XYZ\"   \"POINT\" \"sfg\""
  },
  {
    "objectID": "slides/week-2-1.html#sfg-simple-feature-geometry-2",
    "href": "slides/week-2-1.html#sfg-simple-feature-geometry-2",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\n\n\n\n(m1 = rbind(c(8, 1), c(2, 5), c(3, 2)))\n     [,1] [,2]\n[1,]    8    1\n[2,]    2    5\n[3,]    3    2\n\n(mp = st_multipoint(m1))\nMULTIPOINT ((8 1), (2 5), (3 2))\nattributes(mp)\n$dim\n[1] 3 2\n\n$class\n[1] \"XY\"         \"MULTIPOINT\" \"sfg\"       \n\n\n\n(ls = st_linestring(m1))\nLINESTRING (8 1, 2 5, 3 2)\nattributes(ls)\n$dim\n[1] 3 2\n\n$class\n[1] \"XY\"         \"LINESTRING\" \"sfg\""
  },
  {
    "objectID": "slides/week-2-1.html#sfg-simple-feature-geometry-3",
    "href": "slides/week-2-1.html#sfg-simple-feature-geometry-3",
    "title": "Week 2",
    "section": "sfg: simple feature geometry",
    "text": "sfg: simple feature geometry\nAlthough these geometries contain the same points (m1), they have entirely different meaning: the point set is a zero-dimensional, the line a one-dimensional geometry:\nHere, dimensions is no the XY vs XYZ, but rather whether the geometry has length (1D) or area (2D) or greater‚Ä¶\n\nst_dimension(mp)\n[1] 0\nst_length(mp)\n[1] 0\nst_dimension(ls)\n[1] 1\nst_length(ls)\n[1] 10.37338"
  },
  {
    "objectID": "slides/week-2-1.html#geometrycollection",
    "href": "slides/week-2-1.html#geometrycollection",
    "title": "Week 2",
    "section": "GEOMETRYCOLLECTION",
    "text": "GEOMETRYCOLLECTION\n\nSingle features can have a geometry that consists of several geometries of different types.\nSuch cases arise rather naturally when looking for intersections. For instance, the intersection of two LINESTRING geometries may be the combination of a LINESTRING and a POINT.\nPutting this intersection into a single feature geometry needs a GEOMETRYCOLLECTION\n\n\npt &lt;- st_point(c(1, 0))\nls &lt;- st_linestring(matrix(c(4, 3, 0, 0), ncol = 2))\npoly1 &lt;- st_polygon(list(matrix(c(5.5, 7, 7, 6, 5.5, 0, 0, -0.5, -0.5, 0), ncol = 2)))\npoly2 &lt;- st_polygon(list(matrix(c(6.6, 8, 8, 7, 6.6, 1, 1, 1.5, 1.5, 1), ncol = 2)))\nmultipoly &lt;- st_multipolygon(list(poly1, poly2))\n\n(j &lt;- st_geometrycollection(list(pt, ls, poly1, poly2, multipoly)))\nGEOMETRYCOLLECTION (POINT (1 0), LINESTRING (4 0, 3 0), POLYGON ((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5 0)), POLYGON ((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1)), MULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5 0)), ((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1))))\n\n\nIn case we end up with GEOMETRYCOLLECTION objects, the next question is often what to do with them. One thing we can do is extract elements from them:\n\n\nst_collection_extract(j, \"POLYGON\")\nGeometry set for 3 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5.5 ymin: -0.5 xmax: 8 ymax: 1.5\nCRS:           NA\nMULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5...\nMULTIPOLYGON (((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1)))\nMULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5...\n\nst_collection_extract(j, \"POINT\")\nPOINT (1 0)\n\nst_collection_extract(j, \"LINESTRING\")\nLINESTRING (4 0, 3 0)"
  },
  {
    "objectID": "slides/week-2-1.html#sfc-sets-of-geometries",
    "href": "slides/week-2-1.html#sfc-sets-of-geometries",
    "title": "Week 2",
    "section": "sfc: sets of geometries",
    "text": "sfc: sets of geometries\n\nsf provides a dedicated class for handeling geometry sets, called sfc (simple feature geometry list column).\nWe can create such a list column with constructor function st_sfc:\n\n\n(sfc = st_sfc(st_point(c(0,1)), st_point(c(-3,2))))\nGeometry set for 2 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -3 ymin: 1 xmax: 0 ymax: 2\nCRS:           NA\nPOINT (0 1)\nPOINT (-3 2)\n\nThe default report from the print method for sfc gives\n\nthe number of features geometries\nthe feature geometry type (here: POINT)\nthe feature geometry dimension (here: XY)\nthe bounding box for the set\nthe coordinate reference system for the set (epsg and proj4string)\nthe first few geometries, as (abbreviated) WKT\n\nThe class of the geometry list-column is a combination of a specific class, and a superclass.\n\nclass(sfc)\n[1] \"sfc_POINT\" \"sfc\"      \n\nIn addition to a class, the sfc object has further attributes (remember S3 class!)\n\nattributes(sfc) |&gt; names()\n[1] \"class\"     \"precision\" \"bbox\"      \"crs\"       \"n_empty\"  \n\nwhich are used to record for the whole set:\n\na precision value\nthe bounding box enclosing all geometries (for x and y)\na coordinate reference system\nthe number of empty geometries contained in the set\n\nThis means that all these properties are defined for the set (sfc), and not for geometries (sfg) individually.\nsfc objects are lists with each entry being an sfg object:\n\np[[2]]\nPOINT (1 1)\n\nand we will use these lists as list columns in data.frame or tibble objects to represent simple features with geometries in a list column."
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1))\n\n\n\n     [,1] [,2]\n[1,]    0    0\n[2,]    1    1\n[3,]    1    0\n[4,]    0    1"
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-1",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-1",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring()\n\n\n\nLINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-2",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-2",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc()\n\n\n\nGeometry set for 1 feature \nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\nCRS:           NA\nLINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-3",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-3",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\")\n\n\n\nGeometry set for 4 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\nCRS:           NA\nPOINT (0 0)\nPOINT (1 1)\nPOINT (1 0)\nPOINT (0 1)"
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-4",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-4",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p"
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-5",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-5",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1))\n\n\n\n     [,1] [,2]\n[1,]    0    0\n[2,]    1    1\n[3,]    1    0\n[4,]    0    1"
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-6",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-6",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring()\n\n\n\nLINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-7",
    "href": "slides/week-2-1.html#sets-of-geometries-arise-when-we-separate-compound-geometries-7",
    "title": "Week 2",
    "section": "Sets of geometries arise when we separate compound geometries:",
    "text": "Sets of geometries arise when we separate compound geometries:\n\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_sfc() |&gt;\n   st_cast(\"POINT\") -&gt;\n  p\n\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt;\n   st_linestring() |&gt;\n   st_cast(\"POINT\")\n\n\n\nPOINT (0 0)\n\n\n\nOn the last slide, st_sfc creates a set of one LINESTRING (p), with a size of 4.\nGoing the other way around (from set to feature), we need to combine geometries:\n\n\n\n\np\nGeometry set for 4 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\nCRS:           NA\nPOINT (0 0)\nPOINT (1 1)\nPOINT (1 0)\nPOINT (0 1)\n\n\n\nst_combine(p)\nGeometry set for 1 feature \nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\nCRS:           NA\nMULTIPOINT ((0 0), (1 1), (1 0), (0 1))"
  },
  {
    "objectID": "slides/week-2-1.html#casting-must-be-done-the-level-of-the-feature",
    "href": "slides/week-2-1.html#casting-must-be-done-the-level-of-the-feature",
    "title": "Week 2",
    "section": "Casting must be done the level of the feature",
    "text": "Casting must be done the level of the feature\nIf we want to go from the 4 feature (p) object to a 1 feature LINESTRING, we must combine before casting ‚Ä¶\n\nst_combine(p) |&gt; \n  st_cast(\"LINESTRING\")\nGeometry set for 1 feature \nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\nCRS:           NA\nLINESTRING (0 0, 1 1, 1 0, 0 1)"
  },
  {
    "objectID": "slides/week-2-1.html#mixed-geometries",
    "href": "slides/week-2-1.html#mixed-geometries",
    "title": "Week 2",
    "section": "Mixed geometries",
    "text": "Mixed geometries\nSets of simple features also consist of features with heterogeneous geometries. In this case, the geometry type of the set is GEOMETRY:\n\n\n\n(g = st_sfc(st_point(c(0,0)), \n            st_linestring(rbind(c(0,0), c(1,1)))))\nGeometry set for 2 features \nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\nCRS:           NA\nPOINT (0 0)\nLINESTRING (0 0, 1 1)\n\n\nThese set can be filtered by using st_is\n\ng |&gt; st_is(\"LINESTRING\")\n[1] FALSE  TRUE\n\nor, when working with sf objects,\n\n# Note need of %&gt;%\nst_sf(g) %&gt;%\n  filter(st_is(., \"LINESTRING\"))\nSimple feature collection with 1 feature and 0 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\nCRS:           NA\n                      g\n1 LINESTRING (0 0, 1 1)"
  },
  {
    "objectID": "slides/week-2-1.html#sf-objects-with-simple-features",
    "href": "slides/week-2-1.html#sf-objects-with-simple-features",
    "title": "Week 2",
    "section": "sf: objects with simple features",
    "text": "sf: objects with simple features\nSimple features geometries and feature attributes are put together in sf (simple feature) objects.\n\nco\nSimple feature collection with 64 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   geoid       name      aland state_nm                       geometry\n1  08001      Adams 3021840487 Colorado MULTIPOLYGON (((-105.0532 3...\n2  08003    Alamosa 1871643028 Colorado MULTIPOLYGON (((-105.4855 3...\n3  08005   Arapahoe 2066438714 Colorado MULTIPOLYGON (((-103.7065 3...\n4  08007  Archuleta 3496712164 Colorado MULTIPOLYGON (((-107.1287 3...\n5  08009       Baca 6617400567 Colorado MULTIPOLYGON (((-102.0416 3...\n6  08011       Bent 3918255148 Colorado MULTIPOLYGON (((-102.7476 3...\n7  08013    Boulder 1881325109 Colorado MULTIPOLYGON (((-105.3978 3...\n8  08014 Broomfield   85386685 Colorado MULTIPOLYGON (((-105.1092 3...\n9  08015    Chaffee 2624715692 Colorado MULTIPOLYGON (((-105.9698 3...\n10 08017   Cheyenne 4605713960 Colorado MULTIPOLYGON (((-103.1729 3...\n\nThis sf object is of class\n\nclass(co)\n[1] \"sf\"         \"data.frame\"\n\nmeaning it extends data.frame, but with a single list-column with geometries, which is held in the column named:\n\nattr(co, \"sf_column\")\n[1] \"geometry\""
  },
  {
    "objectID": "slides/week-2-1.html#sfc-simple-feature-geometry-list-column",
    "href": "slides/week-2-1.html#sfc-simple-feature-geometry-list-column",
    "title": "Week 2",
    "section": "sfc: simple feature geometry list-column",
    "text": "sfc: simple feature geometry list-column\nThe column in the sf data.frame that contains the geometries is a list, of class sfc.\nWe can retrieve the geometry list-column as we would any data.frame column (e.g.¬†ca$geometry), or more generally with st_geometry:\n\n(co_geom &lt;- st_geometry(co))\nGeometry set for 64 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\nMULTIPOLYGON (((-105.0532 39.79106, -104.976 39...\nMULTIPOLYGON (((-105.4855 37.5779, -105.4859 37...\nMULTIPOLYGON (((-103.7065 39.73989, -103.7239 3...\nMULTIPOLYGON (((-107.1287 37.42294, -107.2803 3...\nMULTIPOLYGON (((-102.0416 37.64428, -102.0558 3...\n\nGeometries are printed in abbreviated form, but we can view a complete geometry by selecting it:\n\nco_geom[[1]]\nMULTIPOLYGON (((-105.0532 39.79106, -104.976 39.79104, -104.9731 39.79242, -104.9716 39.79829, -104.9687 39.7984, -104.9689 39.79104, -104.9602 39.79102, -104.9554 39.79463, -104.9405 39.7946, -104.9405 39.791, -104.927 39.79105, -104.927 39.78378, -104.9034 39.78381, -104.9036 39.79839, -104.8845 39.79832, -104.8844 39.81282, -104.8661 39.81285, -104.866 39.79839, -104.8291 39.79806, -104.7909 39.79825, -104.7909 39.8418, -104.7623 39.84179, -104.7623 39.84539, -104.731 39.84519, -104.7304 39.89613, -104.7033 39.89595, -104.7032 39.90693, -104.6921 39.90685, -104.6921 39.91418, -104.6796 39.91402, -104.6797 39.90701, -104.6309 39.90664, -104.6309 39.89929, -104.5996 39.89904, -104.5998 39.88131, -104.6052 39.88135, -104.6053 39.87311, -104.6192 39.87322, -104.6198 39.82242, -104.6554 39.82261, -104.6554 39.813, -104.6662 39.81307, -104.6661 39.82279, -104.7625 39.82344, -104.7626 39.79843, -104.7344 39.79844, -104.7346 39.76918, -104.7646 39.76919, -104.7646 39.77157, -104.7722 39.7715, -104.7722 39.77641, -104.7816 39.77648, -104.7816 39.7728, -104.8282 39.77278, -104.8282 39.76916, -104.833 39.76918, -104.8376 39.76717, -104.8564 39.76858, -104.8563 39.75813, -104.8482 39.75642, -104.8469 39.75469, -104.8799 39.75473, -104.88 39.74744, -104.8847 39.74747, -104.8846 39.74016, -104.8217 39.74029, -104.7256 39.74027, -104.6783 39.74, -104.6603 39.74048, -104.6528 39.73978, -104.6304 39.7395, -104.6246 39.74008, -104.5589 39.73933, -104.5074 39.73825, -104.3919 39.73804, -104.3401 39.73825, -104.265 39.73888, -104.2095 39.73902, -104.1151 39.73977, -104.0407 39.73998, -103.9776 39.74027, -103.9655 39.74052, -103.9075 39.74065, -103.8661 39.74024, -103.8002 39.7402, -103.794 39.74005, -103.7239 39.73978, -103.7065 39.73989, -103.7066 39.76555, -103.7063 39.82855, -103.7063 39.889, -103.7061 39.90854, -103.7062 39.95888, -103.7057 39.98511, -103.7057 40.00137, -103.8677 40.0012, -103.9546 40.00113, -104.0371 40.00113, -104.1503 40.00086, -104.1693 40.00078, -104.2674 40.00092, -104.3015 40.00077, -104.4523 40.00062, -104.6019 40.00053, -104.6406 40.00057, -104.7885 40.00041, -104.9048 40.00032, -104.9614 40.00034, -104.9809 40.00032, -104.9877 39.98648, -104.9878 39.97575, -104.9944 39.9758, -104.9971 39.97215, -104.9881 39.97218, -104.9881 39.96847, -104.9972 39.96853, -104.9974 39.98121, -105.0158 39.98119, -105.0156 39.95519, -105.0171 39.95281, -105.0122 39.95045, -105.0063 39.95044, -105.0063 39.9468, -104.9972 39.94677, -104.9971 39.94324, -105.0157 39.94313, -105.0155 39.9214, -105.0344 39.9213, -105.0343 39.91418, -105.0529 39.91422, -105.0532 39.86362, -105.0532 39.79106)))"
  },
  {
    "objectID": "slides/week-2-1.html#reading-and-writing",
    "href": "slides/week-2-1.html#reading-and-writing",
    "title": "Week 2",
    "section": "Reading and writing",
    "text": "Reading and writing\nAs we‚Äôve seen above, reading spatial data from an external file can be done via sf - reading data requires the ‚Äúparser function‚Äù and the file path\n\nco &lt;- st_read(\"data/co.shp\")\n\nwe can suppress the output by adding argument quiet=TRUE or by using the otherwise nearly identical but more quiet\n\nca &lt;- read_sf(\"data/co.shp\")\n\nWriting takes place in the same fashion, using st_write:\n\nst_write(co, \"data/co.shp\")\n\nor its quiet alternative that silently overwrites existing files by default,\n\nwrite_sf(co, \"co.shp\") # silently overwrites"
  },
  {
    "objectID": "slides/week-2-1.html#from-tables-e.g.-csv",
    "href": "slides/week-2-1.html#from-tables-e.g.-csv",
    "title": "Week 2",
    "section": "From Tables (e.g.¬†CSV)",
    "text": "From Tables (e.g.¬†CSV)\nSpatial data can also be created from CSV and other flat files once it is in R:\n\n(cities = readr::read_csv(\"../labs/data/uscities.csv\") |&gt; \n  select(city, state_name, county_name, population, lat, lng) )\n# A tibble: 31,254 √ó 6\n   city         state_name           county_name         population   lat    lng\n   &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 New York     New York             Queens                18832416  40.7  -73.9\n 2 Los Angeles  California           Los Angeles           11885717  34.1 -118. \n 3 Chicago      Illinois             Cook                   8489066  41.8  -87.7\n 4 Miami        Florida              Miami-Dade             6113982  25.8  -80.2\n 5 Houston      Texas                Harris                 6046392  29.8  -95.4\n 6 Dallas       Texas                Dallas                 5843632  32.8  -96.8\n 7 Philadelphia Pennsylvania         Philadelphia           5696588  40.0  -75.1\n 8 Atlanta      Georgia              Fulton                 5211164  33.8  -84.4\n 9 Washington   District of Columbia District of Columb‚Ä¶    5146120  38.9  -77.0\n10 Boston       Massachusetts        Suffolk                4355184  42.3  -71.1\n# ‚Ñπ 31,244 more rows"
  },
  {
    "objectID": "slides/week-2-1.html#data-manipulation",
    "href": "slides/week-2-1.html#data-manipulation",
    "title": "Week 2",
    "section": "Data Manipulation",
    "text": "Data Manipulation\nSince sf objects are data.frames, our dplyr verbs work!\nLets find the most populous city in each California county‚Ä¶"
  },
  {
    "objectID": "slides/week-2-1.html#sf-and-dplyr",
    "href": "slides/week-2-1.html#sf-and-dplyr",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf\n\n\n\nSimple feature collection with 31254 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -176.6295 ymin: 17.9559 xmax: 174.111 ymax: 71.2727\nGeodetic CRS:  WGS 84\n# A tibble: 31,254 √ó 5\n   city         state_name      county_name population            geometry\n * &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n 1 New York     New York        Queens        18832416  (-73.9249 40.6943)\n 2 Los Angeles  California      Los Angeles   11885717 (-118.4068 34.1141)\n 3 Chicago      Illinois        Cook           8489066  (-87.6866 41.8375)\n 4 Miami        Florida         Miami-Dade     6113982   (-80.2101 25.784)\n 5 Houston      Texas           Harris         6046392   (-95.3885 29.786)\n 6 Dallas       Texas           Dallas         5843632  (-96.7667 32.7935)\n 7 Philadelphia Pennsylvania    Philadelph‚Ä¶    5696588  (-75.1339 40.0077)\n 8 Atlanta      Georgia         Fulton         5211164   (-84.422 33.7628)\n 9 Washington   District of Co‚Ä¶ District o‚Ä¶    5146120  (-77.0163 38.9047)\n10 Boston       Massachusetts   Suffolk        4355184  (-71.0852 42.3188)\n# ‚Ñπ 31,244 more rows"
  },
  {
    "objectID": "slides/week-2-1.html#sf-and-dplyr-1",
    "href": "slides/week-2-1.html#sf-and-dplyr-1",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\")\n\n\n\nSimple feature collection with 477 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -109.0066 ymin: 37.0155 xmax: -102.0804 ymax: 40.9849\nGeodetic CRS:  WGS 84\n# A tibble: 477 √ó 5\n   city             state_name county_name population            geometry\n * &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n 1 Denver           Colorado   Denver         2691349  (-104.8758 39.762)\n 2 Colorado Springs Colorado   El Paso         638421 (-104.7605 38.8674)\n 3 Aurora           Colorado   Arapahoe        390201 (-104.7237 39.7083)\n 4 Fort Collins     Colorado   Larimer         339256 (-105.0656 40.5477)\n 5 Lakewood         Colorado   Jefferson       156309 (-105.1172 39.6977)\n 6 Greeley          Colorado   Weld            143554 (-104.7706 40.4152)\n 7 Thornton         Colorado   Adams           142878 (-104.9438 39.9197)\n 8 Grand Junction   Colorado   Mesa            141008 (-108.5673 39.0877)\n 9 Arvada           Colorado   Jefferson       122835   (-105.151 39.832)\n10 Boulder          Colorado   Boulder         120121 (-105.2524 40.0248)\n# ‚Ñπ 467 more rows"
  },
  {
    "objectID": "slides/week-2-1.html#sf-and-dplyr-2",
    "href": "slides/week-2-1.html#sf-and-dplyr-2",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\") |&gt;\n  group_by(county_name)\n\n\n\nSimple feature collection with 477 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -109.0066 ymin: 37.0155 xmax: -102.0804 ymax: 40.9849\nGeodetic CRS:  WGS 84\n# A tibble: 477 √ó 5\n# Groups:   county_name [64]\n   city             state_name county_name population            geometry\n   &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n 1 Denver           Colorado   Denver         2691349  (-104.8758 39.762)\n 2 Colorado Springs Colorado   El Paso         638421 (-104.7605 38.8674)\n 3 Aurora           Colorado   Arapahoe        390201 (-104.7237 39.7083)\n 4 Fort Collins     Colorado   Larimer         339256 (-105.0656 40.5477)\n 5 Lakewood         Colorado   Jefferson       156309 (-105.1172 39.6977)\n 6 Greeley          Colorado   Weld            143554 (-104.7706 40.4152)\n 7 Thornton         Colorado   Adams           142878 (-104.9438 39.9197)\n 8 Grand Junction   Colorado   Mesa            141008 (-108.5673 39.0877)\n 9 Arvada           Colorado   Jefferson       122835   (-105.151 39.832)\n10 Boulder          Colorado   Boulder         120121 (-105.2524 40.0248)\n# ‚Ñπ 467 more rows"
  },
  {
    "objectID": "slides/week-2-1.html#sf-and-dplyr-3",
    "href": "slides/week-2-1.html#sf-and-dplyr-3",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\") |&gt;\n  group_by(county_name) |&gt;\n  slice_max(population, n = 1)\n\n\n\nSimple feature collection with 64 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -108.9071 ymin: 37.1751 xmax: -102.2627 ymax: 40.9849\nGeodetic CRS:  WGS 84\n# A tibble: 64 √ó 5\n# Groups:   county_name [64]\n   city           state_name county_name population            geometry\n   &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [¬∞]&gt;\n 1 Thornton       Colorado   Adams           142878 (-104.9438 39.9197)\n 2 Alamosa        Colorado   Alamosa           9847  (-105.877 37.4752)\n 3 Aurora         Colorado   Arapahoe        390201 (-104.7237 39.7083)\n 4 Pagosa Springs Colorado   Archuleta         1718 (-107.0307 37.2675)\n 5 Springfield    Colorado   Baca              1482  (-102.6189 37.405)\n 6 Las Animas     Colorado   Bent              2480 (-103.2236 38.0695)\n 7 Boulder        Colorado   Boulder         120121 (-105.2524 40.0248)\n 8 Broomfield     Colorado   Broomfield       75110 (-105.0526 39.9542)\n 9 Salida         Colorado   Chaffee           5786 (-105.9979 38.5298)\n10 Cheyenne Wells Colorado   Cheyenne           949 (-102.3521 38.8192)\n# ‚Ñπ 54 more rows"
  },
  {
    "objectID": "slides/week-2-1.html#sf-and-dplyr-4",
    "href": "slides/week-2-1.html#sf-and-dplyr-4",
    "title": "Week 2",
    "section": "sf and dplyr",
    "text": "sf and dplyr\n\n\n\ncities_sf |&gt;\n  filter(state_name == \"Colorado\") |&gt;\n  group_by(county_name) |&gt;\n  slice_max(population, n = 1) -&gt;\n  co_cities"
  },
  {
    "objectID": "slides/week-2-1.html#plotting",
    "href": "slides/week-2-1.html#plotting",
    "title": "Week 2",
    "section": "Plotting",
    "text": "Plotting\nWe‚Äôve already seen that ggplot() is a powerful visualization tool:\n‚Äì\nThe 5 steps we described for building a ggplot are: 1. canvas 2. layers (geoms) 3. labels 4. facets 5. themes\n‚Äì\nspatial work in R is becoming so common that ggplot() comes with a sf geom (geom_sf)"
  },
  {
    "objectID": "slides/week-2-1.html#sf-an-ggplot",
    "href": "slides/week-2-1.html#sf-an-ggplot",
    "title": "Week 2",
    "section": "##sf an ggplot",
    "text": "##sf an ggplot\n\n\n\nggplot()"
  },
  {
    "objectID": "slides/week-2-1.html#sf-an-ggplot-1",
    "href": "slides/week-2-1.html#sf-an-ggplot-1",
    "title": "Week 2",
    "section": "##sf an ggplot",
    "text": "##sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10))"
  },
  {
    "objectID": "slides/week-2-1.html#sf-an-ggplot-2",
    "href": "slides/week-2-1.html#sf-an-ggplot-2",
    "title": "Week 2",
    "section": "##sf an ggplot",
    "text": "##sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10)) +\n  geom_sf(data = co_cities, aes(size = population/1e5), col = \"red\")"
  },
  {
    "objectID": "slides/week-2-1.html#sf-an-ggplot-3",
    "href": "slides/week-2-1.html#sf-an-ggplot-3",
    "title": "Week 2",
    "section": "##sf an ggplot",
    "text": "##sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10)) +\n  geom_sf(data = co_cities, aes(size = population/1e5), col = \"red\") +\n  theme_linedraw()"
  },
  {
    "objectID": "slides/week-2-1.html#sf-an-ggplot-4",
    "href": "slides/week-2-1.html#sf-an-ggplot-4",
    "title": "Week 2",
    "section": "##sf an ggplot",
    "text": "##sf an ggplot\n\n\n\nggplot() +\n  geom_sf(data = co, aes(fill = aland/1e10)) +\n  geom_sf(data = co_cities, aes(size = population/1e5), col = \"red\") +\n  theme_linedraw() +\n  labs(title = \"California Counties: Land Area\",\n       size = \"Population \\n(100,000)\",\n       fill = \"Acres \\n(billions)\")"
  },
  {
    "objectID": "slides/week2-2.html#feature-resoloved-and-combined",
    "href": "slides/week2-2.html#feature-resoloved-and-combined",
    "title": "Week 2-2",
    "section": "1 feature: resoloved and combined:",
    "text": "1 feature: resoloved and combined:\n\n\n\nst_cast / st_union work on sfg, sfc, and sf objects:\n\n\nus_c_ml = st_combine(conus) |&gt;\n  st_cast(\"MULTILINESTRING\")\n   \nus_u_ml = st_union(conus) |&gt;\n  st_cast(\"MULTILINESTRING\")"
  },
  {
    "objectID": "slides/week2-2.html#determine-the-3-closest-states",
    "href": "slides/week2-2.html#determine-the-3-closest-states",
    "title": "Week 2-2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus\n\n\n\n#&gt; Simple feature collection with 49 features and 12 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    statefp  statens    affgeoid geoid stusps       name lsad        aland\n#&gt; 1       06 01779778 0400000US06    06     CA California   00 403671196038\n#&gt; 2       55 01779806 0400000US55    55     WI  Wisconsin   00 140292246684\n#&gt; 3       16 01779783 0400000US16    16     ID      Idaho   00 214049923496\n#&gt; 4       27 00662849 0400000US27    27     MN  Minnesota   00 206232157570\n#&gt; 5       19 01779785 0400000US19    19     IA       Iowa   00 144659688848\n#&gt; 6       29 01779791 0400000US29    29     MO   Missouri   00 178052563675\n#&gt; 7       24 01714934 0400000US24    24     MD   Maryland   00  25151895765\n#&gt; 8       41 01155107 0400000US41    41     OR     Oregon   00 248628426864\n#&gt; 9       26 01779789 0400000US26    26     MI   Michigan   00 146614604273\n#&gt; 10      30 00767982 0400000US30    30     MT    Montana   00 376973673895\n#&gt;          awater state_name state_abbr jurisdiction_type\n#&gt; 1   20294133830 California         CA             state\n#&gt; 2   29343721650  Wisconsin         WI             state\n#&gt; 3    2391577745      Idaho         ID             state\n#&gt; 4   18949864226  Minnesota         MN             state\n#&gt; 5    1085996889       Iowa         IA             state\n#&gt; 6    2487215790   Missouri         MO             state\n#&gt; 7    6979171386   Maryland         MD             state\n#&gt; 8    6170953359     Oregon         OR             state\n#&gt; 9  103872203398   Michigan         MI             state\n#&gt; 10   3866689601    Montana         MT             state\n#&gt;                          geometry\n#&gt; 1  MULTIPOLYGON (((-118.594 33...\n#&gt; 2  MULTIPOLYGON (((-86.93428 4...\n#&gt; 3  MULTIPOLYGON (((-117.243 44...\n#&gt; 4  MULTIPOLYGON (((-97.22904 4...\n#&gt; 5  MULTIPOLYGON (((-96.62187 4...\n#&gt; 6  MULTIPOLYGON (((-95.76564 4...\n#&gt; 7  MULTIPOLYGON (((-76.04621 3...\n#&gt; 8  MULTIPOLYGON (((-124.5524 4...\n#&gt; 9  MULTIPOLYGON (((-84.61622 4...\n#&gt; 10 MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week2-2.html#determine-the-3-closest-states-1",
    "href": "slides/week2-2.html#determine-the-3-closest-states-1",
    "title": "Week 2-2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name)\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry\n#&gt; 1  California MULTIPOLYGON (((-118.594 33...\n#&gt; 2   Wisconsin MULTIPOLYGON (((-86.93428 4...\n#&gt; 3       Idaho MULTIPOLYGON (((-117.243 44...\n#&gt; 4   Minnesota MULTIPOLYGON (((-97.22904 4...\n#&gt; 5        Iowa MULTIPOLYGON (((-96.62187 4...\n#&gt; 6    Missouri MULTIPOLYGON (((-95.76564 4...\n#&gt; 7    Maryland MULTIPOLYGON (((-76.04621 3...\n#&gt; 8      Oregon MULTIPOLYGON (((-124.5524 4...\n#&gt; 9    Michigan MULTIPOLYGON (((-84.61622 4...\n#&gt; 10    Montana MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week2-2.html#determine-the-3-closest-states-2",
    "href": "slides/week2-2.html#determine-the-3-closest-states-2",
    "title": "Week 2-2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name) %&gt;%\n  mutate(dist = st_distance(., denver_sf))\n\n\n\n#&gt; Simple feature collection with 49 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry          dist\n#&gt; 1  California MULTIPOLYGON (((-118.594 33... 1000950.5 [m]\n#&gt; 2   Wisconsin MULTIPOLYGON (((-86.93428 4... 1146522.6 [m]\n#&gt; 3       Idaho MULTIPOLYGON (((-117.243 44...  567809.5 [m]\n#&gt; 4   Minnesota MULTIPOLYGON (((-97.22904 4...  823100.9 [m]\n#&gt; 5        Iowa MULTIPOLYGON (((-96.62187 4...  773889.8 [m]\n#&gt; 6    Missouri MULTIPOLYGON (((-95.76564 4...  789142.1 [m]\n#&gt; 7    Maryland MULTIPOLYGON (((-76.04621 3... 2174383.4 [m]\n#&gt; 8      Oregon MULTIPOLYGON (((-124.5524 4... 1041819.1 [m]\n#&gt; 9    Michigan MULTIPOLYGON (((-84.61622 4... 1401455.7 [m]\n#&gt; 10    Montana MULTIPOLYGON (((-116.0492 4...  585011.2 [m]"
  },
  {
    "objectID": "slides/week2-2.html#determine-the-3-closest-states-3",
    "href": "slides/week2-2.html#determine-the-3-closest-states-3",
    "title": "Week 2-2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name) %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3)\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado      0.0 [m] MULTIPOLYGON (((-109.06 38....\n#&gt; 2    Wyoming 139988.4 [m] MULTIPOLYGON (((-111.0569 4...\n#&gt; 3   Nebraska 161243.2 [m] MULTIPOLYGON (((-104.0531 4..."
  },
  {
    "objectID": "slides/week2-2.html#determine-the-3-closest-states-4",
    "href": "slides/week2-2.html#determine-the-3-closest-states-4",
    "title": "Week 2-2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name) %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3) -&gt;\n  near3\n\n\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado      0.0 [m] MULTIPOLYGON (((-109.06 38....\n#&gt; 2    Wyoming 139988.4 [m] MULTIPOLYGON (((-111.0569 4...\n#&gt; 3   Nebraska 161243.2 [m] MULTIPOLYGON (((-104.0531 4...\n\n\nThat‚Äôs close, but the distance to Colorado is 0, that‚Äôs not a state border."
  },
  {
    "objectID": "slides/week2-2.html#geometry-selection",
    "href": "slides/week2-2.html#geometry-selection",
    "title": "Week 2-2",
    "section": "Geometry Selection",
    "text": "Geometry Selection\n\nPolygon (therefore MULTIPOLGYGONS) describe areas!\nThe distance to a point in a polygon to that polygon is 0."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear represnetation:",
    "text": "To determine distance to border we need a linear represnetation:\n\n\n\nconus\n\n\n\n#&gt; Simple feature collection with 49 features and 12 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    statefp  statens    affgeoid geoid stusps       name lsad        aland\n#&gt; 1       06 01779778 0400000US06    06     CA California   00 403671196038\n#&gt; 2       55 01779806 0400000US55    55     WI  Wisconsin   00 140292246684\n#&gt; 3       16 01779783 0400000US16    16     ID      Idaho   00 214049923496\n#&gt; 4       27 00662849 0400000US27    27     MN  Minnesota   00 206232157570\n#&gt; 5       19 01779785 0400000US19    19     IA       Iowa   00 144659688848\n#&gt; 6       29 01779791 0400000US29    29     MO   Missouri   00 178052563675\n#&gt; 7       24 01714934 0400000US24    24     MD   Maryland   00  25151895765\n#&gt; 8       41 01155107 0400000US41    41     OR     Oregon   00 248628426864\n#&gt; 9       26 01779789 0400000US26    26     MI   Michigan   00 146614604273\n#&gt; 10      30 00767982 0400000US30    30     MT    Montana   00 376973673895\n#&gt;          awater state_name state_abbr jurisdiction_type\n#&gt; 1   20294133830 California         CA             state\n#&gt; 2   29343721650  Wisconsin         WI             state\n#&gt; 3    2391577745      Idaho         ID             state\n#&gt; 4   18949864226  Minnesota         MN             state\n#&gt; 5    1085996889       Iowa         IA             state\n#&gt; 6    2487215790   Missouri         MO             state\n#&gt; 7    6979171386   Maryland         MD             state\n#&gt; 8    6170953359     Oregon         OR             state\n#&gt; 9  103872203398   Michigan         MI             state\n#&gt; 10   3866689601    Montana         MT             state\n#&gt;                          geometry\n#&gt; 1  MULTIPOLYGON (((-118.594 33...\n#&gt; 2  MULTIPOLYGON (((-86.93428 4...\n#&gt; 3  MULTIPOLYGON (((-117.243 44...\n#&gt; 4  MULTIPOLYGON (((-97.22904 4...\n#&gt; 5  MULTIPOLYGON (((-96.62187 4...\n#&gt; 6  MULTIPOLYGON (((-95.76564 4...\n#&gt; 7  MULTIPOLYGON (((-76.04621 3...\n#&gt; 8  MULTIPOLYGON (((-124.5524 4...\n#&gt; 9  MULTIPOLYGON (((-84.61622 4...\n#&gt; 10 MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-1",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-1",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear represnetation:",
    "text": "To determine distance to border we need a linear represnetation:\n\n\n\nconus |&gt;\n  select(state_name)\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry\n#&gt; 1  California MULTIPOLYGON (((-118.594 33...\n#&gt; 2   Wisconsin MULTIPOLYGON (((-86.93428 4...\n#&gt; 3       Idaho MULTIPOLYGON (((-117.243 44...\n#&gt; 4   Minnesota MULTIPOLYGON (((-97.22904 4...\n#&gt; 5        Iowa MULTIPOLYGON (((-96.62187 4...\n#&gt; 6    Missouri MULTIPOLYGON (((-95.76564 4...\n#&gt; 7    Maryland MULTIPOLYGON (((-76.04621 3...\n#&gt; 8      Oregon MULTIPOLYGON (((-124.5524 4...\n#&gt; 9    Michigan MULTIPOLYGON (((-84.61622 4...\n#&gt; 10    Montana MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-2",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-2",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear represnetation:",
    "text": "To determine distance to border we need a linear represnetation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\")\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry\n#&gt; 1  California MULTILINESTRING ((-118.594 ...\n#&gt; 2   Wisconsin MULTILINESTRING ((-86.93428...\n#&gt; 3       Idaho MULTILINESTRING ((-117.243 ...\n#&gt; 4   Minnesota MULTILINESTRING ((-97.22904...\n#&gt; 5        Iowa MULTILINESTRING ((-96.62187...\n#&gt; 6    Missouri MULTILINESTRING ((-95.76564...\n#&gt; 7    Maryland MULTILINESTRING ((-76.04621...\n#&gt; 8      Oregon MULTILINESTRING ((-124.5524...\n#&gt; 9    Michigan MULTILINESTRING ((-84.61622...\n#&gt; 10    Montana MULTILINESTRING ((-116.0492..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-3",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-3",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear represnetation:",
    "text": "To determine distance to border we need a linear represnetation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf))\n\n\n\n#&gt; Simple feature collection with 49 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry          dist\n#&gt; 1  California MULTILINESTRING ((-118.594 ... 1000950.5 [m]\n#&gt; 2   Wisconsin MULTILINESTRING ((-86.93428... 1146522.6 [m]\n#&gt; 3       Idaho MULTILINESTRING ((-117.243 ...  567809.5 [m]\n#&gt; 4   Minnesota MULTILINESTRING ((-97.22904...  823100.9 [m]\n#&gt; 5        Iowa MULTILINESTRING ((-96.62187...  773889.8 [m]\n#&gt; 6    Missouri MULTILINESTRING ((-95.76564...  789142.1 [m]\n#&gt; 7    Maryland MULTILINESTRING ((-76.04621... 2174383.4 [m]\n#&gt; 8      Oregon MULTILINESTRING ((-124.5524... 1041819.1 [m]\n#&gt; 9    Michigan MULTILINESTRING ((-84.61622... 1401455.7 [m]\n#&gt; 10    Montana MULTILINESTRING ((-116.0492...  585011.2 [m]"
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-4",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-4",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear represnetation:",
    "text": "To determine distance to border we need a linear represnetation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3)\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado 139988.4 [m] MULTILINESTRING ((-109.06 3...\n#&gt; 2    Wyoming 139988.4 [m] MULTILINESTRING ((-111.0569...\n#&gt; 3   Nebraska 161243.2 [m] MULTILINESTRING ((-104.0531..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-5",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-represnetation-5",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear represnetation:",
    "text": "To determine distance to border we need a linear represnetation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3) -&gt;\n  near3\n\n\n\n\n\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado 139988.4 [m] MULTILINESTRING ((-109.06 3...\n#&gt; 2    Wyoming 139988.4 [m] MULTILINESTRING ((-111.0569...\n#&gt; 3   Nebraska 161243.2 [m] MULTILINESTRING ((-104.0531...\n\n\nGood. However, we were only interested in the distance to the closest border not to ALL boarders. Therefore we calculated 48 (49 - 1) more distances then needed!\nWhile this is not to complex for 1 &lt;-&gt; 49 features imagine we had 28,000+ (like) your lab!\nThat would result in 1,344,000 more calculations then needed ‚Ä¶"
  },
  {
    "objectID": "slides/week2-2.html#coordinate-systems-1",
    "href": "slides/week2-2.html#coordinate-systems-1",
    "title": "Week 2-2",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n\nCoordinate Reference Systems (CRS) defines how spatial features relate to the surface of the Earth.\nCRSs are either geographic or projected‚Ä¶\nCRSs are measurement units for coordinates:"
  },
  {
    "objectID": "slides/week2-2.html#geographic-coordinate-systms-gcs",
    "href": "slides/week2-2.html#geographic-coordinate-systms-gcs",
    "title": "Week 2-2",
    "section": "Geographic Coordinate Systms (GCS)",
    "text": "Geographic Coordinate Systms (GCS)\nA GCS identifies locations on the curved surface of the earth.\nLocations are measured in angular units from the center of the earth relative to the plane defined by the equator and the plane defined by the prime meridian.\nThe vertical angle describes the latitude and the horizontal angle the longitude\nIn most coordinate systems, the North-South and East-West directions are encoded as +/-.\nNorth and East are positive (+) and South and West are negative (-) sign.\nA GCS is defined by 3 components:\n\nan ellipsoid\na geoid\na datum"
  },
  {
    "objectID": "slides/week2-2.html#sphere-and-ellipsoid",
    "href": "slides/week2-2.html#sphere-and-ellipsoid",
    "title": "Week 2-2",
    "section": "Sphere and Ellipsoid",
    "text": "Sphere and Ellipsoid\n\nAssuming that the earth is a perfect sphere simplifies calculations and works for small-scale maps (maps that show a large area of the earth).\nBut ‚Ä¶ the earth is not a sphere do to its rotation inducing a centripetal force along the equator.\nThis results in an equatorial axis that is roughly 21 km longer than the polar axis.\nTo account for this, the earth is modeled as an ellipsoid (slighty squished sphere) defined by two radii:\n\nthe semi-major axis (along the equatorial radius)\nthe semi-minor axis (along the polar radius)"
  },
  {
    "objectID": "slides/week2-2.html#datum",
    "href": "slides/week2-2.html#datum",
    "title": "Week 2-2",
    "section": "Datum",
    "text": "Datum\n\nSo how are we to reconcile our need to work with a (simple) mathematical model of the earth‚Äôs shape with the undulating nature of the geoid?\nWe align the geoid with the ellipsoid to map the the earths departures from the smooth assumption\nThe alignment can be local where the ellipsoid surface is closely fit to the geoid at a particular location on the earth‚Äôs surface\n\nor\n\ngeocentric where the ellipsoid is aligned with the center of the earth.\nThe alignment of the smooth ellipsoid to the geoid model defines a datum."
  },
  {
    "objectID": "slides/week2-2.html#local-datums",
    "href": "slides/week2-2.html#local-datums",
    "title": "Week 2-2",
    "section": "Local Datums",
    "text": "Local Datums\n\nThere are many local datums to choose from\nThe choice of datum is largely driven by the location\nWhen working in the USA, a the North American Datum of 1927 (or NAD27 for short) is standard\n\nNAD27 is not well suited for other parts of the world.\n\n\nExamples of common local datums are shown in the following table:"
  },
  {
    "objectID": "slides/week2-2.html#projected-coordinate-systems",
    "href": "slides/week2-2.html#projected-coordinate-systems",
    "title": "Week 2-2",
    "section": "Projected Coordinate Systems",
    "text": "Projected Coordinate Systems\n\nThe surface of the earth is curved but maps (and to data GIS) is flat.\nA projected coordinate system (PCS) is a reference system for identifying locations and measuring features on a flat (2D) surfaces. I\nProjected coordinate systems have an origin, an x axis, a y axis, and a linear unit of measure.\nGoing from a GCS to a PCS requires mathematical transformations.\nThere are three main groups of projection types:\n\nconic\ncylindrical\nplanar"
  },
  {
    "objectID": "slides/week2-2.html#projection-types",
    "href": "slides/week2-2.html#projection-types",
    "title": "Week 2-2",
    "section": "Projection Types:",
    "text": "Projection Types:\n\n\nIn all cases, distortion is minimized at the line/point of tangency (denoted by black line/point)\nDistortions are minimized along the tangency lines and increase with the distance from those lines."
  },
  {
    "objectID": "slides/week2-2.html#plannar",
    "href": "slides/week2-2.html#plannar",
    "title": "Week 2-2",
    "section": "Plannar",
    "text": "Plannar\n\nA planar projection projects data onto a flat surface touching the globe at a point or along 1 line of tangency.\nTypically used to map polar regions."
  },
  {
    "objectID": "slides/week2-2.html#cylindrical",
    "href": "slides/week2-2.html#cylindrical",
    "title": "Week 2-2",
    "section": "Cylindrical",
    "text": "Cylindrical\n\nA cylindrical projection maps the surface onto a cylinder.\nThis projection could also be created by touching the Earth‚Äôs surface along 1 or 2 lines of tangency\nMost often when mapping the entire world."
  },
  {
    "objectID": "slides/week2-2.html#conic",
    "href": "slides/week2-2.html#conic",
    "title": "Week 2-2",
    "section": "Conic",
    "text": "Conic\nIn a conic projection, the Earth‚Äôs surface is projected onto a cone along 1 or 2 lines of tangency\nTherefore, it is the best suited for maps of mid-latitude areas."
  },
  {
    "objectID": "slides/week2-2.html#spatial-properties",
    "href": "slides/week2-2.html#spatial-properties",
    "title": "Week 2-2",
    "section": "Spatial Properties",
    "text": "Spatial Properties\n\nAll projections distort real-world geographic features.\nThink about trying to unpeel an orange while preserving the skin\n\nThe four spatial properties that are subject to distortion are: shape, area, distance and direction\n\nA map that preserves shape is called conformal;\none that preserves area is called equal-area;\none that preserves distance is called equidistant\none that preserves direction is called azimuthal\nEach map projection can preserve only one or two of the four spatial properties.\nOften, projections are named after the spatial properties they preserve.\nWhen working with small-scale (large area) maps and when multiple spatial properties are needed, it is best to break the analyses across projections to minimize errors associated with spatial distortion."
  },
  {
    "objectID": "slides/week2-2.html#setting-crsspcss",
    "href": "slides/week2-2.html#setting-crsspcss",
    "title": "Week 2-2",
    "section": "Setting CRSs/PCSs",
    "text": "Setting CRSs/PCSs\n\nWe saw that sfc objects have two attributes to store a CRS: epsg and proj4string\n\n\nst_geometry(conus)\n#&gt; Geometry set for 49 features \n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 5 geometries:\n\n\nThis implies that all geometries in a geometry list-column (sfc) must have the same CRS.\nproj4string is a generic, string-based description of a CRS, understood by PROJ\nIt defines projection types and parameter values for particular projections,\nAs a result it can cover an infinite amount of different projections.\nepsg is the integer ID for a known CRS that can be resolved into a proj4string.\n\nThis is somewhat equivalent to the idea that a 6-digit FIP code can be resolved to a state/county pair\n\nSome proj4string values can resolved back into their corresponding epsg ID, but this does not always work.\nThe importance of having epsg values stored with data besides proj4string values is that the epsg refers to particular, well-known CRS, whose parameters may change (improve) over time\nfixing only the proj4string may remove the possibility to benefit from such improvements, and limit some of the provenance of datasets (but may help reproducibility)"
  },
  {
    "objectID": "slides/week2-2.html#proj4-coordinate-syntax",
    "href": "slides/week2-2.html#proj4-coordinate-syntax",
    "title": "Week 2-2",
    "section": "PROJ4 coordinate syntax",
    "text": "PROJ4 coordinate syntax\nThe PROJ4 syntax contains a list of parameters, each prefixed with the + character.\nA list of some PROJ4 parameters follows and the full list can be found here:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n+a\nSemi-major radius of the ellipsoid axis\n\n\n+b\nSemi-minor radius of the ellipsoid axis\n\n\n+datum\nDatum name\n\n\n+ellps\nEllipsoid name\n\n\n+lat_0\nLatitude of origin\n\n\n+lat_1\nLatitude of first standard parallel\n\n\n+lat_2\nLatitude of second standard parallel\n\n\n+lat_ts\nLatitude of true scale\n\n\n+lon_0\nCentral meridian\n\n\n+over\nAllow longitude output outside -180 to 180 range, disables wrapping\n\n\n+proj\nProjection name\n\n\n+south\nDenotes southern hemisphere UTM zone\n\n\n+units\nmeters, US survey feet, etc.\n\n\n+x_0\nFalse easting\n\n\n+y_0\nFalse northing\n\n\n+zone\nUTM zone"
  },
  {
    "objectID": "slides/week2-2.html#transform-and-retrive",
    "href": "slides/week2-2.html#transform-and-retrive",
    "title": "Week 2-2",
    "section": "Transform and retrive",
    "text": "Transform and retrive\n\n\n\nst_crs(conus)$epsg\n#&gt; [1] 4326\nst_crs(conus)$proj4string\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs\"\nst_crs(conus)$datum\n#&gt; [1] \"WGS84\"\n\n\n\nconus5070 &lt;- st_transform(conus, 5070)\n\nst_crs(conus5070)$epsg\n#&gt; [1] 5070\nst_crs(conus5070)$proj4string\n#&gt; [1] \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\nst_crs(conus5070)$datum\n#&gt; [1] \"NAD83\""
  },
  {
    "objectID": "slides/week2-2.html#revisit-denver",
    "href": "slides/week2-2.html#revisit-denver",
    "title": "Week 2-2",
    "section": "Revisit Denver",
    "text": "Revisit Denver\n\necho -104.9903 39.7392 | proj +proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\n#&gt; -723281.88   6827.29\n\n\nred = false origin : blue = Denver"
  },
  {
    "objectID": "slides/week2-2.html#geodesic-geometries",
    "href": "slides/week2-2.html#geodesic-geometries",
    "title": "Week 2-2",
    "section": "Geodesic geometries",
    "text": "Geodesic geometries\n\nPCSs introduce errors in their geometric measurements because the distance between two points on an ellipsoid is difficult to replicate on a projected coordinate system unless these points are close to one another.\nIn most cases, such errors other sources of error in the feature representation outweigh measurement errors made in a PCS making them tolorable.\n\nHowever, if the domain of analysis is large (i.e.¬†the North American continent), then the measurement errors associated with a projected coordinate system may no longer be acceptable.\nA way to circumvent projected coordinate system limitations is to adopt a geodesic solution."
  },
  {
    "objectID": "slides/week2-2.html#geodesic-measurments",
    "href": "slides/week2-2.html#geodesic-measurments",
    "title": "Week 2-2",
    "section": "Geodesic Measurments",
    "text": "Geodesic Measurments\n\nA geodesic distance is the shortest distance between two points on an ellipsoid\nA geodesic area measurement is one that is measured on an ellipsoid.\nSuch measurements are independent of the underlying projected coordinate system.\nWhy does this matter?\nCompare the distances measured between Santa Barbara and Amsterdam. The blue line represents the shortest distance between the two points on a planar coordinate system. The red line as measured on a ellipsoid."
  },
  {
    "objectID": "slides/week2-2.html#distance-example",
    "href": "slides/week2-2.html#distance-example",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB"
  },
  {
    "objectID": "slides/week2-2.html#distance-example-1",
    "href": "slides/week2-2.html#distance-example-1",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)"
  },
  {
    "objectID": "slides/week2-2.html#distance-example-2",
    "href": "slides/week2-2.html#distance-example-2",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)"
  },
  {
    "objectID": "slides/week2-2.html#distance-example-3",
    "href": "slides/week2-2.html#distance-example-3",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0"
  },
  {
    "objectID": "slides/week2-2.html#distance-example-4",
    "href": "slides/week2-2.html#distance-example-4",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000"
  },
  {
    "objectID": "slides/week2-2.html#distance-example-5",
    "href": "slides/week2-2.html#distance-example-5",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 4017987\n#&gt; 2 4017987       0"
  },
  {
    "objectID": "slides/week2-2.html#distance-example-6",
    "href": "slides/week2-2.html#distance-example-6",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n# Equal Distance\nst_distance(st_transform(pts, eqds))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 4017987\n#&gt; 2 4017987       0\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 3823549\n#&gt; 2 3823549       0"
  },
  {
    "objectID": "slides/week2-2.html#distance-example-7",
    "href": "slides/week2-2.html#distance-example-7",
    "title": "Week 2-2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n# Equal Distance\nst_distance(st_transform(pts, eqds))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 4017987\n#&gt; 2 4017987       0\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 3823549\n#&gt; 2 3823549       0"
  },
  {
    "objectID": "slides/week2-2.html#area-example-conus",
    "href": "slides/week2-2.html#area-example-conus",
    "title": "Week 2-2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")"
  },
  {
    "objectID": "slides/week2-2.html#area-example-conus-1",
    "href": "slides/week2-2.html#area-example-conus-1",
    "title": "Week 2-2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))"
  },
  {
    "objectID": "slides/week2-2.html#area-example-conus-2",
    "href": "slides/week2-2.html#area-example-conus-2",
    "title": "Week 2-2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df)"
  },
  {
    "objectID": "slides/week2-2.html#area-example-conus-3",
    "href": "slides/week2-2.html#area-example-conus-3",
    "title": "Week 2-2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) ))"
  },
  {
    "objectID": "slides/week2-2.html#area-example-conus-4",
    "href": "slides/week2-2.html#area-example-conus-4",
    "title": "Week 2-2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) )) +\n  theme_linedraw()"
  },
  {
    "objectID": "slides/week2-2.html#area-example-conus-5",
    "href": "slides/week2-2.html#area-example-conus-5",
    "title": "Week 2-2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) )) +\n  theme_linedraw() +\n  labs(x = \"SRS\", y = \"m2\")"
  },
  {
    "objectID": "slides/week2-2.html#units-in-sf",
    "href": "slides/week2-2.html#units-in-sf",
    "title": "Week 2-2",
    "section": "Units in sf",
    "text": "Units in sf\n\nThe CRS in sf encodes the units of measurement relating to spatial features\nWhere possible geometric operations such as st_distance(), st_length() and st_area() report results with a units attribute appropriate for the CRS:\nThis can be both handy and very confusing for those new to it. Consider the following:\n\n\n(l = sum(st_length(conus)))\n#&gt; 94980149 [m]\n(a = sum(st_area(conus)))\n#&gt; 7.83761e+12 [m^2]"
  },
  {
    "objectID": "slides/week2-2.html#units-are-a-class",
    "href": "slides/week2-2.html#units-are-a-class",
    "title": "Week 2-2",
    "section": "Units are a class",
    "text": "Units are a class\n\nunits are an S3 data object with attribute information and ‚Äúrules of engagement‚Äù\n\n\nclass(st_length(conus)) \n#&gt; [1] \"units\"\nattributes(st_length(conus)) |&gt; unlist()\n#&gt; units.numerator           class \n#&gt;             \"m\"         \"units\"\n\nst_length(conus) + 100\n#&gt; Error in Ops.units(st_length(conus), 100): both operands of the expression should be \"units\" objects\n\nconus |&gt; \n  mutate(area = st_area(.)) |&gt; \n  ggplot(aes(x = name, y = area)) + \n  geom_col()\n#&gt; Error in `stopifnot()`:\n#&gt; ‚Ñπ In argument: `area = st_area(.)`.\n#&gt; Caused by error:\n#&gt; ! object '.' not found"
  },
  {
    "objectID": "slides/week2-2.html#unit-values-can-be-stripped-of-their-attributes-if-need-be",
    "href": "slides/week2-2.html#unit-values-can-be-stripped-of-their-attributes-if-need-be",
    "title": "Week 2-2",
    "section": "Unit values can be stripped of their attributes if need be:",
    "text": "Unit values can be stripped of their attributes if need be:\n\n# Via drop_units\n(units::drop_units(sum(st_length(conus))))\n#&gt; [1] 94980149\n\n# Via casting\n(as.numeric(sum(st_length(conus))))\n#&gt; [1] 94980149"
  },
  {
    "objectID": "slides/week2-2.html#picking-up-again",
    "href": "slides/week2-2.html#picking-up-again",
    "title": "Week 2-2",
    "section": "Picking up again ‚Ä¶",
    "text": "Picking up again ‚Ä¶\nYesterday, we discussed the simple feature standard\n\nGeometries (type, dimension, and structure)\n\n- Empty, Valid, Simple\n\nEncoding (WKT & WKB)\nA set of operations\n\nAnd the implementation of the simple features standard in R\n\nsfg: a single feature geometry\nsfc: a set of geometries (sfg) stored as a list\nsf: a sfc list joined with a data.frame (attributes)\n\nThis R implementation is ideal/special because it achieves the simple feature abstract goal of:\n\n‚ÄúA simple feature is defined by the OpenGIS Abstract specification to have both spatial and non-spatial attributes‚Ä¶‚Äù - standard.\n\nThe shapefile/GIS traditional GIS view does not do this and seperates geometry (shp), from projection (prj), from data (dbf) and relates them through an shx file"
  },
  {
    "objectID": "slides/week2-2.html#section",
    "href": "slides/week2-2.html#section",
    "title": "Week 2-2",
    "section": "",
    "text": "Thanks to satellite and computational capabilities our estimates of these radii are be quite precise\n\nThe semi-major axis is 6,378,137 m\nThe semi-minor axis is 6,356,752 m\n\nDifferences in distance along the surfaces of an ellipsoid vs.¬†a perfect sphere are small but measurable (the difference can be as high as 20 km)"
  },
  {
    "objectID": "slides/week2-2.html#integration-with-tidyverse",
    "href": "slides/week2-2.html#integration-with-tidyverse",
    "title": "Week 2-2",
    "section": "Integration with tidyverse",
    "text": "Integration with tidyverse\n\nWe saw how the dplyr verbs still work on an sf object since sf extends the data.frame class\nHow geom_sf support mapping (‚Äúspatial plotting‚Äù) in ggplot\nHow to read spatial data into R via GDAL drivers:\n\nspatial files (read_sf)\nflat files via st_as_sf\n\nIntegration with a few GEOS geometry operations like:\n\nst_combine()\nst_union()"
  },
  {
    "objectID": "slides/week2-2.html#yesterday",
    "href": "slides/week2-2.html#yesterday",
    "title": "Week 2-2",
    "section": "Yesterday ‚Ä¶",
    "text": "Yesterday ‚Ä¶\n\n\n\nconus &lt;-  USAboundaries::us_states() |&gt;\n  filter(!state_name %in% c(\"Puerto Rico\", \n                            \"Alaska\", \n                            \"Hawaii\"))\n\nlength(st_geometry(conus))\n#&gt; [1] 49"
  },
  {
    "objectID": "slides/week2-2.html#so-what",
    "href": "slides/week2-2.html#so-what",
    "title": "Week 2-2",
    "section": "So what?",
    "text": "So what?\nLets imagine we want to know the distance from Denver to the nearest state border:\nTo do this, we need to:\n\n1: define Denver as a geometry in a CRS\n2: determine the correct geometry types / representation\n3: calculate the distance between (1) and (2)"
  },
  {
    "objectID": "slides/week2-2.html#make-denver-in-the-crs-of-our-states",
    "href": "slides/week2-2.html#make-denver-in-the-crs-of-our-states",
    "title": "Week 2-2",
    "section": "1. Make ‚ÄúDenver‚Äù in the CRS of our states",
    "text": "1. Make ‚ÄúDenver‚Äù in the CRS of our states\n\ndenver = data.frame(y = 39.7392, x = -104.9903, name = \"Denver\")\n(denver_sf = st_as_sf(denver, coords = c(\"x\", \"y\"), crs = 4326))\n#&gt; Simple feature collection with 1 feature and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -104.9903 ymin: 39.7392 xmax: -104.9903 ymax: 39.7392\n#&gt; Geodetic CRS:  WGS 84\n#&gt;     name                  geometry\n#&gt; 1 Denver POINT (-104.9903 39.7392)"
  },
  {
    "objectID": "slides/week2-2.html#revisting-the-idea-of-the-feature-level",
    "href": "slides/week2-2.html#revisting-the-idea-of-the-feature-level",
    "title": "Week 2-2",
    "section": "Revisting the idea of the feature level:",
    "text": "Revisting the idea of the feature level:\nA ‚Äúfeature‚Äù can ‚Äúbe part of the whole‚Äù or the whole\n\nA island (POLYGON), or a set of islands acting as 1 unit (MULTIPOLYGON)\nA city (POINT), or a set of cities meeting a condition (MULTIPOINT)\nA road (LINESTRING), or a route (MULTILINESTRING)\nSince we want the distance to the nearest border, regardless of the state. Our feature is the set of borders with preserved boundaries.\nIn other words, a 1 feature MULTILINESTRING\n\n\nst_distance(denver_sf, st_cast(st_combine(conus), \"MULTILINESTRING\"))\n#&gt; Units: [m]\n#&gt;          [,1]\n#&gt; [1,] 139988.4\n\nThe same principle would apply if the question was ‚Äúdistance to national border‚Äù"
  },
  {
    "objectID": "slides/week2-2.html#the-stickness-of-sfc-column",
    "href": "slides/week2-2.html#the-stickness-of-sfc-column",
    "title": "Week 2-2",
    "section": "The stickness of sfc column",
    "text": "The stickness of sfc column\n\nA simple features object (sf) is the connection of a sfc list-column and data.frame of attributes\n\n\n\nThis binding is unique compared to other column bindings built with things like\n\ndplyr::bind_cols()\ncbind()\ndo.call(cbind, list())"
  },
  {
    "objectID": "slides/week2-2.html#the-stickness-of-sfc-column-1",
    "href": "slides/week2-2.html#the-stickness-of-sfc-column-1",
    "title": "Week 2-2",
    "section": "The stickness of sfc column",
    "text": "The stickness of sfc column\n\nGeometry columns are ‚Äústicky‚Äù meaning they persist through data manipulation:\n\n\nUSAboundaries::us_states() |&gt; \n  select(name) |&gt; \n  slice(1:2)\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.4096 ymin: 32.53416 xmax: -86.80587 ymax: 47.05468\n#&gt; Geodetic CRS:  WGS 84\n#&gt;         name                       geometry\n#&gt; 1 California MULTIPOLYGON (((-118.594 33...\n#&gt; 2  Wisconsin MULTIPOLYGON (((-86.93428 4...\n\nDropping the geometry column requires dropping the geometry via sf:\n\nUSAboundaries::us_states() |&gt; \n  st_drop_geometry() |&gt; #&lt;&lt;\n  select(name) |&gt; \n  slice(1:2)\n#&gt;         name\n#&gt; 1 California\n#&gt; 2  Wisconsin\n\nOr cohersing the sf object to a data.frame:\n\nUSAboundaries::us_states() |&gt; \n  as.data.frame() |&gt; #&lt;&lt;\n  select(name) |&gt; \n  slice(1:2)\n#&gt;         name\n#&gt; 1 California\n#&gt; 2  Wisconsin"
  },
  {
    "objectID": "slides/week2-2.html#section-1",
    "href": "slides/week2-2.html#section-1",
    "title": "Week 2-2",
    "section": "",
    "text": "Local datum\nAcronym\nBest for\nComment\n\n\n\n\nNorth American Datum of 1927\nNAD27\nContinental US\nThis is an old datum but still prevalent\n\n\nEuropean Datum of 1950\nED50\nWestern Europe\nDeveloped after World War II and still quite popular\n\n\nWorld Geodetic System 1972\nWGS72\nGlobal\nDeveloped by the Department of Defense."
  },
  {
    "objectID": "slides/week2-2.html#coordinate-systems",
    "href": "slides/week2-2.html#coordinate-systems",
    "title": "Week 2-2",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n\nWhat makes a feature geometry spatial is the reference system‚Ä¶"
  },
  {
    "objectID": "slides/week2-2.html#sf-tools",
    "href": "slides/week2-2.html#sf-tools",
    "title": "Week 2-2",
    "section": "sf tools",
    "text": "sf tools\nIn sf we have three tools for exploring, define, and changing CRS systems:\n\nst_crs : Retrieve coordinate reference system from sf or sfc object\nst_set_crs : Set or replace coordinate reference system from object\nst_transform : Transform or convert coordinates of simple feature\nAgain, ‚Äúst‚Äù (like PostGIS) denotes it is an operation that can work on a ‚Äù s patial t ype ‚Äù"
  },
  {
    "objectID": "slides/week2-2.html#section-2",
    "href": "slides/week2-2.html#section-2",
    "title": "Week 2-2",
    "section": "",
    "text": "Geocentric datum\nAcronym\nBest for\nComment\n\n\n\n\nNorth American Datum of 1983\nNAD83\nContinental US\nThis is one of the most popular modern datums for the contiguous US.\n\n\nEuropean Terrestrial Reference System 1989\nETRS89\nWestern Europe\nThis is the most popular modern datum for much of Europe.\n\n\nWorld Geodetic System 1984\nWGS84\nGlobal\nDeveloped by the Department of Defense.\n\n\n\n\n\n\n\n\n\nNote\n\n\nNAD 27 is based on Clarke Ellipsoid of 1866 which is calculated by manual surveying. NAD83 is based on the Geodetic Reference System (GRS) of 1980."
  },
  {
    "objectID": "slides/week2-2.html#geoid",
    "href": "slides/week2-2.html#geoid",
    "title": "Week 2-2",
    "section": "Geoid",
    "text": "Geoid\n\nThe ellipsoid gives us the earths form as a perfectly smooth object\nBut ‚Ä¶ the earth is not perfectly smooth\nDeviations from the perfect sphere are measurable and can influence measurements.\nA geoid is a mathematical model fore representing these deviations\n\nWe are not talking about mountains and ocean trenches but the earth‚Äôs gravitational potential which is tied to the flow of the earth‚Äôs hot and fluid core.\nTherefore the geoid is constantly changing, albeit a large temporal scale.\n\nThe measurement and representation of the earth‚Äôs shape is at the heart of geodesy\n\n\nNASA‚Äôs geoid models"
  },
  {
    "objectID": "slides/week2-2.html#section-3",
    "href": "slides/week2-2.html#section-3",
    "title": "Week 2-2",
    "section": "",
    "text": "WGS84\nEPSG: 4326\nPROJ4: +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n- projection name: longlat\n- Latitude of origin: WGS84\n- Longitude of origin: WGS84\n\nWGS84\nEPSG: 5070\n\"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\n- projection name: aea (Albers Equal Area)\n- Latitude of origin: 23\n- Longitude of origin: -96\n- Latitude of first standard parallel: 29.5\n- Latitude of second standard parallel: 45.5\n- False Easting: 0\n- False Northing: 0\n- Datum: NAD83\n- Units: m"
  },
  {
    "objectID": "slides/week2-2.html#geocentric-datum",
    "href": "slides/week2-2.html#geocentric-datum",
    "title": "Week 2-2",
    "section": "Geocentric Datum",
    "text": "Geocentric Datum\n\nMany modern datums use a geocentric alignment\n\nWorld Geodetic Survey for 1984 (WGS84)\nNorth American Datums of 1983 (NAD83)\n\nMost popular geocentric datums use the WGS84 ellipsoid or the GRS80 ellipsoid which share nearly identical semi-major and semi-minor axes"
  },
  {
    "objectID": "slides/week2-2.html#section-4",
    "href": "slides/week2-2.html#section-4",
    "title": "Week 2-2",
    "section": "",
    "text": "the geodesic distance looks weird given its curved appearance on the projected map.\nthis curvature is a byproduct of the current reference system‚Äôs increasing distance distortion as one moves towards the pole!\nWe can display the geodesic and planar distance on a 3D globe (or a projection that mimics the view of the 3D earth)."
  },
  {
    "objectID": "slides/week2-2.html#building-a-gcs",
    "href": "slides/week2-2.html#building-a-gcs",
    "title": "Week 2-2",
    "section": "Building a GCS",
    "text": "Building a GCS\n\nSo, a GCS is defined by the ellipsoid model and its alignment to the geoid defining the datum.\nSmooth Sphere - Mathmatical Geoid (in angular units)"
  },
  {
    "objectID": "slides/week2-2.html",
    "href": "slides/week2-2.html",
    "title": "Week 2-2",
    "section": "",
    "text": "Yesterday, we discussed the simple feature standard\n\nGeometries (type, dimension, and structure)\n\n- Empty, Valid, Simple\n\nEncoding (WKT & WKB)\nA set of operations\n\nAnd the implementation of the simple features standard in R\n\nsfg: a single feature geometry\nsfc: a set of geometries (sfg) stored as a list\nsf: a sfc list joined with a data.frame (attributes)\n\nThis R implementation is ideal/special because it achieves the simple feature abstract goal of:\n\n‚ÄúA simple feature is defined by the OpenGIS Abstract specification to have both spatial and non-spatial attributes‚Ä¶‚Äù - standard.\n\nThe shapefile/GIS traditional GIS view does not do this and seperates geometry (shp), from projection (prj), from data (dbf) and relates them through an shx file"
  },
  {
    "objectID": "slides/week2-2.html#distances",
    "href": "slides/week2-2.html#distances",
    "title": "Week 2-2",
    "section": "Distances",
    "text": "Distances\n?st_distance"
  },
  {
    "objectID": "slides/week2-2.html#section-6",
    "href": "slides/week2-2.html#section-6",
    "title": "Week 2-2",
    "section": "",
    "text": "We can set units if we do manipulations as well using the units package\n\nunits::set_units(l, \"km\")\n#&gt; 94980.15 [km]\nunits::set_units(l, \"mile\")\n#&gt; 59017.93 [mile]\n\nunits::set_units(a, \"ha\")\n#&gt; 783760974 [ha]\nunits::set_units(a, \"km2\")\n#&gt; 7837610 [km^2]\nunits::set_units(a, \"in2\")\n#&gt; 1.214832e+16 [in^2]"
  },
  {
    "objectID": "slides/week-2-1.html#section",
    "href": "slides/week-2-1.html#section",
    "title": "Week 2",
    "section": "",
    "text": "The remaining geometries 10 are rarer, but increasingly find implementations:\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nCIRCULARSTRING\nThe CIRCULARSTRING is the basic curve type, similar to a LINESTRING in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the center of the arc, i.e., the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LINESTRING. This means that a valid circular string must have an odd number of points greater than 1.\n\n\nCOMPOUNDCURVE\nA compound curve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component.\n\n\nCURVEPOLYGON\nExample compound curve in a curve polygon: CURVEPOLYGON(COMPOUNDCURVE(CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1) )\n\n\nMULTICURVE\nA MultiCurve is a 1-dimensional GeometryCollection whose elements are Curves, it can include linear strings, circular strings or compound strings.\n\n\nMULTISURFACE\nA MultiSurface is a 2-dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system.\n\n\nCURVE\nA Curve is a 1-dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points\n\n\nSURFACE\nA Surface is a 2-dimensional geometric object\n\n\nPOLYHEDRALSURFACE\nA PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments\n\n\nTIN\nA TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches.\n\n\nTRIANGLE\nA Triangle is a polygon with 3 distinct, non-collinear vertices and no interior boundary"
  },
  {
    "objectID": "slides/week2-2.html#section-5",
    "href": "slides/week2-2.html#section-5",
    "title": "Week 2-2",
    "section": "",
    "text": "So if a geodesic measurement is more precise than a planar measurement, why not perform all spatial operations using geodesic geometry?\nThe downside is in its computational requirements.\nIt‚Äôs far more efficient to compute area/distance on a plane than it is on a spheroid.\nThis is because geodesic calculations have no simple algebraic solutions and involve approximations that may require iteration! (think optimization or nonlinear solutions)\nSo this may be a computationally taxing approach if processing 1,000(s) or 1,000,000(s) of line segments."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus\n\n\n\n#&gt; Simple feature collection with 49 features and 12 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    statefp  statens    affgeoid geoid stusps       name lsad        aland\n#&gt; 1       06 01779778 0400000US06    06     CA California   00 403671196038\n#&gt; 2       55 01779806 0400000US55    55     WI  Wisconsin   00 140292246684\n#&gt; 3       16 01779783 0400000US16    16     ID      Idaho   00 214049923496\n#&gt; 4       27 00662849 0400000US27    27     MN  Minnesota   00 206232157570\n#&gt; 5       19 01779785 0400000US19    19     IA       Iowa   00 144659688848\n#&gt; 6       29 01779791 0400000US29    29     MO   Missouri   00 178052563675\n#&gt; 7       24 01714934 0400000US24    24     MD   Maryland   00  25151895765\n#&gt; 8       41 01155107 0400000US41    41     OR     Oregon   00 248628426864\n#&gt; 9       26 01779789 0400000US26    26     MI   Michigan   00 146614604273\n#&gt; 10      30 00767982 0400000US30    30     MT    Montana   00 376973673895\n#&gt;          awater state_name state_abbr jurisdiction_type\n#&gt; 1   20294133830 California         CA             state\n#&gt; 2   29343721650  Wisconsin         WI             state\n#&gt; 3    2391577745      Idaho         ID             state\n#&gt; 4   18949864226  Minnesota         MN             state\n#&gt; 5    1085996889       Iowa         IA             state\n#&gt; 6    2487215790   Missouri         MO             state\n#&gt; 7    6979171386   Maryland         MD             state\n#&gt; 8    6170953359     Oregon         OR             state\n#&gt; 9  103872203398   Michigan         MI             state\n#&gt; 10   3866689601    Montana         MT             state\n#&gt;                          geometry\n#&gt; 1  MULTIPOLYGON (((-118.594 33...\n#&gt; 2  MULTIPOLYGON (((-86.93428 4...\n#&gt; 3  MULTIPOLYGON (((-117.243 44...\n#&gt; 4  MULTIPOLYGON (((-97.22904 4...\n#&gt; 5  MULTIPOLYGON (((-96.62187 4...\n#&gt; 6  MULTIPOLYGON (((-95.76564 4...\n#&gt; 7  MULTIPOLYGON (((-76.04621 3...\n#&gt; 8  MULTIPOLYGON (((-124.5524 4...\n#&gt; 9  MULTIPOLYGON (((-84.61622 4...\n#&gt; 10 MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-1",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-1",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name)\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry\n#&gt; 1  California MULTIPOLYGON (((-118.594 33...\n#&gt; 2   Wisconsin MULTIPOLYGON (((-86.93428 4...\n#&gt; 3       Idaho MULTIPOLYGON (((-117.243 44...\n#&gt; 4   Minnesota MULTIPOLYGON (((-97.22904 4...\n#&gt; 5        Iowa MULTIPOLYGON (((-96.62187 4...\n#&gt; 6    Missouri MULTIPOLYGON (((-95.76564 4...\n#&gt; 7    Maryland MULTIPOLYGON (((-76.04621 3...\n#&gt; 8      Oregon MULTIPOLYGON (((-124.5524 4...\n#&gt; 9    Michigan MULTIPOLYGON (((-84.61622 4...\n#&gt; 10    Montana MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-2",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-2",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\")\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry\n#&gt; 1  California MULTILINESTRING ((-118.594 ...\n#&gt; 2   Wisconsin MULTILINESTRING ((-86.93428...\n#&gt; 3       Idaho MULTILINESTRING ((-117.243 ...\n#&gt; 4   Minnesota MULTILINESTRING ((-97.22904...\n#&gt; 5        Iowa MULTILINESTRING ((-96.62187...\n#&gt; 6    Missouri MULTILINESTRING ((-95.76564...\n#&gt; 7    Maryland MULTILINESTRING ((-76.04621...\n#&gt; 8      Oregon MULTILINESTRING ((-124.5524...\n#&gt; 9    Michigan MULTILINESTRING ((-84.61622...\n#&gt; 10    Montana MULTILINESTRING ((-116.0492..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-3",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-3",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf))\n\n\n\n#&gt; Simple feature collection with 49 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_name                       geometry          dist\n#&gt; 1  California MULTILINESTRING ((-118.594 ... 1000950.5 [m]\n#&gt; 2   Wisconsin MULTILINESTRING ((-86.93428... 1146522.6 [m]\n#&gt; 3       Idaho MULTILINESTRING ((-117.243 ...  567809.5 [m]\n#&gt; 4   Minnesota MULTILINESTRING ((-97.22904...  823100.9 [m]\n#&gt; 5        Iowa MULTILINESTRING ((-96.62187...  773889.8 [m]\n#&gt; 6    Missouri MULTILINESTRING ((-95.76564...  789142.1 [m]\n#&gt; 7    Maryland MULTILINESTRING ((-76.04621... 2174383.4 [m]\n#&gt; 8      Oregon MULTILINESTRING ((-124.5524... 1041819.1 [m]\n#&gt; 9    Michigan MULTILINESTRING ((-84.61622... 1401455.7 [m]\n#&gt; 10    Montana MULTILINESTRING ((-116.0492...  585011.2 [m]"
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-4",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-4",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3)\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado 139988.4 [m] MULTILINESTRING ((-109.06 3...\n#&gt; 2    Wyoming 139988.4 [m] MULTILINESTRING ((-111.0569...\n#&gt; 3   Nebraska 161243.2 [m] MULTILINESTRING ((-104.0531..."
  },
  {
    "objectID": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-5",
    "href": "slides/week2-2.html#to-determine-distance-to-border-we-need-a-linear-representation-5",
    "title": "Week 2-2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3) -&gt;\n  near3\n\n\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado 139988.4 [m] MULTILINESTRING ((-109.06 3...\n#&gt; 2    Wyoming 139988.4 [m] MULTILINESTRING ((-111.0569...\n#&gt; 3   Nebraska 161243.2 [m] MULTILINESTRING ((-104.0531...\n\n\nGood. However, we were only interested in the distance to the closest border not to ALL boarders. Therefore we calculated 48 (49 - 1) more distances then needed!\nWhile this is not to complex for 1 &lt;-&gt; 49 features imagine we had 28,000+ (like) your lab!\nThat would result in 1,344,000 more calculations then needed ‚Ä¶"
  },
  {
    "objectID": "slides/week2-2.html#gedesic-area-and-length-measurements",
    "href": "slides/week2-2.html#gedesic-area-and-length-measurements",
    "title": "Week 2-2",
    "section": "Gedesic Area and Length Measurements",
    "text": "Gedesic Area and Length Measurements\n\nNot all algorthimns are equal (in terms of speed or accuracy)\nSome more efficient algorithms that minimize computation time may reduce precision in the process.\nSome of ArcMap‚Äôs functions offer the option to compute geodesic distances and areas however ArcMap does not clearly indicate how its geodesic calculations are implemented (cite\nR is well documented, and is efficient!"
  },
  {
    "objectID": "slides/week2-2.html#native-sf-binds-to-libwgeom",
    "href": "slides/week2-2.html#native-sf-binds-to-libwgeom",
    "title": "Week 2-2",
    "section": "native sf binds to libwgeom",
    "text": "native sf binds to libwgeom"
  },
  {
    "objectID": "slides/week-2-1.html#so-far",
    "href": "slides/week-2-1.html#so-far",
    "title": "Week 2",
    "section": "So far ‚Ä¶",
    "text": "So far ‚Ä¶\nWe‚Äôve discussed the simple feature standard\n\nGeometries (type, dimension, and structure)\n\n- Empty, Valid, Simple\n\nEncoding (WKT & WKB)\nA set of operations\n\nAnd the implementation of the simple features standard in R\n\nsfg: a single feature geometry\nsfc: a set of geometries (sfg) stored as a list\nsf: a sfc list joined with a data.frame (attributes)\n\nThis R implementation is ideal/special because it achieves the simple feature abstract goal of:\n\n‚ÄúA simple feature is defined by the OpenGIS Abstract specification to have both spatial and non-spatial attributes‚Ä¶‚Äù - standard.\n\nThe shapefile/GIS traditional GIS view does not do this and seperates geometry (shp), from projection (prj), from data (dbf) and relates them through an shx file"
  },
  {
    "objectID": "slides/week-2-1.html#integration-with-tidyverse",
    "href": "slides/week-2-1.html#integration-with-tidyverse",
    "title": "Week 2",
    "section": "Integration with tidyverse",
    "text": "Integration with tidyverse\n\nWe saw how the dplyr verbs still work on an sf object since sf extends the data.frame class\nHow geom_sf support mapping (‚Äúspatial plotting‚Äù) in ggplot\nHow to read spatial data into R via GDAL drivers:\n\nspatial files (read_sf)\nflat files via st_as_sf\n\nIntegration with a few GEOS geometry operations like:\n\nst_combine()\nst_union()"
  },
  {
    "objectID": "slides/week-2-1.html#taki",
    "href": "slides/week-2-1.html#taki",
    "title": "Week 2",
    "section": "Taki ‚Ä¶",
    "text": "Taki ‚Ä¶\n\n\n\nconus &lt;-  USAboundaries::us_states() |&gt;\n  filter(!state_name %in% c(\"Puerto Rico\", \n                            \"Alaska\", \n                            \"Hawaii\"))\n\nlength(st_geometry(conus))\n[1] 49"
  },
  {
    "objectID": "slides/week-2-1.html#feature-resoloved-and-combined",
    "href": "slides/week-2-1.html#feature-resoloved-and-combined",
    "title": "Week 2",
    "section": "1 feature: resoloved and combined:",
    "text": "1 feature: resoloved and combined:\n\n\n\nst_cast / st_union work on sfg, sfc, and sf objects:\n\n\nus_c_ml = st_combine(conus) |&gt;\n  st_cast(\"MULTILINESTRING\")\n   \nus_u_ml = st_union(conus) |&gt;\n  st_cast(\"MULTILINESTRING\")"
  },
  {
    "objectID": "slides/week-2-1.html#so-what",
    "href": "slides/week-2-1.html#so-what",
    "title": "Week 2",
    "section": "So what?",
    "text": "So what?\nLets imagine we want to know the distance from Denver to the nearest state border:\nTo do this, we need to:\n\n1: define Denver as a geometry in a CRS\n2: determine the correct geometry types / representation\n3: calculate the distance between (1) and (2)"
  },
  {
    "objectID": "slides/week-2-1.html#make-denver-in-the-crs-of-our-states",
    "href": "slides/week-2-1.html#make-denver-in-the-crs-of-our-states",
    "title": "Week 2",
    "section": "1. Make ‚ÄúDenver‚Äù in the CRS of our states",
    "text": "1. Make ‚ÄúDenver‚Äù in the CRS of our states\n\ndenver = data.frame(y = 39.7392, x = -104.9903, name = \"Denver\")\n(denver_sf = st_as_sf(denver, coords = c(\"x\", \"y\"), crs = 4326))\nSimple feature collection with 1 feature and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -104.9903 ymin: 39.7392 xmax: -104.9903 ymax: 39.7392\nGeodetic CRS:  WGS 84\n    name                  geometry\n1 Denver POINT (-104.9903 39.7392)"
  },
  {
    "objectID": "slides/week-2-1.html#determine-the-3-closest-states",
    "href": "slides/week-2-1.html#determine-the-3-closest-states",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus\n\n\n\nSimple feature collection with 49 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   statefp  statens    affgeoid geoid stusps       name lsad        aland\n1       06 01779778 0400000US06    06     CA California   00 403671196038\n2       55 01779806 0400000US55    55     WI  Wisconsin   00 140292246684\n3       16 01779783 0400000US16    16     ID      Idaho   00 214049923496\n4       27 00662849 0400000US27    27     MN  Minnesota   00 206232157570\n5       19 01779785 0400000US19    19     IA       Iowa   00 144659688848\n6       29 01779791 0400000US29    29     MO   Missouri   00 178052563675\n7       24 01714934 0400000US24    24     MD   Maryland   00  25151895765\n8       41 01155107 0400000US41    41     OR     Oregon   00 248628426864\n9       26 01779789 0400000US26    26     MI   Michigan   00 146614604273\n10      30 00767982 0400000US30    30     MT    Montana   00 376973673895\n         awater state_name state_abbr jurisdiction_type\n1   20294133830 California         CA             state\n2   29343721650  Wisconsin         WI             state\n3    2391577745      Idaho         ID             state\n4   18949864226  Minnesota         MN             state\n5    1085996889       Iowa         IA             state\n6    2487215790   Missouri         MO             state\n7    6979171386   Maryland         MD             state\n8    6170953359     Oregon         OR             state\n9  103872203398   Michigan         MI             state\n10   3866689601    Montana         MT             state\n                         geometry\n1  MULTIPOLYGON (((-118.594 33...\n2  MULTIPOLYGON (((-86.93428 4...\n3  MULTIPOLYGON (((-117.243 44...\n4  MULTIPOLYGON (((-97.22904 4...\n5  MULTIPOLYGON (((-96.62187 4...\n6  MULTIPOLYGON (((-95.76564 4...\n7  MULTIPOLYGON (((-76.04621 3...\n8  MULTIPOLYGON (((-124.5524 4...\n9  MULTIPOLYGON (((-84.61622 4...\n10 MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week-2-1.html#determine-the-3-closest-states-1",
    "href": "slides/week-2-1.html#determine-the-3-closest-states-1",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name)\n\n\n\nSimple feature collection with 49 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   state_name                       geometry\n1  California MULTIPOLYGON (((-118.594 33...\n2   Wisconsin MULTIPOLYGON (((-86.93428 4...\n3       Idaho MULTIPOLYGON (((-117.243 44...\n4   Minnesota MULTIPOLYGON (((-97.22904 4...\n5        Iowa MULTIPOLYGON (((-96.62187 4...\n6    Missouri MULTIPOLYGON (((-95.76564 4...\n7    Maryland MULTIPOLYGON (((-76.04621 3...\n8      Oregon MULTIPOLYGON (((-124.5524 4...\n9    Michigan MULTIPOLYGON (((-84.61622 4...\n10    Montana MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week-2-1.html#determine-the-3-closest-states-2",
    "href": "slides/week-2-1.html#determine-the-3-closest-states-2",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name) %&gt;%\n  mutate(dist = st_distance(., denver_sf))\n\n\n\nSimple feature collection with 49 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   state_name                       geometry          dist\n1  California MULTIPOLYGON (((-118.594 33... 1000950.5 [m]\n2   Wisconsin MULTIPOLYGON (((-86.93428 4... 1146522.6 [m]\n3       Idaho MULTIPOLYGON (((-117.243 44...  567809.5 [m]\n4   Minnesota MULTIPOLYGON (((-97.22904 4...  823100.9 [m]\n5        Iowa MULTIPOLYGON (((-96.62187 4...  773889.8 [m]\n6    Missouri MULTIPOLYGON (((-95.76564 4...  789142.1 [m]\n7    Maryland MULTIPOLYGON (((-76.04621 3... 2174383.4 [m]\n8      Oregon MULTIPOLYGON (((-124.5524 4... 1041819.1 [m]\n9    Michigan MULTIPOLYGON (((-84.61622 4... 1401455.7 [m]\n10    Montana MULTIPOLYGON (((-116.0492 4...  585011.2 [m]"
  },
  {
    "objectID": "slides/week-2-1.html#determine-the-3-closest-states-3",
    "href": "slides/week-2-1.html#determine-the-3-closest-states-3",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name) %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3)\n\n\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\nGeodetic CRS:  WGS 84\n  state_name         dist                       geometry\n1   Colorado      0.0 [m] MULTIPOLYGON (((-109.06 38....\n2    Wyoming 139988.4 [m] MULTIPOLYGON (((-111.0569 4...\n3   Nebraska 161243.2 [m] MULTIPOLYGON (((-104.0531 4...\n\n\n\nThat‚Äôs close, but the distance to Colorado is 0, that‚Äôs not a state border."
  },
  {
    "objectID": "slides/week-2-1.html#geometry-selection",
    "href": "slides/week-2-1.html#geometry-selection",
    "title": "Week 2",
    "section": "Geometry Selection",
    "text": "Geometry Selection\n\nPolygon (therefore MULTIPOLGYGONS) describe areas!\nThe distance to a point in a polygon to that polygon is 0."
  },
  {
    "objectID": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation",
    "href": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus\n\n\n\nSimple feature collection with 49 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   statefp  statens    affgeoid geoid stusps       name lsad        aland\n1       06 01779778 0400000US06    06     CA California   00 403671196038\n2       55 01779806 0400000US55    55     WI  Wisconsin   00 140292246684\n3       16 01779783 0400000US16    16     ID      Idaho   00 214049923496\n4       27 00662849 0400000US27    27     MN  Minnesota   00 206232157570\n5       19 01779785 0400000US19    19     IA       Iowa   00 144659688848\n6       29 01779791 0400000US29    29     MO   Missouri   00 178052563675\n7       24 01714934 0400000US24    24     MD   Maryland   00  25151895765\n8       41 01155107 0400000US41    41     OR     Oregon   00 248628426864\n9       26 01779789 0400000US26    26     MI   Michigan   00 146614604273\n10      30 00767982 0400000US30    30     MT    Montana   00 376973673895\n         awater state_name state_abbr jurisdiction_type\n1   20294133830 California         CA             state\n2   29343721650  Wisconsin         WI             state\n3    2391577745      Idaho         ID             state\n4   18949864226  Minnesota         MN             state\n5    1085996889       Iowa         IA             state\n6    2487215790   Missouri         MO             state\n7    6979171386   Maryland         MD             state\n8    6170953359     Oregon         OR             state\n9  103872203398   Michigan         MI             state\n10   3866689601    Montana         MT             state\n                         geometry\n1  MULTIPOLYGON (((-118.594 33...\n2  MULTIPOLYGON (((-86.93428 4...\n3  MULTIPOLYGON (((-117.243 44...\n4  MULTIPOLYGON (((-97.22904 4...\n5  MULTIPOLYGON (((-96.62187 4...\n6  MULTIPOLYGON (((-95.76564 4...\n7  MULTIPOLYGON (((-76.04621 3...\n8  MULTIPOLYGON (((-124.5524 4...\n9  MULTIPOLYGON (((-84.61622 4...\n10 MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-1",
    "href": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-1",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name)\n\n\n\nSimple feature collection with 49 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   state_name                       geometry\n1  California MULTIPOLYGON (((-118.594 33...\n2   Wisconsin MULTIPOLYGON (((-86.93428 4...\n3       Idaho MULTIPOLYGON (((-117.243 44...\n4   Minnesota MULTIPOLYGON (((-97.22904 4...\n5        Iowa MULTIPOLYGON (((-96.62187 4...\n6    Missouri MULTIPOLYGON (((-95.76564 4...\n7    Maryland MULTIPOLYGON (((-76.04621 3...\n8      Oregon MULTIPOLYGON (((-124.5524 4...\n9    Michigan MULTIPOLYGON (((-84.61622 4...\n10    Montana MULTIPOLYGON (((-116.0492 4..."
  },
  {
    "objectID": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-2",
    "href": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-2",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\")\n\n\n\nSimple feature collection with 49 features and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   state_name                       geometry\n1  California MULTILINESTRING ((-118.594 ...\n2   Wisconsin MULTILINESTRING ((-86.93428...\n3       Idaho MULTILINESTRING ((-117.243 ...\n4   Minnesota MULTILINESTRING ((-97.22904...\n5        Iowa MULTILINESTRING ((-96.62187...\n6    Missouri MULTILINESTRING ((-95.76564...\n7    Maryland MULTILINESTRING ((-76.04621...\n8      Oregon MULTILINESTRING ((-124.5524...\n9    Michigan MULTILINESTRING ((-84.61622...\n10    Montana MULTILINESTRING ((-116.0492..."
  },
  {
    "objectID": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-3",
    "href": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-3",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf))\n\n\n\nSimple feature collection with 49 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   state_name                       geometry          dist\n1  California MULTILINESTRING ((-118.594 ... 1000950.5 [m]\n2   Wisconsin MULTILINESTRING ((-86.93428... 1146522.6 [m]\n3       Idaho MULTILINESTRING ((-117.243 ...  567809.5 [m]\n4   Minnesota MULTILINESTRING ((-97.22904...  823100.9 [m]\n5        Iowa MULTILINESTRING ((-96.62187...  773889.8 [m]\n6    Missouri MULTILINESTRING ((-95.76564...  789142.1 [m]\n7    Maryland MULTILINESTRING ((-76.04621... 2174383.4 [m]\n8      Oregon MULTILINESTRING ((-124.5524... 1041819.1 [m]\n9    Michigan MULTILINESTRING ((-84.61622... 1401455.7 [m]\n10    Montana MULTILINESTRING ((-116.0492...  585011.2 [m]"
  },
  {
    "objectID": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-4",
    "href": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-4",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3)\n\n\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\nGeodetic CRS:  WGS 84\n  state_name         dist                       geometry\n1   Colorado 139988.4 [m] MULTILINESTRING ((-109.06 3...\n2    Wyoming 139988.4 [m] MULTILINESTRING ((-111.0569...\n3   Nebraska 161243.2 [m] MULTILINESTRING ((-104.0531..."
  },
  {
    "objectID": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-5",
    "href": "slides/week-2-1.html#to-determine-distance-to-border-we-need-a-linear-representation-5",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3) -&gt;\n  near3\n\n\n\n\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\nGeodetic CRS:  WGS 84\n  state_name         dist                       geometry\n1   Colorado 139988.4 [m] MULTILINESTRING ((-109.06 3...\n2    Wyoming 139988.4 [m] MULTILINESTRING ((-111.0569...\n3   Nebraska 161243.2 [m] MULTILINESTRING ((-104.0531...\n\n\nGood. However, we were only interested in the distance to the closest border not to ALL boarders. Therefore we calculated 48 (49 - 1) more distances then needed!\nWhile this is not to complex for 1 &lt;-&gt; 49 features imagine we had 28,000+ (like) your lab!\nThat would result in 1,344,000 more calculations then needed ‚Ä¶"
  },
  {
    "objectID": "slides/week-2-1.html#revisting-the-idea-of-the-feature-level",
    "href": "slides/week-2-1.html#revisting-the-idea-of-the-feature-level",
    "title": "Week 2",
    "section": "Revisting the idea of the feature level:",
    "text": "Revisting the idea of the feature level:\nA ‚Äúfeature‚Äù can ‚Äúbe part of the whole‚Äù or the whole\n\nA island (POLYGON), or a set of islands acting as 1 unit (MULTIPOLYGON)\nA city (POINT), or a set of cities meeting a condition (MULTIPOINT)\nA road (LINESTRING), or a route (MULTILINESTRING)\nSince we want the distance to the nearest border, regardless of the state. Our feature is the set of borders with preserved boundaries.\nIn other words, a 1 feature MULTILINESTRING\n\n\nst_distance(denver_sf, st_cast(st_combine(conus), \"MULTILINESTRING\"))\nUnits: [m]\n         [,1]\n[1,] 139988.4\n\nThe same principle would apply if the question was ‚Äúdistance to national border‚Äù"
  },
  {
    "objectID": "slides/week-2-1.html#the-stickness-of-sfc-column",
    "href": "slides/week-2-1.html#the-stickness-of-sfc-column",
    "title": "Week 2",
    "section": "The stickness of sfc column",
    "text": "The stickness of sfc column\n\nA simple features object (sf) is the connection of a sfc list-column and data.frame of attributes\n\n\n\nThis binding is unique compared to other column bindings built with things like\n\ndplyr::bind_cols()\ncbind()\ndo.call(cbind, list())"
  },
  {
    "objectID": "slides/week-2-1.html#the-stickness-of-sfc-column-1",
    "href": "slides/week-2-1.html#the-stickness-of-sfc-column-1",
    "title": "Week 2",
    "section": "The stickness of sfc column",
    "text": "The stickness of sfc column\n\nGeometry columns are ‚Äústicky‚Äù meaning they persist through data manipulation:\n\n\nUSAboundaries::us_states() |&gt; \n  select(name) |&gt; \n  slice(1:2)\nSimple feature collection with 2 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.4096 ymin: 32.53416 xmax: -86.80587 ymax: 47.05468\nGeodetic CRS:  WGS 84\n        name                       geometry\n1 California MULTIPOLYGON (((-118.594 33...\n2  Wisconsin MULTIPOLYGON (((-86.93428 4...\n\nDropping the geometry column requires dropping the geometry via sf:\n\nUSAboundaries::us_states() |&gt; \n  st_drop_geometry() |&gt; #&lt;&lt;\n  select(name) |&gt; \n  slice(1:2)\n        name\n1 California\n2  Wisconsin\n\nOr cohersing the sf object to a data.frame:\n\nUSAboundaries::us_states() |&gt; \n  as.data.frame() |&gt; #&lt;&lt;\n  select(name) |&gt; \n  slice(1:2)\n        name\n1 California\n2  Wisconsin"
  },
  {
    "objectID": "slides/week-2-1.html#coordinate-systems",
    "href": "slides/week-2-1.html#coordinate-systems",
    "title": "Week 2",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n\nWhat makes a feature geometry spatial is the reference system‚Ä¶"
  },
  {
    "objectID": "slides/week-2-1.html#coordinate-systems-1",
    "href": "slides/week-2-1.html#coordinate-systems-1",
    "title": "Week 2",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n\nCoordinate Reference Systems (CRS) defines how spatial features relate to the surface of the Earth.\nCRSs are either geographic or projected‚Ä¶\nCRSs are measurement units for coordinates:"
  },
  {
    "objectID": "slides/week-2-1.html#sf-tools",
    "href": "slides/week-2-1.html#sf-tools",
    "title": "Week 2",
    "section": "sf tools",
    "text": "sf tools\nIn sf we have three tools for exploring, define, and changing CRS systems:\n\nst_crs : Retrieve coordinate reference system from sf or sfc object\nst_set_crs : Set or replace coordinate reference system from object\nst_transform : Transform or convert coordinates of simple feature\nAgain, ‚Äúst‚Äù (like PostGIS) denotes it is an operation that can work on a ‚Äù s patial t ype ‚Äù"
  },
  {
    "objectID": "slides/week-2-1.html#geographic-coordinate-systms-gcs",
    "href": "slides/week-2-1.html#geographic-coordinate-systms-gcs",
    "title": "Week 2",
    "section": "Geographic Coordinate Systms (GCS)",
    "text": "Geographic Coordinate Systms (GCS)\nA GCS identifies locations on the curved surface of the earth.\nLocations are measured in angular units from the center of the earth relative to the plane defined by the equator and the plane defined by the prime meridian.\nThe vertical angle describes the latitude and the horizontal angle the longitude\nIn most coordinate systems, the North-South and East-West directions are encoded as +/-.\nNorth and East are positive (+) and South and West are negative (-) sign.\nA GCS is defined by 3 components:\n\nan ellipsoid\na geoid\na datum"
  },
  {
    "objectID": "slides/week-2-1.html#sphere-and-ellipsoid",
    "href": "slides/week-2-1.html#sphere-and-ellipsoid",
    "title": "Week 2",
    "section": "Sphere and Ellipsoid",
    "text": "Sphere and Ellipsoid\n\nAssuming that the earth is a perfect sphere simplifies calculations and works for small-scale maps (maps that show a large area of the earth).\nBut ‚Ä¶ the earth is not a sphere do to its rotation inducing a centripetal force along the equator.\nThis results in an equatorial axis that is roughly 21 km longer than the polar axis.\nTo account for this, the earth is modeled as an ellipsoid (slighty squished sphere) defined by two radii:\n\nthe semi-major axis (along the equatorial radius)\nthe semi-minor axis (along the polar radius)"
  },
  {
    "objectID": "slides/week-2-1.html#section-1",
    "href": "slides/week-2-1.html#section-1",
    "title": "Week 2",
    "section": "",
    "text": "Thanks to satellite and computational capabilities our estimates of these radii are be quite precise\n\nThe semi-major axis is 6,378,137 m\nThe semi-minor axis is 6,356,752 m\n\nDifferences in distance along the surfaces of an ellipsoid vs.¬†a perfect sphere are small but measurable (the difference can be as high as 20 km)"
  },
  {
    "objectID": "slides/week-2-1.html#geoid",
    "href": "slides/week-2-1.html#geoid",
    "title": "Week 2",
    "section": "Geoid",
    "text": "Geoid\n\nThe ellipsoid gives us the earths form as a perfectly smooth object\nBut ‚Ä¶ the earth is not perfectly smooth\nDeviations from the perfect sphere are measurable and can influence measurements.\nA geoid is a mathematical model fore representing these deviations\n\nWe are not talking about mountains and ocean trenches but the earth‚Äôs gravitational potential which is tied to the flow of the earth‚Äôs hot and fluid core.\nTherefore the geoid is constantly changing, albeit a large temporal scale.\n\nThe measurement and representation of the earth‚Äôs shape is at the heart of geodesy\n\n\nNASA‚Äôs geoid models"
  },
  {
    "objectID": "slides/week-2-1.html#datum",
    "href": "slides/week-2-1.html#datum",
    "title": "Week 2",
    "section": "Datum",
    "text": "Datum\n\nSo how are we to reconcile our need to work with a (simple) mathematical model of the earth‚Äôs shape with the undulating nature of the geoid?\nWe align the geoid with the ellipsoid to map the the earths departures from the smooth assumption\nThe alignment can be local where the ellipsoid surface is closely fit to the geoid at a particular location on the earth‚Äôs surface\n\nor\n\ngeocentric where the ellipsoid is aligned with the center of the earth.\nThe alignment of the smooth ellipsoid to the geoid model defines a datum."
  },
  {
    "objectID": "slides/week-2-1.html#local-datums",
    "href": "slides/week-2-1.html#local-datums",
    "title": "Week 2",
    "section": "Local Datums",
    "text": "Local Datums\n\nThere are many local datums to choose from\nThe choice of datum is largely driven by the location\nWhen working in the USA, a the North American Datum of 1927 (or NAD27 for short) is standard\n\nNAD27 is not well suited for other parts of the world.\n\n\nExamples of common local datums are shown in the following table:"
  },
  {
    "objectID": "slides/week-2-1.html#section-2",
    "href": "slides/week-2-1.html#section-2",
    "title": "Week 2",
    "section": "",
    "text": "Local datum\nAcronym\nBest for\nComment\n\n\n\n\nNorth American Datum of 1927\nNAD27\nContinental US\nThis is an old datum but still prevalent\n\n\nEuropean Datum of 1950\nED50\nWestern Europe\nDeveloped after World War II and still quite popular\n\n\nWorld Geodetic System 1972\nWGS72\nGlobal\nDeveloped by the Department of Defense."
  },
  {
    "objectID": "slides/week-2-1.html#geocentric-datum",
    "href": "slides/week-2-1.html#geocentric-datum",
    "title": "Week 2",
    "section": "Geocentric Datum",
    "text": "Geocentric Datum\n\nMany modern datums use a geocentric alignment\n\nWorld Geodetic Survey for 1984 (WGS84)\nNorth American Datums of 1983 (NAD83)\n\nMost popular geocentric datums use the WGS84 ellipsoid or the GRS80 ellipsoid which share nearly identical semi-major and semi-minor axes"
  },
  {
    "objectID": "slides/week-2-1.html#section-3",
    "href": "slides/week-2-1.html#section-3",
    "title": "Week 2",
    "section": "",
    "text": "Geocentric datum\nAcronym\nBest for\nComment\n\n\n\n\nNorth American Datum of 1983\nNAD83\nContinental US\nThis is one of the most popular modern datums for the contiguous US.\n\n\nEuropean Terrestrial Reference System 1989\nETRS89\nWestern Europe\nThis is the most popular modern datum for much of Europe.\n\n\nWorld Geodetic System 1984\nWGS84\nGlobal\nDeveloped by the Department of Defense.\n\n\n\n\n\n\n\n\n\nNote\n\n\nNAD 27 is based on Clarke Ellipsoid of 1866 which is calculated by manual surveying. NAD83 is based on the Geodetic Reference System (GRS) of 1980."
  },
  {
    "objectID": "slides/week-2-1.html#building-a-gcs",
    "href": "slides/week-2-1.html#building-a-gcs",
    "title": "Week 2",
    "section": "Building a GCS",
    "text": "Building a GCS\n\nSo, a GCS is defined by the ellipsoid model and its alignment to the geoid defining the datum.\nSmooth Sphere - Mathmatical Geoid (in angular units)"
  },
  {
    "objectID": "slides/week-2-1.html#projected-coordinate-systems",
    "href": "slides/week-2-1.html#projected-coordinate-systems",
    "title": "Week 2",
    "section": "Projected Coordinate Systems",
    "text": "Projected Coordinate Systems\n\nThe surface of the earth is curved but maps (and to data GIS) is flat.\nA projected coordinate system (PCS) is a reference system for identifying locations and measuring features on a flat (2D) surfaces. I\nProjected coordinate systems have an origin, an x axis, a y axis, and a linear unit of measure.\nGoing from a GCS to a PCS requires mathematical transformations.\nThere are three main groups of projection types:\n\nconic\ncylindrical\nplanar"
  },
  {
    "objectID": "slides/week-2-1.html#projection-types",
    "href": "slides/week-2-1.html#projection-types",
    "title": "Week 2",
    "section": "Projection Types:",
    "text": "Projection Types:\n\n\nIn all cases, distortion is minimized at the line/point of tangency (denoted by black line/point)\nDistortions are minimized along the tangency lines and increase with the distance from those lines."
  },
  {
    "objectID": "slides/week-2-1.html#plannar",
    "href": "slides/week-2-1.html#plannar",
    "title": "Week 2",
    "section": "Plannar",
    "text": "Plannar\n\nA planar projection projects data onto a flat surface touching the globe at a point or along 1 line of tangency.\nTypically used to map polar regions."
  },
  {
    "objectID": "slides/week-2-1.html#cylindrical",
    "href": "slides/week-2-1.html#cylindrical",
    "title": "Week 2",
    "section": "Cylindrical",
    "text": "Cylindrical\n\nA cylindrical projection maps the surface onto a cylinder.\nThis projection could also be created by touching the Earth‚Äôs surface along 1 or 2 lines of tangency\nMost often when mapping the entire world."
  },
  {
    "objectID": "slides/week-2-1.html#conic",
    "href": "slides/week-2-1.html#conic",
    "title": "Week 2",
    "section": "Conic",
    "text": "Conic\nIn a conic projection, the Earth‚Äôs surface is projected onto a cone along 1 or 2 lines of tangency\nTherefore, it is the best suited for maps of mid-latitude areas."
  },
  {
    "objectID": "slides/week-2-1.html#spatial-properties",
    "href": "slides/week-2-1.html#spatial-properties",
    "title": "Week 2",
    "section": "Spatial Properties",
    "text": "Spatial Properties\n\nAll projections distort real-world geographic features.\nThink about trying to unpeel an orange while preserving the skin\n\nThe four spatial properties that are subject to distortion are: shape, area, distance and direction\n\nA map that preserves shape is called conformal;\none that preserves area is called equal-area;\none that preserves distance is called equidistant\none that preserves direction is called azimuthal\nEach map projection can preserve only one or two of the four spatial properties.\nOften, projections are named after the spatial properties they preserve.\nWhen working with small-scale (large area) maps and when multiple spatial properties are needed, it is best to break the analyses across projections to minimize errors associated with spatial distortion."
  },
  {
    "objectID": "slides/week-2-1.html#setting-crsspcss",
    "href": "slides/week-2-1.html#setting-crsspcss",
    "title": "Week 2",
    "section": "Setting CRSs/PCSs",
    "text": "Setting CRSs/PCSs\n\nWe saw that sfc objects have two attributes to store a CRS: epsg and proj4string\n\n\nst_geometry(conus)\nGeometry set for 49 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\nMULTIPOLYGON (((-118.594 33.4672, -118.4848 33....\nMULTIPOLYGON (((-86.93428 45.42115, -86.83575 4...\nMULTIPOLYGON (((-117.243 44.39097, -117.2151 44...\nMULTIPOLYGON (((-97.22904 49.00069, -96.93096 4...\nMULTIPOLYGON (((-96.62187 42.77925, -96.57794 4...\n\n\nThis implies that all geometries in a geometry list-column (sfc) must have the same CRS.\nproj4string is a generic, string-based description of a CRS, understood by PROJ\nIt defines projection types and parameter values for particular projections,\nAs a result it can cover an infinite amount of different projections.\nepsg is the integer ID for a known CRS that can be resolved into a proj4string.\n\nThis is somewhat equivalent to the idea that a 6-digit FIP code can be resolved to a state/county pair\n\nSome proj4string values can resolved back into their corresponding epsg ID, but this does not always work.\nThe importance of having epsg values stored with data besides proj4string values is that the epsg refers to particular, well-known CRS, whose parameters may change (improve) over time\nfixing only the proj4string may remove the possibility to benefit from such improvements, and limit some of the provenance of datasets (but may help reproducibility)"
  },
  {
    "objectID": "slides/week-2-1.html#proj4-coordinate-syntax",
    "href": "slides/week-2-1.html#proj4-coordinate-syntax",
    "title": "Week 2",
    "section": "PROJ4 coordinate syntax",
    "text": "PROJ4 coordinate syntax\nThe PROJ4 syntax contains a list of parameters, each prefixed with the + character.\nA list of some PROJ4 parameters follows and the full list can be found here:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n+a\nSemi-major radius of the ellipsoid axis\n\n\n+b\nSemi-minor radius of the ellipsoid axis\n\n\n+datum\nDatum name\n\n\n+ellps\nEllipsoid name\n\n\n+lat_0\nLatitude of origin\n\n\n+lat_1\nLatitude of first standard parallel\n\n\n+lat_2\nLatitude of second standard parallel\n\n\n+lat_ts\nLatitude of true scale\n\n\n+lon_0\nCentral meridian\n\n\n+over\nAllow longitude output outside -180 to 180 range, disables wrapping\n\n\n+proj\nProjection name\n\n\n+south\nDenotes southern hemisphere UTM zone\n\n\n+units\nmeters, US survey feet, etc.\n\n\n+x_0\nFalse easting\n\n\n+y_0\nFalse northing\n\n\n+zone\nUTM zone"
  },
  {
    "objectID": "slides/week-2-1.html#section-4",
    "href": "slides/week-2-1.html#section-4",
    "title": "Week 2",
    "section": "",
    "text": "WGS84\nEPSG: 4326\nPROJ4: +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n- projection name: longlat\n- Latitude of origin: WGS84\n- Longitude of origin: WGS84\n\nWGS84\nEPSG: 5070\n\"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\n- projection name: aea (Albers Equal Area)\n- Latitude of origin: 23\n- Longitude of origin: -96\n- Latitude of first standard parallel: 29.5\n- Latitude of second standard parallel: 45.5\n- False Easting: 0\n- False Northing: 0\n- Datum: NAD83\n- Units: m"
  },
  {
    "objectID": "slides/week-2-1.html#transform-and-retrive",
    "href": "slides/week-2-1.html#transform-and-retrive",
    "title": "Week 2",
    "section": "Transform and retrive",
    "text": "Transform and retrive\n\n\n\nst_crs(conus)$epsg\n[1] 4326\nst_crs(conus)$proj4string\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\nst_crs(conus)$datum\n[1] \"WGS84\"\n\n\n\nconus5070 &lt;- st_transform(conus, 5070)\n\nst_crs(conus5070)$epsg\n[1] 5070\nst_crs(conus5070)$proj4string\n[1] \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\nst_crs(conus5070)$datum\n[1] \"NAD83\""
  },
  {
    "objectID": "slides/week-2-1.html#revisit-denver",
    "href": "slides/week-2-1.html#revisit-denver",
    "title": "Week 2",
    "section": "Revisit Denver",
    "text": "Revisit Denver\n\necho -104.9903 39.7392 | proj +proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\n-723281.88  6827.29\n\n\nred = false origin : blue = Denver"
  },
  {
    "objectID": "slides/week-2-1.html#geodesic-geometries",
    "href": "slides/week-2-1.html#geodesic-geometries",
    "title": "Week 2",
    "section": "Geodesic geometries",
    "text": "Geodesic geometries\n\nPCSs introduce errors in their geometric measurements because the distance between two points on an ellipsoid is difficult to replicate on a projected coordinate system unless these points are close to one another.\nIn most cases, such errors other sources of error in the feature representation outweigh measurement errors made in a PCS making them tolorable.\n\nHowever, if the domain of analysis is large (i.e.¬†the North American continent), then the measurement errors associated with a projected coordinate system may no longer be acceptable.\nA way to circumvent projected coordinate system limitations is to adopt a geodesic solution."
  },
  {
    "objectID": "slides/week-2-1.html#geodesic-measurments",
    "href": "slides/week-2-1.html#geodesic-measurments",
    "title": "Week 2",
    "section": "Geodesic Measurments",
    "text": "Geodesic Measurments\n\nA geodesic distance is the shortest distance between two points on an ellipsoid\nA geodesic area measurement is one that is measured on an ellipsoid.\nSuch measurements are independent of the underlying projected coordinate system.\nWhy does this matter?\nCompare the distances measured between Santa Barbara and Amsterdam. The blue line represents the shortest distance between the two points on a planar coordinate system. The red line as measured on a ellipsoid.\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nAttaching package: 'maps'\nThe following object is masked from 'package:purrr':\n\n    map\nSpherical geometry (s2) switched off\nalthough coordinates are longitude/latitude, st_union assumes that they are\nplanar\nstars object downsampled to 1369 by 730 cells. See tm_shape manual (argument raster.downsample)"
  },
  {
    "objectID": "slides/week-2-1.html#section-5",
    "href": "slides/week-2-1.html#section-5",
    "title": "Week 2",
    "section": "",
    "text": "the geodesic distance looks weird given its curved appearance on the projected map.\nthis curvature is a byproduct of the current reference system‚Äôs increasing distance distortion as one moves towards the pole!\nWe can display the geodesic and planar distance on a 3D globe (or a projection that mimics the view of the 3D earth)."
  },
  {
    "objectID": "slides/week-2-1.html#section-6",
    "href": "slides/week-2-1.html#section-6",
    "title": "Week 2",
    "section": "",
    "text": "So if a geodesic measurement is more precise than a planar measurement, why not perform all spatial operations using geodesic geometry?\nThe downside is in its computational requirements.\nIt‚Äôs far more efficient to compute area/distance on a plane than it is on a spheroid.\nThis is because geodesic calculations have no simple algebraic solutions and involve approximations that may require iteration! (think optimization or nonlinear solutions)\nSo this may be a computationally taxing approach if processing 1,000(s) or 1,000,000(s) of line segments."
  },
  {
    "objectID": "slides/week-2-1.html#gedesic-area-and-length-measurements",
    "href": "slides/week-2-1.html#gedesic-area-and-length-measurements",
    "title": "Week 2",
    "section": "Gedesic Area and Length Measurements",
    "text": "Gedesic Area and Length Measurements\n\nNot all algorthimns are equal (in terms of speed or accuracy)\nSome more efficient algorithms that minimize computation time may reduce precision in the process.\nSome of ArcMap‚Äôs functions offer the option to compute geodesic distances and areas however ArcMap does not clearly indicate how its geodesic calculations are implemented (cite\nR is well documented, and is efficient!"
  },
  {
    "objectID": "slides/week-2-1.html#distances",
    "href": "slides/week-2-1.html#distances",
    "title": "Week 2",
    "section": "Distances",
    "text": "Distances\n?st_distance"
  },
  {
    "objectID": "slides/week-2-1.html#native-sf-binds-to-libwgeom",
    "href": "slides/week-2-1.html#native-sf-binds-to-libwgeom",
    "title": "Week 2",
    "section": "native sf binds to libwgeom",
    "text": "native sf binds to libwgeom"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example",
    "href": "slides/week-2-1.html#distance-example",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example-1",
    "href": "slides/week-2-1.html#distance-example-1",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\nGeodetic CRS:  WGS 84\n  name                  geometry\n1  NYC   POINT (-74.006 40.7128)\n2   SB POINT (-119.6982 34.4208)"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example-2",
    "href": "slides/week-2-1.html#distance-example-2",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\nGeodetic CRS:  WGS 84\n  name                  geometry\n1  NYC   POINT (-74.006 40.7128)\n2   SB POINT (-119.6982 34.4208)"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example-3",
    "href": "slides/week-2-1.html#distance-example-3",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\nGeodetic CRS:  WGS 84\n  name                  geometry\n1  NYC   POINT (-74.006 40.7128)\n2   SB POINT (-119.6982 34.4208)\nUnits: [m]\n        [,1]    [,2]\n[1,]       0 4050406\n[2,] 4050406       0"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example-4",
    "href": "slides/week-2-1.html#distance-example-4",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\nGeodetic CRS:  WGS 84\n  name                  geometry\n1  NYC   POINT (-74.006 40.7128)\n2   SB POINT (-119.6982 34.4208)\nUnits: [m]\n        [,1]    [,2]\n[1,]       0 4050406\n[2,] 4050406       0\nUnits: [¬∞]\n         1        2\n1  0.00000 46.12338\n2 46.12338  0.00000"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example-5",
    "href": "slides/week-2-1.html#distance-example-5",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\nGeodetic CRS:  WGS 84\n  name                  geometry\n1  NYC   POINT (-74.006 40.7128)\n2   SB POINT (-119.6982 34.4208)\nUnits: [m]\n        [,1]    [,2]\n[1,]       0 4050406\n[2,] 4050406       0\nUnits: [¬∞]\n         1        2\n1  0.00000 46.12338\n2 46.12338  0.00000\nUnits: [m]\n        1       2\n1       0 4017987\n2 4017987       0"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example-6",
    "href": "slides/week-2-1.html#distance-example-6",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n# Equal Distance\nst_distance(st_transform(pts, eqds))\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\nGeodetic CRS:  WGS 84\n  name                  geometry\n1  NYC   POINT (-74.006 40.7128)\n2   SB POINT (-119.6982 34.4208)\nUnits: [m]\n        [,1]    [,2]\n[1,]       0 4050406\n[2,] 4050406       0\nUnits: [¬∞]\n         1        2\n1  0.00000 46.12338\n2 46.12338  0.00000\nUnits: [m]\n        1       2\n1       0 4017987\n2 4017987       0\nUnits: [m]\n        1       2\n1       0 3823549\n2 3823549       0"
  },
  {
    "objectID": "slides/week-2-1.html#distance-example-7",
    "href": "slides/week-2-1.html#distance-example-7",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"), crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n# Equal Distance\nst_distance(st_transform(pts, eqds))\n\n\n\n        y         x name\n1 40.7128  -74.0060  NYC\n2 34.4208 -119.6982   SB\nSimple feature collection with 2 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\nGeodetic CRS:  WGS 84\n  name                  geometry\n1  NYC   POINT (-74.006 40.7128)\n2   SB POINT (-119.6982 34.4208)\nUnits: [m]\n        [,1]    [,2]\n[1,]       0 4050406\n[2,] 4050406       0\nUnits: [¬∞]\n         1        2\n1  0.00000 46.12338\n2 46.12338  0.00000\nUnits: [m]\n        1       2\n1       0 4017987\n2 4017987       0\nUnits: [m]\n        1       2\n1       0 3823549\n2 3823549       0"
  },
  {
    "objectID": "slides/week-2-1.html#area-example-conus",
    "href": "slides/week-2-1.html#area-example-conus",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")"
  },
  {
    "objectID": "slides/week-2-1.html#area-example-conus-1",
    "href": "slides/week-2-1.html#area-example-conus-1",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))"
  },
  {
    "objectID": "slides/week-2-1.html#area-example-conus-2",
    "href": "slides/week-2-1.html#area-example-conus-2",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df)"
  },
  {
    "objectID": "slides/week-2-1.html#area-example-conus-3",
    "href": "slides/week-2-1.html#area-example-conus-3",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) ))"
  },
  {
    "objectID": "slides/week-2-1.html#area-example-conus-4",
    "href": "slides/week-2-1.html#area-example-conus-4",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) )) +\n  theme_linedraw()"
  },
  {
    "objectID": "slides/week-2-1.html#area-example-conus-5",
    "href": "slides/week-2-1.html#area-example-conus-5",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) )) +\n  theme_linedraw() +\n  labs(x = \"SRS\", y = \"m2\")"
  },
  {
    "objectID": "slides/week-2-1.html#units-in-sf",
    "href": "slides/week-2-1.html#units-in-sf",
    "title": "Week 2",
    "section": "Units in sf",
    "text": "Units in sf\n\nThe CRS in sf encodes the units of measurement relating to spatial features\nWhere possible geometric operations such as st_distance(), st_length() and st_area() report results with a units attribute appropriate for the CRS:\nThis can be both handy and very confusing for those new to it. Consider the following:\n\n\n(l = sum(st_length(conus)))\n94980149 [m]\n(a = sum(st_area(conus)))\n7.83761e+12 [m^2]"
  },
  {
    "objectID": "slides/week-2-1.html#section-7",
    "href": "slides/week-2-1.html#section-7",
    "title": "Week 2",
    "section": "",
    "text": "We can set units if we do manipulations as well using the units package\n\nunits::set_units(l, \"km\")\n94980.15 [km]\nunits::set_units(l, \"mile\")\n59017.93 [mile]\n\nunits::set_units(a, \"ha\")\n783760974 [ha]\nunits::set_units(a, \"km2\")\n7837610 [km^2]\nunits::set_units(a, \"in2\")\n1.214832e+16 [in^2]"
  },
  {
    "objectID": "slides/week-2-1.html#units-are-a-class",
    "href": "slides/week-2-1.html#units-are-a-class",
    "title": "Week 2",
    "section": "Units are a class",
    "text": "Units are a class\n\nunits are an S3 data object with attribute information and ‚Äúrules of engagement‚Äù\n\n\nclass(st_length(conus)) \n[1] \"units\"\nattributes(st_length(conus)) |&gt; unlist()\nunits.numerator           class \n            \"m\"         \"units\" \n\nst_length(conus) + 100\nError in Ops.units(st_length(conus), 100): both operands of the expression should be \"units\" objects\n\nconus |&gt; \n  mutate(area = st_area(.)) |&gt; \n  ggplot(aes(x = name, y = area)) + \n  geom_col()\nError in `stopifnot()`:\n‚Ñπ In argument: `area = st_area(.)`.\nCaused by error:\n! object '.' not found"
  },
  {
    "objectID": "slides/week-2-1.html#unit-values-can-be-stripped-of-their-attributes-if-need-be",
    "href": "slides/week-2-1.html#unit-values-can-be-stripped-of-their-attributes-if-need-be",
    "title": "Week 2",
    "section": "Unit values can be stripped of their attributes if need be:",
    "text": "Unit values can be stripped of their attributes if need be:\n\n# Via drop_units\n(units::drop_units(sum(st_length(conus))))\n[1] 94980149\n\n# Via casting\n(as.numeric(sum(st_length(conus))))\n[1] 94980149"
  },
  {
    "objectID": "slides/leaflet-examples.html",
    "href": "slides/leaflet-examples.html",
    "title": "Interactive Mapping in R",
    "section": "",
    "text": "This is the supplementary notes and examples for an introduction to leaflet.\nIt is based on data and examples seen in class\nBuilt on the excellent tutorial from RStudio\n\nIf not already installed, install leaflet:\n\ninstall.packages(\"leaflet\")\n\nThen attach the library:\n\nlibrary(leaflet)\n\nBring in the other needed libraries:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(USAboundaries)\n\n\n\nBasic Usage\nCreating a Leaflet map requires a few basic steps (not dissimilar to ggplot):\n\nInitialize a map widget by calling leaflet().\nAdd layers (i.e., features) to the map by using layer functions (e.g.¬†addTiles, addMarkers, addPolygons,‚Ä¶)\nPrint the map widget to display it.\n\n\nHere‚Äôs a basic example:\n\nleaflet() |&gt;\n  addTiles() |&gt;\n  addMarkers(lng=-105.0848, lat=40.5729, popup=\"CSU\")\n\n\n\n\n\n\n\nBy default, leaflet sets the view of the map to the range of latitude/longitude data in the map layers\nYou can adjust these if needed using:\n\nsetView(): sets the center of the map view and the zoom level;\nfitBounds(): fits the view into the rectangle [lng1, lat1] ‚Äì [lng2, lat2];\nclearBounds() clears the bound\n\n\n\n\n\nBasemaps\n\nDefault (OpenStreetMap) Tiles\nThe easiest way to add tiles is by calling addTiles() with no arguments; by default, OpenStreetMap tiles are used.\n\nleaflet() |&gt; \n  setView(lng=-105.0848, lat=40.5729, zoom = 16) |&gt; \n  addTiles()\n\n\n\n\n\n\n\nThird-Party Tiles\n\nMany third-party basemaps can be added using the addProviderTiles() function\nAs a convenience, leaflet provides a named list of all the third-party tile providers supported by the plugin: just type providers$ and choose from one of the options.\n\n\nlength(providers)\n\n#&gt; [1] 233\n\nnames(providers) |&gt; \n  head()\n\n#&gt; [1] \"OpenStreetMap\"        \"OpenStreetMap.Mapnik\" \"OpenStreetMap.DE\"    \n#&gt; [4] \"OpenStreetMap.CH\"     \"OpenStreetMap.France\" \"OpenStreetMap.HOT\"\n\n\n\nNote that some tile set providers require you to register. You can pass access tokens/keys, and other options, to the tile provider by populating the options argument with the providerTileOptions() function.\n\n\nI would personal stick with the OSM, CartoDB, ESRI, and Stamen servers ‚Ä¶ [see more here] (https://leaflet-extras.github.io/leaflet-providers/preview/)\n\nBelow are a few examples:\n\n\nCartoDB\n\nleaflet() |&gt; \n  setView(lng=-105.0848, lat=40.5729, zoom = 16) |&gt; \n  addProviderTiles(providers$CartoDB)\n\n\n\n\n\n\n\nESRI Imagery\n\nleaflet() |&gt; \n  setView(lng=-105.0848, lat=40.5729, zoom = 16) |&gt; \n  addProviderTiles(providers$Esri.WorldImagery)\n\n\n\n\n\n\n\nCombining Tile Layers\nYou can stack multiple tile layers if the front tiles have some level of opacity. Here we layer the Stamen.TonerLines with aerial imagery\n\nleaflet() |&gt; \n  setView(lng=-105.0848, lat=40.5729, zoom = 14) |&gt; \n  addProviderTiles(providers$Esri.WorldImagery) |&gt; \n  addProviderTiles(providers$Stadia.StamenTonerLines,\n                   options = providerTileOptions(opacity = .5)) \n\n\n\n\n\n\n\n\n\nMarkers\nMarkers are one way to identify point information on a map:\n\n\nExample Starbucks data:\n\n(starbucks = read_csv('data/directory.csv') |&gt; \n  filter(City %in% c(\"Fort Collins\", \"Loveland\"),\n         `State/Province` == \"CO\") |&gt; \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |&gt; \n  select(store_name = `Store Name`, phone = `Phone Number`, address = `Street Address`, city = City, brand = Brand))\n\n#&gt; Simple feature collection with 21 features and 5 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -105.12 ymin: 40.38 xmax: -105.01 ymax: 40.61\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 21 √ó 6\n#&gt;    store_name                phone address city  brand        geometry\n#&gt;    &lt;chr&gt;                     &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;POINT [¬∞]&gt;\n#&gt;  1 Harmony & Timberline-For‚Ä¶ 970-‚Ä¶ 4609 S‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.04 40.52)\n#&gt;  2 Safeway-Fort Collins #10‚Ä¶ 970-‚Ä¶ 460A S‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.08 40.58)\n#&gt;  3 Scotch Pines              970-‚Ä¶ 2601 S‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.06 40.55)\n#&gt;  4 King Soopers-Fort Collin‚Ä¶ 970-‚Ä¶ 2602 S‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.04 40.55)\n#&gt;  5 Safeway-Fort Collins #29‚Ä¶ 970-‚Ä¶ 2160 W‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.12 40.55)\n#&gt;  6 Harmony & JFK             (970‚Ä¶ 250 E.‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.07 40.52)\n#&gt;  7 Safeway-Fort Collins #15‚Ä¶ 970-‚Ä¶ 1426 E‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.05 40.52)\n#&gt;  8 SuperTarget Fort Collins‚Ä¶ &lt;NA&gt;  2936 C‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.08 40.53)\n#&gt;  9 King Soopers-Fort Collin‚Ä¶ 970-‚Ä¶ 1015 S‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.12 40.57)\n#&gt; 10 King Soopers-Ft. Collins‚Ä¶ 970-‚Ä¶ 1842 N‚Ä¶ Fort‚Ä¶ Star‚Ä¶ (-105.08 40.61)\n#&gt; # ‚Ñπ 11 more rows\n\n\n\n\nMarkers\nMarkers are added using the addMarkers or addAwesomeMarkers\nTheir default appearance is a blue dropped pin.\nAs with most layer functions, - the popup argument adds a message to be displayed on click - the label argument display a text label either on hover\n\nleaflet() |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addMarkers(data = starbucks, popup = ~store_name, label = ~city)\n\n\n\n\n\n\n\nAwesome Icons\nUsing the Font Awesome Icons seen in lab one, we can make markers with more specific coloring and icons\n\nHere we define the icon as a green marker with a coffee icon from the fa library\nFor fun we can make the coffee cups spin‚Ä¶\n\nWe then use addAwesomeMarkers to spcifiy the icon we created using the icon argument:\n\nicons = awesomeIcons(icon = 'coffee', markerColor = \"green\", library = 'fa', spin = TRUE)\n\nleaflet(data = starbucks) |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addAwesomeMarkers(icon = icons, popup = ~store_name)\n\n\n\n\n\n\n\nCustom Popups\nYou can use HTML, CSS, and Java Script to modify your pop-ups\nFor example, we can associate the name of the Starbucks locations with their google maps URL as an hyper reference (href):\n\nstarbucks = starbucks |&gt; \n  mutate(url = paste0('https://www.google.com/maps/place/',\n                      gsub(\" \", \"+\", address), \"+\",\n                      gsub(\" \", \"+\", city)))\n\npop = paste0('&lt;a href=', starbucks$url, '&gt;', starbucks$store_name, \"&lt;/a&gt;\")\nhead(pop)\n\n#&gt; [1] \"&lt;a href=https://www.google.com/maps/place/4609+S+Timberline+Rd,+Unit+A101+Fort+Collins&gt;Harmony & Timberline-Fort Collins&lt;/a&gt;\"\n#&gt; [2] \"&lt;a href=https://www.google.com/maps/place/460A+South+College+Fort+Collins&gt;Safeway-Fort Collins #1071&lt;/a&gt;\"                    \n#&gt; [3] \"&lt;a href=https://www.google.com/maps/place/2601+S+Lemay,+Ste+130+Fort+Collins&gt;Scotch Pines&lt;/a&gt;\"                               \n#&gt; [4] \"&lt;a href=https://www.google.com/maps/place/2602+S+Timberline+Rd+Fort+Collins&gt;King Soopers-Fort Collins #97&lt;/a&gt;\"               \n#&gt; [5] \"&lt;a href=https://www.google.com/maps/place/2160+W+Drake+Rd,+Unit+6+Fort+Collins&gt;Safeway-Fort Collins #2913&lt;/a&gt;\"               \n#&gt; [6] \"&lt;a href=https://www.google.com/maps/place/250+E.+Harmony+Road+Fort+Collins&gt;Harmony & JFK&lt;/a&gt;\"\n\n\nWe can then add our custom popup to our icons:\n\nleaflet(data = starbucks) |&gt;\n  addProviderTiles(providers$CartoDB) |&gt; \n  addAwesomeMarkers(icon = icons, \n                    label = ~address, popup = pop)\n\n\n\n\n\n\n\nCircle Markers\nCircle markers are much like regular circles (shapes), except their radius in onscreen pixels stays constant regardless of zoom level (z).\n\nleaflet(data = starbucks) |&gt;\n  addProviderTiles(providers$CartoDB) |&gt; \n  addCircleMarkers(label = ~address, popup = pop)\n\n\n\n\n\n\nMarker Clustering\nSometimes while mapping many points, it is useful to cluster them. For example lets plot all starbucks in the world!\n\nall_co = read_csv('data/directory.csv') |&gt; \n  filter(!is.na(Latitude)) |&gt; \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) \n\nleaflet(data = all_co) |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addMarkers(clusterOptions = markerClusterOptions())\n\n\n\n\n\n\n\n\n\n\nAdding color ramps\n\nColors can be add by factor, numeric, bins, or quartiles using the built in leaflet functions\nEach of these are defined by a palette, and a domain\n\nThe palette argument can be any of the following:\n\nA character vector of RGB or named colors.\n\nExamples: palette(), c(‚Äú#000000‚Äù, ‚Äú#0000FF‚Äù, ‚Äú#FFFFFF‚Äù), topo.colors(10)\n\nThe name of an RColorBrewer palette\n\nExamples: ‚ÄúBuPu‚Äù or ‚ÄúGreens‚Äù.\n\nThe full name of a viridis palette:\n\nExamples: ‚Äúviridis‚Äù, ‚Äúmagma‚Äù, ‚Äúinferno‚Äù, or ‚Äúplasma‚Äù.\n\nA function that receives a single value between 0 and 1 and returns a color.\n\nExamples: colorRamp(c(‚Äú#000000‚Äù, ‚Äú#FFFFFF‚Äù), interpolate = ‚Äúspline‚Äù).\n\n\nThe domain is the values - named by variable - that the color palette should range over\n\n# ?colorFactor\n\n# Create a palette that maps factor levels to colors\npal &lt;- colorFactor(c(\"darkgreen\", \"navy\"), domain = c(\"Goleta\", \"Santa Barbara\"))\n\nleaflet(data = starbucks) |&gt; addProviderTiles(providers$CartoDB) |&gt; \n  addCircleMarkers(color = ~pal(city), fillOpacity = .5, stroke = FALSE)\n\n\n\n\n\n\n\n\nShapes (Polylines, Polygons, Circles)\n\nGetting some data ‚Ä¶\n\n(covid = readr::read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/live/us-states.csv') |&gt; \n  filter(date == max(date)) |&gt; \n  right_join(USAboundaries::us_states(), by = c(\"state\" = \"name\")) |&gt; \n  filter(!stusps %in% c(\"AK\",\"PR\", \"HI\")) |&gt; \n  st_as_sf())\n\n#&gt; Simple feature collection with 49 features and 16 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7258 ymin: 24.49813 xmax: -66.9499 ymax: 49.38436\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 49 √ó 17\n#&gt;    date       state    fips   cases deaths statefp statens affgeoid geoid stusps\n#&gt;    &lt;date&gt;     &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; \n#&gt;  1 2023-03-24 Tenness‚Ä¶ 47    2.46e6  29035 47      013258‚Ä¶ 0400000‚Ä¶ 47    TN    \n#&gt;  2 2023-03-24 Michigan 26    3.07e6  42311 26      017797‚Ä¶ 0400000‚Ä¶ 26    MI    \n#&gt;  3 2023-03-24 Massach‚Ä¶ 25    2.23e6  24441 25      006069‚Ä¶ 0400000‚Ä¶ 25    MA    \n#&gt;  4 2023-03-24 Maryland 24    1.37e6  16672 24      017149‚Ä¶ 0400000‚Ä¶ 24    MD    \n#&gt;  5 2023-03-24 Iowa     19    9.07e5  10770 19      017797‚Ä¶ 0400000‚Ä¶ 19    IA    \n#&gt;  6 2023-03-24 Maine    23    3.20e5   2981 23      017797‚Ä¶ 0400000‚Ä¶ 23    ME    \n#&gt;  7 2023-03-24 Texas    48    8.45e6  94518 48      017798‚Ä¶ 0400000‚Ä¶ 48    TX    \n#&gt;  8 2023-03-24 Louisia‚Ä¶ 22    1.58e6  18835 22      016295‚Ä¶ 0400000‚Ä¶ 22    LA    \n#&gt;  9 2023-03-24 Kansas   20    9.41e5  10232 20      004818‚Ä¶ 0400000‚Ä¶ 20    KS    \n#&gt; 10 2023-03-24 Kentucky 21    1.72e6  18348 21      017797‚Ä¶ 0400000‚Ä¶ 21    KY    \n#&gt; # ‚Ñπ 39 more rows\n#&gt; # ‚Ñπ 7 more variables: lsad &lt;chr&gt;, aland &lt;dbl&gt;, awater &lt;dbl&gt;, state_name &lt;chr&gt;,\n#&gt; #   state_abbr &lt;chr&gt;, jurisdiction_type &lt;chr&gt;, geometry &lt;MULTIPOLYGON [¬∞]&gt;\n\n\n\n\nPolygons\nAdding those cases counts to polygons over a YlOrRd color ramp\n\nleaflet() |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addPolygons(data = covid, \n              fillColor  = ~colorQuantile(\"YlOrRd\", cases)(cases),\n              color = NA,\n              label = ~state_name)\n\n\n\n\n\n\n\nCircles\n\nleaflet() |&gt; \n  addProviderTiles(providers$CartoDB.DarkMatter) |&gt; \n  addCircles(data = st_centroid(covid), \n             fillColor  = ~colorQuantile(\"YlOrRd\", cases)(cases),\n             color = NA,\n             fillOpacity = .5,\n             radius = ~cases/50,\n             label = ~state)\n\n\n\n\n\n\n\n\nWeb based data\n\nUSGS Gage near UCSB: ID-11120000\nhttps://waterdata.usgs.gov/monitoring-location/11120000/\n\n\n\n\n\n\n\n\n\n\n\nNetwork Linked Data (GeoJSON)\nhttps://labs.waterdata.usgs.gov/api/nldi/linked-data/nwissite/USGS-11120000/navigate/UT\nTrance the Upper Tributary (UT) of the USGS-11120000\n\n\n\n\n\n\n\n\n\n\n\nAdding ‚ÄúWeb data‚Äù to the map\n\nid = \"11120000\"\n\n# base URL\n(base = dataRetrieval:::pkg.env$nldi_base)\n\n#&gt; [1] \"https://api.water.usgs.gov/nldi/linked-data/\"\n\n# Reading sf for URLs in line\nleaflet() |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addPolylines(data = read_sf(paste0(base,'nwissite/USGS-',id,'/navigate/UT'))) |&gt; \n  addPolygons(data =  read_sf(paste0(base,'nwissite/USGS-',id,'/basin')), \n              fillColor =  \"transparent\", color = \"black\")\n\n\n\n\n\n\n\n\nWMS Tiles\nWMS tiles can be added directly to a map. Here we use the NEXRAD rainfall information (refelctivity) from the Iowa Mesonet Program\n(You may need to scroll out to find an)\n\nconus = filter(us_states(), !stusps %in% c(\"AK\", \"PR\", \"HI\"))\n\nleaflet() |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addPolygons(data = st_union(conus), fillColor = \"transparent\",\n              color = \"black\", weight = 1) |&gt; \n  addWMSTiles(\n    \"http://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi\",\n    layers = \"nexrad-n0r-900913\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE)\n  )\n\n\n\n\n\n\n\n\nLayer Controls\nUses Leaflet‚Äôs built-in layers control you can choose one of several base layers, and any number of overlay layers to view.\nBy defining groups, you have the ability to toogle layers, and overlays on/off.\n\nleaflet() |&gt; \n  addProviderTiles(providers$CartoDB, group = \"Grayscale\") |&gt; \n  addProviderTiles(providers$Esri.WorldTerrain, group = \"Terrain\") |&gt; \n  addPolylines(data = read_sf(paste0(base,'nwissite/USGS-',id,'/navigate/UT'))) |&gt; \n  addPolygons(data =  read_sf(paste0(base,'nwissite/USGS-',id,'/basin')), fillColor =  \"transparent\", color = \"black\", group = \"basin\") |&gt; \n  addWMSTiles(\"http://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi\", layers = \"nexrad-n0r-900913\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE)) |&gt; \n  addLayersControl(overlayGroups = c(\"basin\"), baseGroups = c(\"Terrain\", \"Grayscale\"))\n\n\n\n‚ÄòFunction-ize‚Äô\nYou can wrap your mapping code in functions to allow reusability\n\nwatershed_map = function(gage_id){\nleaflet() |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addPolylines(data = read_sf(paste0(base,'nwissite/USGS-',gage_id,'/navigate/UT'))) |&gt; \n  addPolygons(data =  read_sf(paste0(base,'nwissite/USGS-',gage_id,'/basin')), \n              fillColor =  \"transparent\", color = \"black\", group = \"basin\") |&gt; \n  addWMSTiles(\"http://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi\", layers = \"nexrad-n0r-900913\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE))\n}\n\n\nwatershed_map(\"06752260\")\n\n\n\n\n\n\nAdding Details\n\nMeasures, graticules, and inset maps\n\n\nwatershed_map(\"06752260\") |&gt; \n  addMeasure() |&gt; \n  addGraticule() |&gt; \n  addMiniMap()\n\n\n\n\n\n\n\nleafem (new library)\n\nHome buttons and Mouse Coordinates\nSupport for raster and stars objects (to come)\n\n\nwatershed_map(\"06752260\") |&gt; \n  addMeasure() |&gt; \n  addGraticule() |&gt; \n  leafem::addHomeButton(group = \"basin\") |&gt; \n  leafem::addMouseCoordinates() \n\n\n\n\n\n\n\nleafpop (new library)\n\nPopup Tables\n\n\nleaflet(data = starbucks) |&gt; \n  addProviderTiles(providers$CartoDB) |&gt; \n  addCircleMarkers(\n    color = ~pal(city), \n    fillOpacity = .5,\n    stroke = FALSE, \n    popup = leafpop::popupTable(starbucks)\n  )\n\n\n\n\n\nMaking that table nicer‚Ä¶\n\nleaflet(data = starbucks) |&gt; addProviderTiles(providers$CartoDB) |&gt; \n  addCircleMarkers(\n    color = ~pal(city), fillOpacity = .5,\n    stroke = FALSE, \n    popup = leafpop::popupTable(st_drop_geometry(starbucks[,1:5]), feature.id = FALSE, row.numbers = FALSE)\n  )\n\n\n\n\n\n\n\n\nMapview\n\neasy, but less control\nSupport for raster and stars objects (to come in our class)\nImplements many of the leafem and leafpop functionalities\n\n\nlibrary(mapview)\nmapview(starbucks)"
  },
  {
    "objectID": "slides/week-2.html#section",
    "href": "slides/week-2.html#section",
    "title": "Week 2",
    "section": "",
    "text": "The remaining geometries 10 are rarer, but increasingly find implementations:\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nCIRCULARSTRING\nThe CIRCULARSTRING is the basic curve type, similar to a LINESTRING in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the center of the arc, i.e., the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LINESTRING. This means that a valid circular string must have an odd number of points greater than 1.\n\n\nCOMPOUNDCURVE\nA compound curve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component.\n\n\nCURVEPOLYGON\nExample compound curve in a curve polygon: CURVEPOLYGON(COMPOUNDCURVE(CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1) )\n\n\nMULTICURVE\nA MultiCurve is a 1-dimensional GeometryCollection whose elements are Curves, it can include linear strings, circular strings or compound strings.\n\n\nMULTISURFACE\nA MultiSurface is a 2-dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system.\n\n\nCURVE\nA Curve is a 1-dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points\n\n\nSURFACE\nA Surface is a 2-dimensional geometric object\n\n\nPOLYHEDRALSURFACE\nA PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments\n\n\nTIN\nA TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches.\n\n\nTRIANGLE\nA Triangle is a polygon with 3 distinct, non-collinear vertices and no interior boundary"
  },
  {
    "objectID": "slides/week-2.html#so-far",
    "href": "slides/week-2.html#so-far",
    "title": "Week 2",
    "section": "So far ‚Ä¶",
    "text": "So far ‚Ä¶\nWe‚Äôve discussed the simple feature standard\n\nGeometries (type, dimension, and structure)\n\n- Empty, Valid, Simple\n\nEncoding (WKT & WKB)\nA set of operations\n\nAnd the implementation of the simple features standard in R\n\nsfg: a single feature geometry\nsfc: a set of geometries (sfg) stored as a list\nsf: a sfc list joined with a data.frame (attributes)\n\nThis R implementation is ideal/special because it achieves the simple feature abstract goal of:\n\n‚ÄúA simple feature is defined by the OpenGIS Abstract specification to have both spatial and non-spatial attributes‚Ä¶‚Äù - standard.\n\nThe shapefile/GIS traditional GIS view does not do this and seperates geometry (shp), from projection (prj), from data (dbf) and relates them through an shx file"
  },
  {
    "objectID": "slides/week-2.html#integration-with-tidyverse",
    "href": "slides/week-2.html#integration-with-tidyverse",
    "title": "Week 2",
    "section": "Integration with tidyverse",
    "text": "Integration with tidyverse\n\nWe saw how the dplyr verbs still work on an sf object since sf extends the data.frame class\nHow geom_sf support mapping (‚Äúspatial plotting‚Äù) in ggplot\nHow to read spatial data into R via GDAL drivers:\n\nspatial files (read_sf)\nflat files via st_as_sf\n\nIntegration with a few GEOS geometry operations like:\n\nst_combine()\nst_union()"
  },
  {
    "objectID": "slides/week-2.html#taki",
    "href": "slides/week-2.html#taki",
    "title": "Week 2",
    "section": "Taki ‚Ä¶",
    "text": "Taki ‚Ä¶\n\n\n\nconus &lt;-  USAboundaries::us_states() |&gt;\n  filter(!state_name %in% c(\"Puerto Rico\", \n                            \"Alaska\", \n                            \"Hawaii\"))\n\nlength(st_geometry(conus))\n#&gt; [1] 49"
  },
  {
    "objectID": "slides/week-2.html#feature-resoloved-and-combined",
    "href": "slides/week-2.html#feature-resoloved-and-combined",
    "title": "Week 2",
    "section": "1 feature: resoloved and combined:",
    "text": "1 feature: resoloved and combined:\n\nst_cast / st_union work on sfg, sfc, and sf objects:\n\n\nconus &lt;- AOI::aoi_get(state = \"conus\")\n\nus_c_ml = st_combine(conus) |&gt;\n  st_cast(\"MULTILINESTRING\")\n   \nus_u_ml = st_union(conus) |&gt;\n  st_cast(\"MULTILINESTRING\")"
  },
  {
    "objectID": "slides/week-2.html#so-what",
    "href": "slides/week-2.html#so-what",
    "title": "Week 2",
    "section": "So what?",
    "text": "So what?\nLets imagine we want to know the distance from Denver to the nearest state border:\nTo do this, we need to:\n\n1: define Denver as a geometry in a CRS\n2: determine the correct geometry types / representation\n3: calculate the distance between (1) and (2)"
  },
  {
    "objectID": "slides/week-2.html#make-denver-in-the-crs-of-our-states",
    "href": "slides/week-2.html#make-denver-in-the-crs-of-our-states",
    "title": "Week 2",
    "section": "1. Make ‚ÄúDenver‚Äù in the CRS of our states",
    "text": "1. Make ‚ÄúDenver‚Äù in the CRS of our states\n\ndenver_sf &lt;- data.frame(y = 39.7392, \n                       x = -104.9903, \n                       name = \"Denver\") |&gt; \n  st_as_sf(coords = c(\"x\", \"y\"), \n           crs = 4326)"
  },
  {
    "objectID": "slides/week-2.html#determine-the-3-closest-states",
    "href": "slides/week-2.html#determine-the-3-closest-states",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus\n\n\n\n#&gt; Simple feature collection with 49 features and 14 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_region state_division feature_code           state_name state_abbr\n#&gt; 1             3              6      1779775              Alabama         AL\n#&gt; 2             4              8      1779777              Arizona         AZ\n#&gt; 3             3              7      0068085             Arkansas         AR\n#&gt; 4             4              9      1779778           California         CA\n#&gt; 5             4              8      1779779             Colorado         CO\n#&gt; 6             1              1      1779780          Connecticut         CT\n#&gt; 7             3              5      1779781             Delaware         DE\n#&gt; 8             3              5      1702382 District of Columbia         DC\n#&gt; 9             3              5      0294478              Florida         FL\n#&gt; 10            3              5      1705317              Georgia         GA\n#&gt;                    name fip_class tiger_class combined_area_code\n#&gt; 1               Alabama      &lt;NA&gt;       G4000                 NA\n#&gt; 2               Arizona      &lt;NA&gt;       G4000                 NA\n#&gt; 3              Arkansas      &lt;NA&gt;       G4000                 NA\n#&gt; 4            California      &lt;NA&gt;       G4000                 NA\n#&gt; 5              Colorado      &lt;NA&gt;       G4000                 NA\n#&gt; 6           Connecticut      &lt;NA&gt;       G4000                 NA\n#&gt; 7              Delaware      &lt;NA&gt;       G4000                 NA\n#&gt; 8  District of Columbia      &lt;NA&gt;       G4000                 NA\n#&gt; 9               Florida      &lt;NA&gt;       G4000                 NA\n#&gt; 10              Georgia      &lt;NA&gt;       G4000                 NA\n#&gt;    metropolitan_area_code functional_status    land_area  water_area fip_code\n#&gt; 1                    &lt;NA&gt;                 A 131175477769  4591897964       01\n#&gt; 2                    &lt;NA&gt;                 A 294363973043   855871553       04\n#&gt; 3                    &lt;NA&gt;                 A 134660767709  3121950081       05\n#&gt; 4                    &lt;NA&gt;                 A 403671756816 20293573058       06\n#&gt; 5                    &lt;NA&gt;                 A 268418796417  1185716938       08\n#&gt; 6                    &lt;NA&gt;                 A  12541690473  1816424193       09\n#&gt; 7                    &lt;NA&gt;                 A   5046731559  1399179670       10\n#&gt; 8                    &lt;NA&gt;                 A    158316124    18709762       11\n#&gt; 9                    &lt;NA&gt;                 A 138961722096 45972570361       12\n#&gt; 10                   &lt;NA&gt;                 A 149486624386  4418360134       13\n#&gt;                          geometry\n#&gt; 1  MULTIPOLYGON (((-85.4883 30...\n#&gt; 2  MULTIPOLYGON (((-110.7507 3...\n#&gt; 3  MULTIPOLYGON (((-90.95577 3...\n#&gt; 4  MULTIPOLYGON (((-116.1062 3...\n#&gt; 5  MULTIPOLYGON (((-105.155 36...\n#&gt; 6  MULTIPOLYGON (((-72.5279 41...\n#&gt; 7  MULTIPOLYGON (((-75.13846 3...\n#&gt; 8  MULTIPOLYGON (((-77.00244 3...\n#&gt; 9  MULTIPOLYGON (((-83.10874 2...\n#&gt; 10 MULTIPOLYGON (((-81.09538 3..."
  },
  {
    "objectID": "slides/week-2.html#determine-the-3-closest-states-1",
    "href": "slides/week-2.html#determine-the-3-closest-states-1",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name)\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;              state_name                       geometry\n#&gt; 1               Alabama MULTIPOLYGON (((-85.4883 30...\n#&gt; 2               Arizona MULTIPOLYGON (((-110.7507 3...\n#&gt; 3              Arkansas MULTIPOLYGON (((-90.95577 3...\n#&gt; 4            California MULTIPOLYGON (((-116.1062 3...\n#&gt; 5              Colorado MULTIPOLYGON (((-105.155 36...\n#&gt; 6           Connecticut MULTIPOLYGON (((-72.5279 41...\n#&gt; 7              Delaware MULTIPOLYGON (((-75.13846 3...\n#&gt; 8  District of Columbia MULTIPOLYGON (((-77.00244 3...\n#&gt; 9               Florida MULTIPOLYGON (((-83.10874 2...\n#&gt; 10              Georgia MULTIPOLYGON (((-81.09538 3..."
  },
  {
    "objectID": "slides/week-2.html#determine-the-3-closest-states-2",
    "href": "slides/week-2.html#determine-the-3-closest-states-2",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name) %&gt;%\n  mutate(dist = st_distance(., denver_sf))\n\n\n\n#&gt; Simple feature collection with 49 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;              state_name                       geometry          dist\n#&gt; 1               Alabama MULTIPOLYGON (((-85.4883 30... 1571007.9 [m]\n#&gt; 2               Arizona MULTIPOLYGON (((-110.7507 3...  466602.9 [m]\n#&gt; 3              Arkansas MULTIPOLYGON (((-90.95577 3...  975519.6 [m]\n#&gt; 4            California MULTIPOLYGON (((-116.1062 3... 1000950.5 [m]\n#&gt; 5              Colorado MULTIPOLYGON (((-105.155 36...       0.0 [m]\n#&gt; 6           Connecticut MULTIPOLYGON (((-72.5279 41... 2636635.7 [m]\n#&gt; 7              Delaware MULTIPOLYGON (((-75.13846 3... 2485997.1 [m]\n#&gt; 8  District of Columbia MULTIPOLYGON (((-77.00244 3... 2388920.9 [m]\n#&gt; 9               Florida MULTIPOLYGON (((-83.10874 2... 1847447.4 [m]\n#&gt; 10              Georgia MULTIPOLYGON (((-81.09538 3... 1788781.6 [m]"
  },
  {
    "objectID": "slides/week-2.html#determine-the-3-closest-states-3",
    "href": "slides/week-2.html#determine-the-3-closest-states-3",
    "title": "Week 2",
    "section": "2. Determine the 3 closest states:",
    "text": "2. Determine the 3 closest states:\n\n\n\nconus |&gt;\n  select(state_name) %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3)\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0546 ymin: 36.99246 xmax: -95.30829 ymax: 45.00582\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado      0.0 [m] MULTIPOLYGON (((-105.155 36...\n#&gt; 2    Wyoming 140002.6 [m] MULTIPOLYGON (((-106.3212 4...\n#&gt; 3   Nebraska 161243.2 [m] MULTIPOLYGON (((-95.93779 4...\n\n\n\nThat‚Äôs close, but the distance to Colorado is 0, that‚Äôs not a state border."
  },
  {
    "objectID": "slides/week-2.html#geometry-selection",
    "href": "slides/week-2.html#geometry-selection",
    "title": "Week 2",
    "section": "Geometry Selection",
    "text": "Geometry Selection\n\nPolygon (therefore MULTIPOLGYGONS) describe areas!\nThe distance to a point in a polygon to that polygon is 0."
  },
  {
    "objectID": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation",
    "href": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus\n\n\n\n#&gt; Simple feature collection with 49 features and 14 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    state_region state_division feature_code           state_name state_abbr\n#&gt; 1             3              6      1779775              Alabama         AL\n#&gt; 2             4              8      1779777              Arizona         AZ\n#&gt; 3             3              7      0068085             Arkansas         AR\n#&gt; 4             4              9      1779778           California         CA\n#&gt; 5             4              8      1779779             Colorado         CO\n#&gt; 6             1              1      1779780          Connecticut         CT\n#&gt; 7             3              5      1779781             Delaware         DE\n#&gt; 8             3              5      1702382 District of Columbia         DC\n#&gt; 9             3              5      0294478              Florida         FL\n#&gt; 10            3              5      1705317              Georgia         GA\n#&gt;                    name fip_class tiger_class combined_area_code\n#&gt; 1               Alabama      &lt;NA&gt;       G4000                 NA\n#&gt; 2               Arizona      &lt;NA&gt;       G4000                 NA\n#&gt; 3              Arkansas      &lt;NA&gt;       G4000                 NA\n#&gt; 4            California      &lt;NA&gt;       G4000                 NA\n#&gt; 5              Colorado      &lt;NA&gt;       G4000                 NA\n#&gt; 6           Connecticut      &lt;NA&gt;       G4000                 NA\n#&gt; 7              Delaware      &lt;NA&gt;       G4000                 NA\n#&gt; 8  District of Columbia      &lt;NA&gt;       G4000                 NA\n#&gt; 9               Florida      &lt;NA&gt;       G4000                 NA\n#&gt; 10              Georgia      &lt;NA&gt;       G4000                 NA\n#&gt;    metropolitan_area_code functional_status    land_area  water_area fip_code\n#&gt; 1                    &lt;NA&gt;                 A 131175477769  4591897964       01\n#&gt; 2                    &lt;NA&gt;                 A 294363973043   855871553       04\n#&gt; 3                    &lt;NA&gt;                 A 134660767709  3121950081       05\n#&gt; 4                    &lt;NA&gt;                 A 403671756816 20293573058       06\n#&gt; 5                    &lt;NA&gt;                 A 268418796417  1185716938       08\n#&gt; 6                    &lt;NA&gt;                 A  12541690473  1816424193       09\n#&gt; 7                    &lt;NA&gt;                 A   5046731559  1399179670       10\n#&gt; 8                    &lt;NA&gt;                 A    158316124    18709762       11\n#&gt; 9                    &lt;NA&gt;                 A 138961722096 45972570361       12\n#&gt; 10                   &lt;NA&gt;                 A 149486624386  4418360134       13\n#&gt;                          geometry\n#&gt; 1  MULTIPOLYGON (((-85.4883 30...\n#&gt; 2  MULTIPOLYGON (((-110.7507 3...\n#&gt; 3  MULTIPOLYGON (((-90.95577 3...\n#&gt; 4  MULTIPOLYGON (((-116.1062 3...\n#&gt; 5  MULTIPOLYGON (((-105.155 36...\n#&gt; 6  MULTIPOLYGON (((-72.5279 41...\n#&gt; 7  MULTIPOLYGON (((-75.13846 3...\n#&gt; 8  MULTIPOLYGON (((-77.00244 3...\n#&gt; 9  MULTIPOLYGON (((-83.10874 2...\n#&gt; 10 MULTIPOLYGON (((-81.09538 3..."
  },
  {
    "objectID": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-1",
    "href": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-1",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name)\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;              state_name                       geometry\n#&gt; 1               Alabama MULTIPOLYGON (((-85.4883 30...\n#&gt; 2               Arizona MULTIPOLYGON (((-110.7507 3...\n#&gt; 3              Arkansas MULTIPOLYGON (((-90.95577 3...\n#&gt; 4            California MULTIPOLYGON (((-116.1062 3...\n#&gt; 5              Colorado MULTIPOLYGON (((-105.155 36...\n#&gt; 6           Connecticut MULTIPOLYGON (((-72.5279 41...\n#&gt; 7              Delaware MULTIPOLYGON (((-75.13846 3...\n#&gt; 8  District of Columbia MULTIPOLYGON (((-77.00244 3...\n#&gt; 9               Florida MULTIPOLYGON (((-83.10874 2...\n#&gt; 10              Georgia MULTIPOLYGON (((-81.09538 3..."
  },
  {
    "objectID": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-2",
    "href": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-2",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\")\n\n\n\n#&gt; Simple feature collection with 49 features and 1 field\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;              state_name                       geometry\n#&gt; 1               Alabama MULTILINESTRING ((-85.4883 ...\n#&gt; 2               Arizona MULTILINESTRING ((-110.7507...\n#&gt; 3              Arkansas MULTILINESTRING ((-90.95577...\n#&gt; 4            California MULTILINESTRING ((-116.1062...\n#&gt; 5              Colorado MULTILINESTRING ((-105.155 ...\n#&gt; 6           Connecticut MULTILINESTRING ((-72.5279 ...\n#&gt; 7              Delaware MULTILINESTRING ((-75.13846...\n#&gt; 8  District of Columbia MULTILINESTRING ((-77.00244...\n#&gt; 9               Florida MULTILINESTRING ((-83.10874...\n#&gt; 10              Georgia MULTILINESTRING ((-81.09538..."
  },
  {
    "objectID": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-3",
    "href": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-3",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf))\n\n\n\n#&gt; Simple feature collection with 49 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;              state_name                       geometry          dist\n#&gt; 1               Alabama MULTILINESTRING ((-85.4883 ... 1571007.9 [m]\n#&gt; 2               Arizona MULTILINESTRING ((-110.7507...  466602.9 [m]\n#&gt; 3              Arkansas MULTILINESTRING ((-90.95577...  975519.6 [m]\n#&gt; 4            California MULTILINESTRING ((-116.1062... 1000950.5 [m]\n#&gt; 5              Colorado MULTILINESTRING ((-105.155 ...  140002.6 [m]\n#&gt; 6           Connecticut MULTILINESTRING ((-72.5279 ... 2636635.7 [m]\n#&gt; 7              Delaware MULTILINESTRING ((-75.13846... 2485997.1 [m]\n#&gt; 8  District of Columbia MULTILINESTRING ((-77.00244... 2388920.9 [m]\n#&gt; 9               Florida MULTILINESTRING ((-83.10874... 1847447.4 [m]\n#&gt; 10              Georgia MULTILINESTRING ((-81.09538... 1788781.6 [m]"
  },
  {
    "objectID": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-4",
    "href": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-4",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3)\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0546 ymin: 36.99246 xmax: -95.30829 ymax: 45.00582\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado 140002.6 [m] MULTILINESTRING ((-105.155 ...\n#&gt; 2    Wyoming 140002.6 [m] MULTILINESTRING ((-106.3212...\n#&gt; 3   Nebraska 161243.2 [m] MULTILINESTRING ((-95.93779...\n\n\n\nGood. However, we were only interested in the distance to the closest border not to ALL boarders. Therefore we calculated 48 (49 - 1) more distances then needed!\nWhile this is not to complex for 1 &lt;-&gt; 49 features imagine we had 28,000+ (like your lab)!\nThat would result in 1,344,000 more calculations then needed ‚Ä¶"
  },
  {
    "objectID": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-5",
    "href": "slides/week-2.html#to-determine-distance-to-border-we-need-a-linear-representation-5",
    "title": "Week 2",
    "section": "To determine distance to border we need a linear representation:",
    "text": "To determine distance to border we need a linear representation:\n\n\n\nconus |&gt;\n  select(state_name) |&gt;\n  st_cast(\"MULTILINESTRING\") %&gt;%\n  mutate(dist = st_distance(., denver_sf)) |&gt;\n  slice_min(dist, n = 3) -&gt;\n  near3\n\n\n\n\n\n#&gt; Simple feature collection with 3 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -111.0569 ymin: 36.99243 xmax: -95.30829 ymax: 45.0059\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   state_name         dist                       geometry\n#&gt; 1   Colorado 139988.4 [m] MULTILINESTRING ((-109.06 3...\n#&gt; 2    Wyoming 139988.4 [m] MULTILINESTRING ((-111.0569...\n#&gt; 3   Nebraska 161243.2 [m] MULTILINESTRING ((-104.0531...\n\n\nGood. However, we were only interested in the distance to the closest border not to ALL boarders. Therefore we calculated 48 (49 - 1) more distances then needed!\nWhile this is not to complex for 1 &lt;-&gt; 49 features imagine we had 28,000+ (like) your lab!\nThat would result in 1,344,000 more calculations then needed ‚Ä¶"
  },
  {
    "objectID": "slides/week-2.html#revisting-the-idea-of-the-feature-level",
    "href": "slides/week-2.html#revisting-the-idea-of-the-feature-level",
    "title": "Week 2",
    "section": "Revisting the idea of the feature level:",
    "text": "Revisting the idea of the feature level:\n\nA ‚Äúfeature‚Äù can ‚Äúbe part of the whole‚Äù or the whole\n\nA island (POLYGON), or a set of islands acting as 1 unit (MULTIPOLYGON)\nA city (POINT), or a set of cities meeting a condition (MULTIPOINT)\nA road (LINESTRING), or a route (MULTILINESTRING)\n\nSince we want the distance to the nearest border, regardless of the state. Our feature is the set of borders with preserved boundaries.\nIn other words, a 1 feature MULTILINESTRING\n\n\n\nst_distance(denver_sf, st_cast(st_combine(conus), \"MULTILINESTRING\"))\n#&gt; Units: [m]\n#&gt;          [,1]\n#&gt; [1,] 140002.6\n\n\nThe same principle would apply if the question was ‚Äúdistance to national border‚Äù"
  },
  {
    "objectID": "slides/week-2.html#the-stickness-of-sfc-column",
    "href": "slides/week-2.html#the-stickness-of-sfc-column",
    "title": "Week 2",
    "section": "The stickness of sfc column",
    "text": "The stickness of sfc column\n\nGeometry columns are ‚Äústicky‚Äù meaning they persist through data manipulation:\n\n\n\nco |&gt; \n  select(name) |&gt; \n  slice(1:2)\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -106.0393 ymin: 37.3562 xmax: -103.7057 ymax: 40.00137\n#&gt; Geodetic CRS:  WGS 84\n#&gt;      name                       geometry\n#&gt; 1   Adams MULTIPOLYGON (((-105.0532 3...\n#&gt; 2 Alamosa MULTIPOLYGON (((-105.4855 3...\n\n\nDropping the geometry column requires dropping the geometry via sf:\n\n\n\n\nco |&gt; \n  st_drop_geometry() |&gt; #&lt;&lt;\n  select(name) |&gt; \n  slice(1:2)\n#&gt;      name\n#&gt; 1   Adams\n#&gt; 2 Alamosa\n\n\nOr cohersing the sf object to a data.frame:\n\n\n\n\nco |&gt; \n  as.data.frame() |&gt; #&lt;&lt;\n  select(name) |&gt; \n  slice(1:2)\n#&gt;      name\n#&gt; 1   Adams\n#&gt; 2 Alamosa"
  },
  {
    "objectID": "slides/week-2.html#the-stickness-of-sfc-column-1",
    "href": "slides/week-2.html#the-stickness-of-sfc-column-1",
    "title": "Week 2",
    "section": "The stickness of sfc column",
    "text": "The stickness of sfc column\n\nA simple features object (sf) is the connection of a sfc list-column and data.frame of attributes\n\n\n\nThis binding is unique compared to other column bindings built with things like\n\ndplyr::bind_cols()\ncbind()\ndo.call(cbind, list())"
  },
  {
    "objectID": "slides/week-2.html#coordinate-systems",
    "href": "slides/week-2.html#coordinate-systems",
    "title": "Week 2",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n\nWhat makes a feature geometry spatial is the reference system‚Ä¶"
  },
  {
    "objectID": "slides/week-2.html#coordinate-systems-1",
    "href": "slides/week-2.html#coordinate-systems-1",
    "title": "Week 2",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\n\nCoordinate Reference Systems (CRS) defines how spatial features relate to the surface of the Earth.\nCRSs are either geographic or projected‚Ä¶\nCRSs are measurement units for coordinates:"
  },
  {
    "objectID": "slides/week-2.html#sf-tools",
    "href": "slides/week-2.html#sf-tools",
    "title": "Week 2",
    "section": "sf tools",
    "text": "sf tools\nIn sf we have three tools for exploring, define, and changing CRS systems:\n\nst_crs : Retrieve coordinate reference system from sf or sfc object\nst_set_crs : Set or replace coordinate reference system from object\nst_transform : Transform or convert coordinates of simple feature\nAgain, ‚Äúst‚Äù (like PostGIS) denotes it is an operation that can work on a ‚Äù s patial t ype‚Äù"
  },
  {
    "objectID": "slides/week-2.html#geographic-coordinate-systms-gcs",
    "href": "slides/week-2.html#geographic-coordinate-systms-gcs",
    "title": "Week 2",
    "section": "Geographic Coordinate Systms (GCS)",
    "text": "Geographic Coordinate Systms (GCS)\n\nA GCS identifies locations on the curved surface of the earth.\nLocations are measured in angular units from the center of the earth relative to the plane defined by the equator and the plane defined by the prime meridian.\nThe vertical angle describes the latitude and the horizontal angle the longitude\nIn most coordinate systems, the North-South and East-West directions are encoded as +/-.\nNorth and East are positive (+) and South and West are negative (-) sign.\nA GCS is defined by 3 components:\n\nan ellipsoid\na geoid\na datum"
  },
  {
    "objectID": "slides/week-2.html#sphere-and-ellipsoid",
    "href": "slides/week-2.html#sphere-and-ellipsoid",
    "title": "Week 2",
    "section": "Sphere and Ellipsoid",
    "text": "Sphere and Ellipsoid\n\nAssuming that the earth is a perfect sphere simplifies calculations and works for small-scale maps (maps that show a large area of the earth).\nBut ‚Ä¶ the earth is not a sphere do to its rotation inducing a centripetal force along the equator.\nThis results in an equatorial axis that is roughly 21 km longer than the polar axis.\nTo account for this, the earth is modeled as an ellipsoid (slighty squished sphere) defined by two radii:\n\nthe semi-major axis (along the equatorial radius)\nthe semi-minor axis (along the polar radius)"
  },
  {
    "objectID": "slides/week-2.html#section-1",
    "href": "slides/week-2.html#section-1",
    "title": "Week 2",
    "section": "",
    "text": "(co_c = st_combine(co_geom))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTIPOLYGON (((-105.0532 39.79106, -104.976 39...\n\n\n\n\n\n\n\n\n\n\n\n(co_u = st_union(co_geom))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; POLYGON ((-105.155 36.99526, -105.1208 36.99543...\n\n\n\n\n\n\n\n\n\n\n\n\n(co_c_ml = st_combine(co_geom) |&gt; \n   st_cast(\"MULTILINESTRING\"))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTILINESTRING ((-105.0532 39.79106, -104.976 ...\n\n\n\n\n\n\n\n\n\n\n\n(co_u_ml = st_union(co_geom)  |&gt; \n    st_cast(\"MULTILINESTRING\"))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTILINESTRING ((-105.155 36.99526, -105.1208 ..."
  },
  {
    "objectID": "slides/week-2.html#geoid",
    "href": "slides/week-2.html#geoid",
    "title": "Week 2",
    "section": "Geoid",
    "text": "Geoid\n\nThe ellipsoid gives us the earths form as a perfectly smooth object\nBut ‚Ä¶ the earth is not perfectly smooth\nDeviations from the perfect sphere are measurable and can influence measurements.\nA geoid is a mathematical model fore representing these deviations\n\nWe are not talking about mountains and ocean trenches but the earth‚Äôs gravitational potential which is tied to the flow of the earth‚Äôs hot and fluid core.\nTherefore the geoid is constantly changing, albeit a large temporal scale.\n\nThe measurement and representation of the earth‚Äôs shape is at the heart of geodesy\n\n\nNASA‚Äôs geoid models"
  },
  {
    "objectID": "slides/week-2.html#datum",
    "href": "slides/week-2.html#datum",
    "title": "Week 2",
    "section": "Datum",
    "text": "Datum\n\nSo how are we to reconcile our need to work with a (simple) mathematical model of the earth‚Äôs shape with the undulating nature of the geoid?\nWe align the geoid with the ellipsoid to map the the earths departures from the smooth assumption\nThe alignment can be local where the ellipsoid surface is closely fit to the geoid at a particular location on the earth‚Äôs surface\n\nor\n\ngeocentric where the ellipsoid is aligned with the center of the earth.\nThe alignment of the smooth ellipsoid to the geoid model defines a datum."
  },
  {
    "objectID": "slides/week-2.html#local-datums",
    "href": "slides/week-2.html#local-datums",
    "title": "Week 2",
    "section": "Local Datums",
    "text": "Local Datums\n\nThere are many local datums to choose from\nThe choice of datum is largely driven by the location\nWhen working in the USA, a the North American Datum of 1927 (or NAD27 for short) is standard\n\nNAD27 is not well suited for other parts of the world."
  },
  {
    "objectID": "slides/week-2.html#section-2",
    "href": "slides/week-2.html#section-2",
    "title": "Week 2",
    "section": "",
    "text": "Thanks to satellite and computational capabilities our estimates of these radii are be quite precise\n\nThe semi-major axis is 6,378,137 m\nThe semi-minor axis is 6,356,752 m\n\nDifferences in distance along the surfaces of an ellipsoid vs.¬†a perfect sphere are small but measurable (the difference can be as high as 20 km)"
  },
  {
    "objectID": "slides/week-2.html#geocentric-datum",
    "href": "slides/week-2.html#geocentric-datum",
    "title": "Week 2",
    "section": "Geocentric Datum",
    "text": "Geocentric Datum\n\nMany modern datums use a geocentric alignment\n\nWorld Geodetic Survey for 1984 (WGS84)\nNorth American Datums of 1983 (NAD83)\n\nMost popular geocentric datums use the WGS84 ellipsoid or the GRS80 ellipsoid which share nearly identical semi-major and semi-minor axes\n\n\n\n\n\n\n\n\n\n\nGeocentric datum\nAcronym\nBest for\nComment\n\n\n\n\nNorth American Datum of 1983\nNAD83\nContinental US\nThis is one of the most popular modern datums for the contiguous US.\n\n\nEuropean Terrestrial Reference System 1989\nETRS89\nWestern Europe\nThis is the most popular modern datum for much of Europe.\n\n\nWorld Geodetic System 1984\nWGS84\nGlobal\nDeveloped by the Department of Defense.\n\n\n\n\n\n\n\n\n\nNote\n\n\nNAD 27 is based on Clarke Ellipsoid of 1866 which is calculated by manual surveying. NAD83 is based on the Geodetic Reference System (GRS) of 1980."
  },
  {
    "objectID": "slides/week-2.html#section-3",
    "href": "slides/week-2.html#section-3",
    "title": "Week 2",
    "section": "",
    "text": "Examples of common local datums are shown in the following table:\n\n\n\n\n\n\n\n\n\nLocal datum\nAcronym\nBest for\nComment\n\n\n\n\nNorth American Datum of 1927\nNAD27\nContinental US\nThis is an old datum but still prevalent\n\n\nEuropean Datum of 1950\nED50\nWestern Europe\nDeveloped after World War II and still quite popular\n\n\nWorld Geodetic System 1972\nWGS72\nGlobal\nDeveloped by the Department of Defense."
  },
  {
    "objectID": "slides/week-2.html#building-a-gcs",
    "href": "slides/week-2.html#building-a-gcs",
    "title": "Week 2",
    "section": "Building a GCS",
    "text": "Building a GCS\n\nSo, a GCS is defined by the ellipsoid model and its alignment to the geoid defining the datum.\nSmooth Sphere - Mathmatical Geoid (in angular units)"
  },
  {
    "objectID": "slides/week-2.html#projected-coordinate-systems",
    "href": "slides/week-2.html#projected-coordinate-systems",
    "title": "Week 2",
    "section": "Projected Coordinate Systems",
    "text": "Projected Coordinate Systems\n\nThe surface of the earth is curved but maps (and to data GIS) is flat.\nA projected coordinate system (PCS) is a reference system for identifying locations and measuring features on a flat (2D) surfaces. I\nProjected coordinate systems have an origin, an x axis, a y axis, and a linear unit of measure.\nGoing from a GCS to a PCS requires mathematical transformations.\nThere are three main groups of projection types:\n\nconic\ncylindrical\nplanar"
  },
  {
    "objectID": "slides/week-2.html#projection-types",
    "href": "slides/week-2.html#projection-types",
    "title": "Week 2",
    "section": "Projection Types:",
    "text": "Projection Types:\n\n\nIn all cases, distortion is minimized at the line/point of tangency (denoted by black line/point)\nDistortions are minimized along the tangency lines and increase with the distance from those lines."
  },
  {
    "objectID": "slides/week-2.html#plannar",
    "href": "slides/week-2.html#plannar",
    "title": "Week 2",
    "section": "Plannar",
    "text": "Plannar\n\nA planar projection projects data onto a flat surface touching the globe at a point or along 1 line of tangency.\nTypically used to map polar regions."
  },
  {
    "objectID": "slides/week-2.html#cylindrical",
    "href": "slides/week-2.html#cylindrical",
    "title": "Week 2",
    "section": "Cylindrical",
    "text": "Cylindrical\n\nA cylindrical projection maps the surface onto a cylinder.\nThis projection could also be created by touching the Earth‚Äôs surface along 1 or 2 lines of tangency\nMost often when mapping the entire world."
  },
  {
    "objectID": "slides/week-2.html#conic",
    "href": "slides/week-2.html#conic",
    "title": "Week 2",
    "section": "Conic",
    "text": "Conic\n\nIn a conic projection, the Earth‚Äôs surface is projected onto a cone along 1 or 2 lines of tangency\nTherefore, it is the best suited for maps of mid-latitude areas."
  },
  {
    "objectID": "slides/week-2.html#spatial-properties",
    "href": "slides/week-2.html#spatial-properties",
    "title": "Week 2",
    "section": "Spatial Properties",
    "text": "Spatial Properties\n\nAll projections distort real-world geographic features.\nThink about trying to unpeel an orange while preserving the skin\nThe four spatial properties that are subject to distortion are: shape, area, distance and direction\n\nA map that preserves shape is called conformal;\none that preserves area is called equal-area;\none that preserves distance is called equidistant\none that preserves direction is called azimuthal\nEach map projection can preserve only one or two of the four spatial properties.\nOften, projections are named after the spatial properties they preserve.\n\nWhen working with small-scale (large area) maps and when multiple spatial properties are needed, it is best to break the analyses across projections to minimize errors associated with spatial distortion."
  },
  {
    "objectID": "slides/week-2.html#setting-crsspcss",
    "href": "slides/week-2.html#setting-crsspcss",
    "title": "Week 2",
    "section": "Setting CRSs/PCSs",
    "text": "Setting CRSs/PCSs\n\nWe saw that sfc objects have two attributes to store a CRS: epsg and proj4string\n\n\n\nst_geometry(conus)\n#&gt; Geometry set for 49 features \n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 24.39631 xmax: -66.88544 ymax: 49.38448\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 5 geometries:\n#&gt; MULTIPOLYGON (((-85.4883 30.99706, -85.36962 30...\n#&gt; MULTIPOLYGON (((-110.7507 37.00301, -110.8054 3...\n#&gt; MULTIPOLYGON (((-90.95577 34.11871, -90.9569 34...\n#&gt; MULTIPOLYGON (((-116.1062 32.61848, -116.0123 3...\n#&gt; MULTIPOLYGON (((-105.155 36.99526, -105.1208 36...\n\n\nThis implies that all geometries in a geometry list-column (sfc) must have the same CRS.\nproj4string is a generic, string-based description of a CRS, understood by PROJ\nIt defines projection types and parameter values for particular projections,\nAs a result it can cover an infinite amount of different projections.\nepsg is the integer ID for a known CRS that can be resolved into a proj4string.\n\nThis is somewhat equivalent to the idea that a 6-digit FIP code can be resolved to a state/county pair\n\nSome proj4string values can resolved back into their corresponding epsg ID, but this does not always work.\nThe importance of having epsg values stored with data besides proj4string values is that the epsg refers to particular, well-known CRS, whose parameters may change (improve) over time\nfixing only the proj4string may remove the possibility to benefit from such improvements, and limit some of the provenance of datasets (but may help reproducibility)"
  },
  {
    "objectID": "slides/week-2.html#proj4-coordinate-syntax",
    "href": "slides/week-2.html#proj4-coordinate-syntax",
    "title": "Week 2",
    "section": "PROJ4 coordinate syntax",
    "text": "PROJ4 coordinate syntax\nThe PROJ4 syntax contains a list of parameters, each prefixed with the + character.\nA list of some PROJ4 parameters follows and the full list can be found here:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n+a\nSemi-major radius of the ellipsoid axis\n\n\n+b\nSemi-minor radius of the ellipsoid axis\n\n\n+datum\nDatum name\n\n\n+ellps\nEllipsoid name\n\n\n+lat_0\nLatitude of origin\n\n\n+lat_1\nLatitude of first standard parallel\n\n\n+lat_2\nLatitude of second standard parallel\n\n\n+lat_ts\nLatitude of true scale\n\n\n+lon_0\nCentral meridian\n\n\n+over\nAllow longitude output outside -180 to 180 range, disables wrapping\n\n\n+proj\nProjection name\n\n\n+south\nDenotes southern hemisphere UTM zone\n\n\n+units\nmeters, US survey feet, etc.\n\n\n+x_0\nFalse easting\n\n\n+y_0\nFalse northing\n\n\n+zone\nUTM zone"
  },
  {
    "objectID": "slides/week-2.html#section-4",
    "href": "slides/week-2.html#section-4",
    "title": "Week 2",
    "section": "",
    "text": "the geodesic distance looks weird given its curved appearance on the projected map.\nthis curvature is a byproduct of the current reference system‚Äôs increasing distance distortion as one moves towards the pole!\nWe can display the geodesic and planar distance on a 3D globe (or a projection that mimics the view of the 3D earth)."
  },
  {
    "objectID": "slides/week-2.html#transform-and-retrive",
    "href": "slides/week-2.html#transform-and-retrive",
    "title": "Week 2",
    "section": "Transform and retrive",
    "text": "Transform and retrive\n\n\n\nst_crs(conus)$epsg\n#&gt; [1] 4326\nst_crs(conus)$proj4string\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs\"\nst_crs(conus)$datum\n#&gt; [1] \"WGS84\"\n\n\n\nconus5070 &lt;- st_transform(conus, 5070)\n\nst_crs(conus5070)$epsg\n#&gt; [1] 5070\nst_crs(conus5070)$proj4string\n#&gt; [1] \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\"\nst_crs(conus5070)$datum\n#&gt; [1] \"NAD83\""
  },
  {
    "objectID": "slides/week-2.html#revisit-denver",
    "href": "slides/week-2.html#revisit-denver",
    "title": "Week 2",
    "section": "Revisit Denver",
    "text": "Revisit Denver\n\necho -104.9903 39.7392 | proj +proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\n#&gt; -723281.88   6827.29\n\n\nred = false origin : blue = Denver"
  },
  {
    "objectID": "slides/week-2.html#geodesic-geometries",
    "href": "slides/week-2.html#geodesic-geometries",
    "title": "Week 2",
    "section": "Geodesic geometries",
    "text": "Geodesic geometries\n\nPCSs introduce errors in their geometric measurements because the distance between two points on an ellipsoid is difficult to replicate on a projected coordinate system unless these points are close to one another.\nIn most cases, such errors other sources of error in the feature representation outweigh measurement errors made in a PCS making them tolorable.\nHowever, if the domain of analysis is large (i.e.¬†the North American continent), then the measurement errors associated with a projected coordinate system may no longer be acceptable.\nA way to circumvent projected coordinate system limitations is to adopt a geodesic solution."
  },
  {
    "objectID": "slides/week-2.html#geodesic-measurments",
    "href": "slides/week-2.html#geodesic-measurments",
    "title": "Week 2",
    "section": "Geodesic Measurments",
    "text": "Geodesic Measurments\n\nA geodesic distance is the shortest distance between two points on an ellipsoid\nA geodesic area measurement is one that is measured on an ellipsoid.\nSuch measurements are independent of the underlying projected coordinate system.\nWhy does this matter?\nCompare the distances measured between Santa Barbara and Amsterdam. The blue line represents the shortest distance between the two points on a planar coordinate system. The red line as measured on a ellipsoid."
  },
  {
    "objectID": "slides/week-2.html#section-5",
    "href": "slides/week-2.html#section-5",
    "title": "Week 2",
    "section": "",
    "text": "So if a geodesic measurement is more precise than a planar measurement, why not perform all spatial operations using geodesic geometry?\nThe downside is in its computational requirements.\nIt‚Äôs far more efficient to compute area/distance on a plane than it is on a spheroid.\nThis is because geodesic calculations have no simple algebraic solutions and involve approximations that may require iteration! (think optimization or nonlinear solutions)\nSo this may be a computationally taxing approach if processing 1,000(s) or 1,000,000(s) of line segments."
  },
  {
    "objectID": "slides/week-2.html#section-6",
    "href": "slides/week-2.html#section-6",
    "title": "Week 2",
    "section": "",
    "text": "We can set units if we do manipulations as well using the units package\n\nunits::set_units(l, \"km\")\n#&gt; 100093.1 [km]\nunits::set_units(l, \"mile\")\n#&gt; 62194.96 [mile]\n\nunits::set_units(a, \"ha\")\n#&gt; 809649680 [ha]\nunits::set_units(a, \"km2\")\n#&gt; 8096497 [km^2]\nunits::set_units(a, \"in2\")\n#&gt; 1.25496e+16 [in^2]"
  },
  {
    "objectID": "slides/week-2.html#gedesic-area-and-length-measurements",
    "href": "slides/week-2.html#gedesic-area-and-length-measurements",
    "title": "Week 2",
    "section": "Gedesic Area and Length Measurements",
    "text": "Gedesic Area and Length Measurements\n\nNot all algorthimns are equal (in terms of speed or accuracy)\nSome more efficient algorithms that minimize computation time may reduce precision in the process.\nSome of ArcMap‚Äôs functions offer the option to compute geodesic distances and areas however ArcMap does not clearly indicate how its geodesic calculations are implemented (cite)\nR is well documented, and is efficient!"
  },
  {
    "objectID": "slides/week-2.html#distances",
    "href": "slides/week-2.html#distances",
    "title": "Week 2",
    "section": "Distances",
    "text": "Distances\n?st_distance"
  },
  {
    "objectID": "slides/week-2.html#native-sf-binds-to-libwgeom",
    "href": "slides/week-2.html#native-sf-binds-to-libwgeom",
    "title": "Week 2",
    "section": "native sf binds to libwgeom",
    "text": "native sf binds to libwgeom"
  },
  {
    "objectID": "slides/week-2.html#distance-example",
    "href": "slides/week-2.html#distance-example",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB"
  },
  {
    "objectID": "slides/week-2.html#distance-example-1",
    "href": "slides/week-2.html#distance-example-1",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"),\n                crs = 4326))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)"
  },
  {
    "objectID": "slides/week-2.html#distance-example-2",
    "href": "slides/week-2.html#distance-example-2",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"),\n                crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)"
  },
  {
    "objectID": "slides/week-2.html#distance-example-3",
    "href": "slides/week-2.html#distance-example-3",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"),\n                crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0"
  },
  {
    "objectID": "slides/week-2.html#distance-example-4",
    "href": "slides/week-2.html#distance-example-4",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"),\n                crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000"
  },
  {
    "objectID": "slides/week-2.html#distance-example-5",
    "href": "slides/week-2.html#distance-example-5",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"),\n                crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 4017987\n#&gt; 2 4017987       0"
  },
  {
    "objectID": "slides/week-2.html#distance-example-6",
    "href": "slides/week-2.html#distance-example-6",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"),\n                crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n# Equal Distance\nst_distance(st_transform(pts, eqds))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 4017987\n#&gt; 2 4017987       0\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 3823549\n#&gt; 2 3823549       0"
  },
  {
    "objectID": "slides/week-2.html#distance-example-7",
    "href": "slides/week-2.html#distance-example-7",
    "title": "Week 2",
    "section": "Distance Example",
    "text": "Distance Example\n\n\n\n(pts = data.frame(y = c(40.7128, 34.4208),\n                  x = c(-74.0060, -119.6982 ),\n                  name = c(\"NYC\",\"SB\")))\n\n(pts = st_as_sf(pts, coords = c(\"x\", \"y\"),\n                crs = 4326))\n\neqds = '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n# Greeat Circle Distance\nst_distance(pts)\n\n# Euclidean Distance\nst_distance(pts, which = \"Euclidean\")\n\n# Equal Area PCS\nst_distance(st_transform(pts, 5070))\n\n# Equal Distance\nst_distance(st_transform(pts, eqds))\n\n\n\n#&gt;         y         x name\n#&gt; 1 40.7128  -74.0060  NYC\n#&gt; 2 34.4208 -119.6982   SB\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -119.6982 ymin: 34.4208 xmax: -74.006 ymax: 40.7128\n#&gt; Geodetic CRS:  WGS 84\n#&gt;   name                  geometry\n#&gt; 1  NYC   POINT (-74.006 40.7128)\n#&gt; 2   SB POINT (-119.6982 34.4208)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 4050406\n#&gt; [2,] 4050406       0\n#&gt; Units: [¬∞]\n#&gt;          1        2\n#&gt; 1  0.00000 46.12338\n#&gt; 2 46.12338  0.00000\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 4017987\n#&gt; 2 4017987       0\n#&gt; Units: [m]\n#&gt;         1       2\n#&gt; 1       0 3823549\n#&gt; 2 3823549       0"
  },
  {
    "objectID": "slides/week-2.html#area-example-conus",
    "href": "slides/week-2.html#area-example-conus",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")"
  },
  {
    "objectID": "slides/week-2.html#area-example-conus-1",
    "href": "slides/week-2.html#area-example-conus-1",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))"
  },
  {
    "objectID": "slides/week-2.html#area-example-conus-2",
    "href": "slides/week-2.html#area-example-conus-2",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df)"
  },
  {
    "objectID": "slides/week-2.html#area-example-conus-3",
    "href": "slides/week-2.html#area-example-conus-3",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) ))"
  },
  {
    "objectID": "slides/week-2.html#area-example-conus-4",
    "href": "slides/week-2.html#area-example-conus-4",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) )) +\n  theme_linedraw()"
  },
  {
    "objectID": "slides/week-2.html#area-example-conus-5",
    "href": "slides/week-2.html#area-example-conus-5",
    "title": "Week 2",
    "section": "Area Example: CONUS",
    "text": "Area Example: CONUS\n\n\n\nus_u_mp = st_cast(us_u_ml, \"MULTIPOLYGON\")\n\ndf = data.frame(name = c(\"WGS84\", \"AEA\", \"EPDS\"),\n           area = c(sum(st_area(conus)),\n            sum(st_area(st_transform(conus, 5070))),\n            sum(st_area(st_transform(conus, eqds)))))\n\nggplot(df) +\n  geom_col(aes(x = name, y = as.numeric(area) )) +\n  theme_linedraw() +\n  labs(x = \"SRS\", y = \"m2\")"
  },
  {
    "objectID": "slides/week-2.html#units-in-sf",
    "href": "slides/week-2.html#units-in-sf",
    "title": "Week 2",
    "section": "Units in sf",
    "text": "Units in sf\n\nThe CRS in sf encodes the units of measurement relating to spatial features\nWhere possible geometric operations such as st_distance(), st_length() and st_area() report results with a units attribute appropriate for the CRS:\nThis can be both handy and very confusing for those new to it. Consider the following:\n\n\n(l = sum(st_length(conus)))\n#&gt; 100093081 [m]\n(a = sum(st_area(conus)))\n#&gt; 8.096497e+12 [m^2]"
  },
  {
    "objectID": "slides/week-2.html#section-7",
    "href": "slides/week-2.html#section-7",
    "title": "Week 2",
    "section": "",
    "text": "We can set units if we do manipulations as well using the units package\n\nunits::set_units(l, \"km\")\n#&gt; 94980.15 [km]\nunits::set_units(l, \"mile\")\n#&gt; 59017.93 [mile]\n\nunits::set_units(a, \"ha\")\n#&gt; 783760974 [ha]\nunits::set_units(a, \"km2\")\n#&gt; 7837610 [km^2]\nunits::set_units(a, \"in2\")\n#&gt; 1.214832e+16 [in^2]"
  },
  {
    "objectID": "slides/week-2.html#units-are-a-class",
    "href": "slides/week-2.html#units-are-a-class",
    "title": "Week 2",
    "section": "Units are a class",
    "text": "Units are a class\n\nunits are an S3 data object with attribute information and ‚Äúrules of engagement‚Äù\n\n\nclass(st_length(conus)) \n#&gt; [1] \"units\"\nattributes(st_length(conus)) |&gt; unlist()\n#&gt; units.numerator           class \n#&gt;             \"m\"         \"units\"\n\nst_length(conus) + 100\n#&gt; Error in Ops.units(st_length(conus), 100): both operands of the expression should be \"units\" objects\n\nconus |&gt; \n  mutate(area = st_area(.)) |&gt; \n  ggplot(aes(x = name, y = area)) + \n  geom_col()\n#&gt; Error in `stopifnot()`:\n#&gt; ‚Ñπ In argument: `area = st_area(.)`.\n#&gt; Caused by error:\n#&gt; ! object '.' not found"
  },
  {
    "objectID": "slides/week-2.html#unit-values-can-be-stripped-of-their-attributes-if-need-be",
    "href": "slides/week-2.html#unit-values-can-be-stripped-of-their-attributes-if-need-be",
    "title": "Week 2",
    "section": "Unit values can be stripped of their attributes if need be:",
    "text": "Unit values can be stripped of their attributes if need be:\n\n# Via drop_units\n(units::drop_units(sum(st_length(conus))))\n#&gt; [1] 100093081\n\n# Via casting\n(as.numeric(sum(st_length(conus))))\n#&gt; [1] 100093081"
  },
  {
    "objectID": "slides/Untitled.html#these-tools-are-all-functions",
    "href": "slides/Untitled.html#these-tools-are-all-functions",
    "title": "Week 3",
    "section": "These tools are all functions",
    "text": "These tools are all functions\n\nR is a statistical computing language that provides features as functions\nEven with just its base installation, R provides hundreds of functions:\n\n\nlength(lsf.str(\"package:base\")) + length(lsf.str(\"package:stats\")) + length(lsf.str(\"package:utils\"))\n#&gt; [1] 1947\n\n\nsf provides over 100 more\n\n\nlength(lsf.str(\"package:sf\")) \n#&gt; [1] 149\n\n\nand the core tidyverse packages (that we use) an additional ~750\n\n\nlength(lsf.str(\"package:dplyr\")) +\nlength(lsf.str(\"package:ggplot2\")) +\nlength(lsf.str(\"package:tidyr\")) +\nlength(lsf.str(\"package:forcats\")) +\nlength(lsf.str(\"package:purrr\")) \n#&gt; [1] 1025"
  },
  {
    "objectID": "slides/Untitled.html#to-date",
    "href": "slides/Untitled.html#to-date",
    "title": "Week 3",
    "section": "To date ‚Ä¶",
    "text": "To date ‚Ä¶\n\nWe have been using functions written for us - mostly by sf and the tidyverse\nThis how any commercial GIS suite operates as well\nAnalysis and workflows are limited to the tools kits and options exposed to the user\nIn R, a lot more is actually exposed!\nEverytime we install a new package, we download code that provides new specific features (as functions)\nEverytime we attach a package to a working session (library()) we are making those functions available/visible"
  },
  {
    "objectID": "slides/Untitled.html#functions-are-objects",
    "href": "slides/Untitled.html#functions-are-objects",
    "title": "Week 3",
    "section": "Functions are objects",
    "text": "Functions are objects\n\nJust like x = 10 binds the value of 10 to the name x creating an object visible in the environment,\nfunctions are objects that can be called by name to execute a set of directions over defined arguments.\n\n\nclass(sf::st_intersects)\n#&gt; [1] \"function\"\nclass(sf::st_as_sf)\n#&gt; [1] \"function\""
  },
  {
    "objectID": "slides/Untitled.html#our-own-functions-are-visable-as-objects-in-the-environemnt",
    "href": "slides/Untitled.html#our-own-functions-are-visable-as-objects-in-the-environemnt",
    "title": "Week 3",
    "section": "Our own functions are visable as objects in the environemnt",
    "text": "Our own functions are visable as objects in the environemnt\n\nx = 10\ny = data.frame(x = 1:10, y = 10:1)\nf = function(x,y){ x  + y }"
  },
  {
    "objectID": "slides/Untitled.html#advancing-your-programming-skills",
    "href": "slides/Untitled.html#advancing-your-programming-skills",
    "title": "Week 3",
    "section": "Advancing your programming skills",
    "text": "Advancing your programming skills\n\nOne of the best ways to improve your skills as a data scientist is to write functions.\nFunctions allow you to automate common tasks in a more general way than copy-and-pasting.\nThe more times you apply a function, the more incentive you have to optimize it for speed/accuracy\nThe more creative/unique your analyses and questions can be"
  },
  {
    "objectID": "slides/Untitled.html#the-form-of-a-function",
    "href": "slides/Untitled.html#the-form-of-a-function",
    "title": "Week 3",
    "section": "The form of a function:",
    "text": "The form of a function:\nCreating a function follows the form:\n\nname = function(arg1, arg2, *){\n  code\n  ..\n  return(...)\n}\n\nWhere:\n\nname is the function name (e.g.¬†st_as_sf)\n\nThis is the name on which R is able to call the object\n\narg1 is the first input\narg2 is the second input\n* is any other argument you want to define\ncode ... defines the instructions to carry out on arg1 and arg2\nreturn(...) is what the function returns"
  },
  {
    "objectID": "slides/Untitled.html#defining-a-function",
    "href": "slides/Untitled.html#defining-a-function",
    "title": "Week 3",
    "section": "Defining a function",
    "text": "Defining a function\n\nTo define a function we need to identify the code we have, and what can/should generalized for future uses?\n\n\nstates = USAboundaries::us_states() %&gt;% \n  filter(!name %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n\n\nHere the input data (us_states) could change\nSo could the variable name we filter by (name)"
  },
  {
    "objectID": "slides/Untitled.html#function-signiture",
    "href": "slides/Untitled.html#function-signiture",
    "title": "Week 3",
    "section": "Function Signiture",
    "text": "Function Signiture\nSo, lets start with a function that takes general input data and a variable name\n\nget_conus = function(data, var){\n\n}"
  },
  {
    "objectID": "slides/Untitled.html#function-arguments",
    "href": "slides/Untitled.html#function-arguments",
    "title": "Week 3",
    "section": "Function arguments",
    "text": "Function arguments\nFunction arguments typically include two two broad sets: - the data to compute on, - arguments that control the details of the calculation\n\nIn st_transform x is the data, crs is the proj4string/EPSG code\nIn ms_simplify input is the data, keep defines the directions\nIn get_conus: data provides the data, var defines the column to filter"
  },
  {
    "objectID": "slides/Untitled.html#section",
    "href": "slides/Untitled.html#section",
    "title": "Week 3",
    "section": "",
    "text": "Generally, data arguments should come first.\nDetail arguments should go on the end\nIt can be useful - and good practice - to define default values.\n\nshould almost always be the most common value.\nThe exceptions to this rule are to do with safety of the process.\n\ne.g.¬†na.rm = FALSE"
  },
  {
    "objectID": "slides/Untitled.html#code-body",
    "href": "slides/Untitled.html#code-body",
    "title": "Week 3",
    "section": "Code body",
    "text": "Code body\nWe then have to carry these generalizations into the function directions using the arguments as our operators:\n\nget_conus = function(data, var){\n  conus = filter(data, !!var %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n  return(conus)\n}\n\n\nhere, we replace us_states() with data\nwe use get() to return the value of a named object\nWe assign our filtered object to the name conus\nAnd explicitly return the conus object from the function\nThe value returned by the function is usually the last evaluated statement, if we don‚Äôt specify return we can take advantage of this default:\n\n\nget_conus = function(data, var){\n  filter(data, !get(var) %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n}"
  },
  {
    "objectID": "slides/Untitled.html#cities",
    "href": "slides/Untitled.html#cities",
    "title": "Week 3",
    "section": "Cities",
    "text": "Cities\n\ncities = read_csv(\"../labs/data/uscities.csv\") %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;% \n  get_conus(\"state_name\")\n\nplot(cities$geometry, pch = 16, cex = .1)"
  },
  {
    "objectID": "slides/Untitled.html#its-ok-to-be-more-detailed",
    "href": "slides/Untitled.html#its-ok-to-be-more-detailed",
    "title": "Week 3",
    "section": "It‚Äôs ok to be more detailed",
    "text": "It‚Äôs ok to be more detailed\n\nAnother advantage of functions is that if our requirements change, we only need to make the change our code in one place.\nThis also means we can spend more time fine-tuning our code since we know it will be recycled.\nHere we can be more focused and make sure to remove other potential ‚Äúnon-conus‚Äù states from any input object:\n\n\nget_conus = function(data, var){\n  filter(data, !get(var) %in% \n           c(\"Hawaii\", \"Puerto Rico\", \"Alaska\",\n             \"Guam\", \"District of Columbia\"))\n}"
  },
  {
    "objectID": "slides/Untitled.html#using-our-function-1",
    "href": "slides/Untitled.html#using-our-function-1",
    "title": "Week 3",
    "section": "Using our function",
    "text": "Using our function\n\nconus = get_conus(us_states(), \"name\")\nnrow(conus)\n#&gt; [1] 48"
  },
  {
    "objectID": "slides/Untitled.html#point-in-polygon-case-study",
    "href": "slides/Untitled.html#point-in-polygon-case-study",
    "title": "Week 3",
    "section": "Point-in-Polygon Case Study",
    "text": "Point-in-Polygon Case Study\n\nThe power of GIS lies in analyzing multiple data sources together.\nOften the answer you want lies in many different layers and you need to do some analysis to extract and compile information.\nOne common analysis is Points-in-Polygon (PIP).\nPIP is useful when you want to know how many - or what kind of - points fall within the bounds of each polygon"
  },
  {
    "objectID": "slides/Untitled.html#data",
    "href": "slides/Untitled.html#data",
    "title": "Week 3",
    "section": "Data",
    "text": "Data\nCONUS counties\n\ncounties = st_transform(us_counties()[,-9], 5070) %&gt;% \n  select(name, geoid, state_name) %&gt;% \n  get_conus(\"state_name\")\n\nCONUS Starbucks\n\nstarbucks = read_csv('data/directory.csv') %&gt;% \n  filter(!is.na(Latitude), Country == \"US\") %&gt;% \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(counties) %&gt;% \n  select(store_name = `Store Name`)\n\nColorado Counties\n\nco = filter(counties, state_name == \"Colorado\")"
  },
  {
    "objectID": "slides/Untitled.html#step-1-spatial-join",
    "href": "slides/Untitled.html#step-1-spatial-join",
    "title": "Week 3",
    "section": "Step 1: Spatial Join",
    "text": "Step 1: Spatial Join\nTo count the Starbucks locations in CA counties, we start by joining the CA counties to the locations:\n\nHere we uses the counties as the x table and the locations as the y table\nThis is because we want to add the starbucks information to the county sf object.\nRemember the default of st_join is a left_join on the st_intersects predicate\n\n\n(starbucks1 = st_join(co, starbucks))\n#&gt; Simple feature collection with 511 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1146480 ymin: 1566911 xmax: -504612.5 ymax: 2073715\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt; First 10 features:\n#&gt;         name geoid state_name                        store_name\n#&gt; 1       Yuma 08125   Colorado                              &lt;NA&gt;\n#&gt; 2   San Juan 08111   Colorado                              &lt;NA&gt;\n#&gt; 3       Baca 08009   Colorado                              &lt;NA&gt;\n#&gt; 4    Prowers 08099   Colorado                              &lt;NA&gt;\n#&gt; 5     Custer 08027   Colorado                              &lt;NA&gt;\n#&gt; 6    Fremont 08043   Colorado     City Market - Canon City #417\n#&gt; 7       Mesa 08077   Colorado    I-70 Business Loop & 32 Rd - C\n#&gt; 7.1     Mesa 08077   Colorado    Hwy 6 & 25 Rd - Grand Junction\n#&gt; 7.2     Mesa 08077   Colorado   City Market-Grand Junction #444\n#&gt; 7.3     Mesa 08077   Colorado City Market - Grand Junction #432\n#&gt;                           geometry\n#&gt; 1   MULTIPOLYGON (((-575240.5 1...\n#&gt; 2   MULTIPOLYGON (((-1042591 16...\n#&gt; 3   MULTIPOLYGON (((-617878.8 1...\n#&gt; 4   MULTIPOLYGON (((-587200.4 1...\n#&gt; 5   MULTIPOLYGON (((-847581.3 1...\n#&gt; 6   MULTIPOLYGON (((-863875.9 1...\n#&gt; 7   MULTIPOLYGON (((-1114392 18...\n#&gt; 7.1 MULTIPOLYGON (((-1114392 18...\n#&gt; 7.2 MULTIPOLYGON (((-1114392 18...\n#&gt; 7.3 MULTIPOLYGON (((-1114392 18..."
  },
  {
    "objectID": "slides/Untitled.html#step-2-point-counts-by-polygon",
    "href": "slides/Untitled.html#step-2-point-counts-by-polygon",
    "title": "Week 3",
    "section": "Step 2: Point counts by Polygon",
    "text": "Step 2: Point counts by Polygon\ncount() is a dplyr function that ‚Äúlets you quickly count the unique values of one or more variables: df %&gt;% count(a, b) is roughly equivalent to df %&gt;% group_by(a, b) %&gt;% summarise(n = n())‚Äù\n\n(count(starbucks1, geoid))\n#&gt; Simple feature collection with 64 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1146480 ymin: 1566911 xmax: -504612.5 ymax: 2073715\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt; First 10 features:\n#&gt;    geoid  n                       geometry\n#&gt; 1  08001 35 MULTIPOLYGON (((-765835.6 1...\n#&gt; 2  08003  1 MULTIPOLYGON (((-878691.4 1...\n#&gt; 3  08005 58 MULTIPOLYGON (((-769015.2 1...\n#&gt; 4  08007  1 MULTIPOLYGON (((-1004101 16...\n#&gt; 5  08009  1 MULTIPOLYGON (((-617878.8 1...\n#&gt; 6  08011  1 MULTIPOLYGON (((-640691.8 1...\n#&gt; 7  08013 36 MULTIPOLYGON (((-819541.3 1...\n#&gt; 8  08014  9 MULTIPOLYGON (((-775389.1 1...\n#&gt; 9  08015  1 MULTIPOLYGON (((-905015.4 1...\n#&gt; 10 08017  1 MULTIPOLYGON (((-616732 176..."
  },
  {
    "objectID": "slides/Untitled.html#step-3-combine-the-processes",
    "href": "slides/Untitled.html#step-3-combine-the-processes",
    "title": "Week 3",
    "section": "Step 3: Combine the processes ‚Ä¶",
    "text": "Step 3: Combine the processes ‚Ä¶\n\nstarbucks1 = st_join(co, starbucks) %&gt;% \n   count(geoid)\n\nplot(starbucks1['n'])"
  },
  {
    "objectID": "slides/Untitled.html#now-for-colorado",
    "href": "slides/Untitled.html#now-for-colorado",
    "title": "Week 3",
    "section": "Now for Colorado?",
    "text": "Now for Colorado?\nWe can anticipate that PIP is a useful process we want to implement over variable points and polygons pairs\nSo, lets make a function named point_in_polygon, that takes a point dataset and a polygon dataset\n\npoint_in_polygon = function(points, polygon){\n st_join(polygon, points) %&gt;% \n   count(geoid)\n}"
  },
  {
    "objectID": "slides/Untitled.html#generalizing-the-count-variable",
    "href": "slides/Untitled.html#generalizing-the-count-variable",
    "title": "Week 3",
    "section": "Generalizing the count variable",
    "text": "Generalizing the count variable\n\nIn its current form, point_in_polygon only counts on geoid\nLets modify that by making the variable name an input\n\nagain, we use get() to return the value of a named object\nwe call this, variable id\n\n\n\npoint_in_polygon2 = function(points, polygon, var){\n  st_join(polygon, points) %&gt;% \n    count(get(var))\n}"
  },
  {
    "objectID": "slides/Untitled.html#optimizing-functions",
    "href": "slides/Untitled.html#optimizing-functions",
    "title": "Week 3",
    "section": "Optimizing functions",
    "text": "Optimizing functions\n\nLets apply our function over the counties and see how long it takes\nWe can check the time it takes by wrapping our function in system.time\n\n\nsystem.time({\n  us = point_in_polygon(starbucks, counties)\n})\n\n# user    system  elapsed \n# 3.719   0.354   4.309"
  },
  {
    "objectID": "slides/Untitled.html#to-keep-geometery-or-not",
    "href": "slides/Untitled.html#to-keep-geometery-or-not",
    "title": "Week 3",
    "section": "To keep geometery or not?",
    "text": "To keep geometery or not?\n\nRemember our geometries are sticky, that means they carry through all calculations - whether they are needed or not\nWe can ease alot of computational overhead by being mindfull of when we retain our geometry data with our attribute data.\n\n\n\n\nsystem.time({\n  st_join(counties, starbucks) %&gt;% \n    count(geoid) \n})\n\n#user    system  elapsed \n#3.970   0.421   5.521\n\n\n\nsystem.time({\n  st_join(counties, starbucks) %&gt;% \n    st_drop_geometry() %&gt;% \n    count(geoid) %&gt;% \n    left_join(counties, by = 'geoid') %&gt;% \n    st_as_sf() \n})\n\n# user    system  elapsed \n# 0.396   0.017   0.598"
  },
  {
    "objectID": "slides/Untitled.html#section-1",
    "href": "slides/Untitled.html#section-1",
    "title": "Week 3",
    "section": "",
    "text": "# How many seconds per point?\n# How many seconds per point?\n(point_per_sec = .598 / (nrow(counties) * nrow(starbucks)))\n#&gt; [1] 1.442037e-08\n\n# How will this scale to our dams data\n# (assuming the process is linear)\n\npoint_per_sec * (nrow(counties) * 91000)\n#&gt; [1] 4.077171\n\nAwesome!\nEffectivly a 86% decrease in time needed ((29-4) / 29)\n‚ÄúFunction-izing‚Äù our improvements\n\npoint_in_polygon3 = function(points, polygon, var){\n  st_join(polygon, points) %&gt;% \n    st_drop_geometry() %&gt;% \n    count(get(var)) %&gt;% \n    setNames(c(var, \"n\")) %&gt;% #&lt;&lt; \n    left_join(polygon, by = var) %&gt;% \n    st_as_sf() \n}"
  },
  {
    "objectID": "slides/Untitled.html#what-else-can-we-wrap",
    "href": "slides/Untitled.html#what-else-can-we-wrap",
    "title": "Week 3",
    "section": "What else can we wrap?",
    "text": "What else can we wrap?\n\nWhat about a really nice, clean, informative plot?\nggplots look great but can be time consuming to program‚Ä¶\nA function would allow us to take care of the groundwork\n\n\nplot_pip = function(data){\n  ggplot() + \n    geom_sf(data = data, aes(fill = log(n)), alpha = .9, size = .2) + \n    scale_fill_gradient(low = \"white\", high = \"darkgreen\") + \n    theme_void() + \n    theme(legend.position = 'none',\n          plot.title = element_text(face = \"bold\", color = \"darkgreen\", hjust = .5, size = 24)) +\n    labs(title = \"Starbucks Locations\",\n         caption = paste0(sum(data$n), \" locations represented\")) \n}\n\n\nThis is great because we can devote the time to making a nice plot and we will be able to recycle the work over other cases‚Ä¶"
  },
  {
    "objectID": "slides/Untitled.html#test-1",
    "href": "slides/Untitled.html#test-1",
    "title": "Week 3",
    "section": "Test",
    "text": "Test\n\n\n\npoint_in_polygon3(starbucks, filter(counties, state_name == \"California\"), \"geoid\") %&gt;% \nplot_pip()\n\n\n\n\n\n\n\n\n\npoint_in_polygon3(starbucks, filter(counties, state_name == \"New York\"), \"geoid\") %&gt;% \nplot_pip()\n\n\n\n\n\n\n\n\n\n\npoint_in_polygon3(starbucks, counties, \"geoid\") %&gt;% \n  plot_pip()\n\n\n\n\n\n\n\n\n\npoint_in_polygon3(starbucks, states, var = \"name\") %&gt;% \n  plot_pip()"
  },
  {
    "objectID": "slides/Untitled.html#section-2",
    "href": "slides/Untitled.html#section-2",
    "title": "Week 3",
    "section": "",
    "text": "sf::sf_use_s2(FALSE)\ncountries = st_as_sf(rnaturalearth::countries110)\n\nurl = '/Users/mikejohnson/Downloads/earthquakes-2025-03-29_21-55-22_-0600.tsv'\n\nquakes  = read_delim(url, delim = \"\\t\") %&gt;% \n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;% \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%     \n  st_transform(st_crs(countries))\n\nnrow(countries)\n#&gt; [1] 177\nnrow(quakes)\n#&gt; [1] 6443\nnrow(st_intersection(quakes, countries))\n#&gt; [1] 4317"
  },
  {
    "objectID": "slides/Untitled.html#moving-beyond-starbucks",
    "href": "slides/Untitled.html#moving-beyond-starbucks",
    "title": "Week 3",
    "section": "Moving beyond Starbucks?",
    "text": "Moving beyond Starbucks?\n\nHere is a nice tutorial of point-in-polygon in QGIS. It is looking at earthquakes by country.\nJust like us they use naturalearth data for the polygon areas\nAnd they are looking at earthquake data maintained by NOAA.\nIn R, we can read the NOAA data directly from a URL.\nThe data is a tab delimited txt file so we use readr::read_delim()"
  },
  {
    "objectID": "slides/Untitled.html#tools-for-reading-and-writing-data",
    "href": "slides/Untitled.html#tools-for-reading-and-writing-data",
    "title": "Week 3",
    "section": "tools for reading and writing data:",
    "text": "tools for reading and writing data:\n\nread_sf / write_sf\nread_csv / write_csv\nread_excel"
  },
  {
    "objectID": "slides/Untitled.html#tools-for-data-maniputation",
    "href": "slides/Untitled.html#tools-for-data-maniputation",
    "title": "Week 3",
    "section": "tools for data maniputation",
    "text": "tools for data maniputation\n\nfilter\nselect\nmutate\ngroup_by\nsummarize\nrollmean, lag"
  },
  {
    "objectID": "slides/Untitled.html#tools-to-managemanipulate-data-typestructure",
    "href": "slides/Untitled.html#tools-to-managemanipulate-data-typestructure",
    "title": "Week 3",
    "section": "tools to manage/manipulate data type/structure",
    "text": "tools to manage/manipulate data type/structure\n\nas.numeric\nas.factor\nst_as_sf\ndata.frame"
  },
  {
    "objectID": "slides/Untitled.html#tools-for-merging-and-shaping-data",
    "href": "slides/Untitled.html#tools-for-merging-and-shaping-data",
    "title": "Week 3",
    "section": "tools for merging and shaping data",
    "text": "tools for merging and shaping data\n\ninner_join\nleft_join\nright_join\nfull_join\npivot_longer\npivot_wider"
  },
  {
    "objectID": "slides/Untitled.html#tools-for-measuring-geometries",
    "href": "slides/Untitled.html#tools-for-measuring-geometries",
    "title": "Week 3",
    "section": "tools for measuring geometries:",
    "text": "tools for measuring geometries:\n\nst_distance\nst_area\nst_length\nset_units / drop_units"
  },
  {
    "objectID": "slides/Untitled.html#so-why-write-functions-opposed-to-scripts",
    "href": "slides/Untitled.html#so-why-write-functions-opposed-to-scripts",
    "title": "Week 3",
    "section": "So why write functions opposed to scripts?",
    "text": "So why write functions opposed to scripts?\n\nThe process can be named\nAs requirements change, you only need to update code in one place\nYou eliminate the chance of making incidental mistakes when you copy and paste (forgetting to change dist_to_state to dist_to_border).\nfunctions can be ‚Äòsourced‚Äô into Rmd files and R Scripts\nYou save youself time"
  },
  {
    "objectID": "slides/Untitled.html#rule-of-thumb",
    "href": "slides/Untitled.html#rule-of-thumb",
    "title": "Week 3",
    "section": "Rule of thumb  ",
    "text": "Rule of thumb  \n\nData is the first argument (better for pipes!)\nWhenever you have copy-and pasted code more than twice you should write a function\n\nFor example how many times have we coded:\n\nstates = USAboundaries::us_states() %&gt;% \n  filter(!name %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n\n\nOr made the same table with knitr/kableExtra only changing the col.names and data.frame\nOr calculated a distance, changed the units, and dropped them?\nAll of these task are repetitive and prone to making errors that could impact our analysis but not break our code‚Ä¶"
  },
  {
    "objectID": "slides/Untitled.html#using-our-function",
    "href": "slides/Untitled.html#using-our-function",
    "title": "Week 3",
    "section": "Using our function:",
    "text": "Using our function:\n\nLike any object, we have to run the lines of code to save it as an object before we can use it directly in our code:\nBut then ‚Ä¶\n\n\n\nStates\n\nx = get_conus(data = us_states(), var = \"name\")\nplot(x$geometry)\n\n\n\n\n\n\n\n\n\nCounties\n\nx2 = get_conus(data = us_counties()[,-9], var = \"state_name\")\nplot(x2$geometry)"
  },
  {
    "objectID": "slides/Untitled.html#test",
    "href": "slides/Untitled.html#test",
    "title": "Week 3",
    "section": "Test",
    "text": "Test\n\n\n\nco_sb = point_in_polygon(starbucks, co)\nplot(co_sb['n'])\n\n\n\n\n\n\n\n\n\nca_sb = point_in_polygon(starbucks, \n                         filter(counties, state_name == \"California\"))\nplot(co_sb['n'])\n\n\n\n\n\n\n\n\n\n\nor_sb = point_in_polygon(starbucks, \n                         filter(counties, \n                                state_name == \"Oregon\"))\nplot(or_sb['n'])\n\n\n\n\n\n\n\n\n\nny_sb = point_in_polygon(starbucks, \n                         filter(counties, \n                                state_name == \"New York\"))\nplot(ny_sb['n'])"
  },
  {
    "objectID": "slides/Untitled.html#applying-the-new-function",
    "href": "slides/Untitled.html#applying-the-new-function",
    "title": "Week 3",
    "section": "Applying the new function",
    "text": "Applying the new function\n\nHere, we can apply the PIP function over states and count by name\n\n\nstates = get_conus(us_states(), \"name\") %&gt;% \n  st_transform(5070)\n\nstate_sb = point_in_polygon2(starbucks, states, 'name')\n\nplot(state_sb['n'])"
  },
  {
    "objectID": "slides/Untitled.html#section-3",
    "href": "slides/Untitled.html#section-3",
    "title": "Week 3",
    "section": "",
    "text": "sf::sf_use_s2(FALSE)\ncountries = st_as_sf(rnaturalearth::countries110)\n\nurl = '/Users/mikejohnson/Downloads/earthquakes-2025-03-29_21-55-22_-0600.tsv'\n\nquakes  = read_delim(url, delim = \"\\t\") %&gt;% \n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;% \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%     \n  st_transform(st_crs(countries))\n\nnrow(countries)\n#&gt; [1] 177\nnrow(quakes)\n#&gt; [1] 6443\nnrow(st_intersection(quakes, countries))\n#&gt; [1] 4317"
  },
  {
    "objectID": "slides/Untitled.html#pip-plotting-method",
    "href": "slides/Untitled.html#pip-plotting-method",
    "title": "Week 3",
    "section": "PIP ‚Äì> Plotting Method",
    "text": "PIP ‚Äì&gt; Plotting Method\n\nWe can use our functions right out of the box for this data\nBut‚Ä¶ somethings are not quite right..\n\n\npoint_in_polygon3(quakes, countries, var = \"ADMIN\") %&gt;% \n  plot_pip()"
  },
  {
    "objectID": "slides/Untitled.html#modify-for-our-analysis",
    "href": "slides/Untitled.html#modify-for-our-analysis",
    "title": "Week 3",
    "section": "Modify for our analysis ‚Ä¶",
    "text": "Modify for our analysis ‚Ä¶\n\npoint_in_polygon3(quakes, countries, var = \"ADMIN\") %&gt;% \n  plot_pip() + #&lt;&lt;\n  labs(title = \"Earthquake Locations\") + \n  scale_fill_viridis_c() + \n  geom_sf(data = quakes, size = .25, alpha = .05, col = 'red')"
  },
  {
    "objectID": "slides/Untitled.html#improve-the-anaylsis",
    "href": "slides/Untitled.html#improve-the-anaylsis",
    "title": "Week 3",
    "section": "Improve the anaylsis‚Ä¶",
    "text": "Improve the anaylsis‚Ä¶\n\npoint_in_polygon3(quakes, countries, var = \"ADMIN\") %&gt;% \n  plot_pip() + \n  labs(title = \"Earthquake Locations\",\n       subtitle = \"Most impacted countries\") + \n  theme(plot.subtitle = element_text(hjust = .5),\n        plot.title = element_text(color = \"navy\")) + \n  scale_fill_viridis_c() + \n  geom_sf(data = quakes, size = .3, alpha = .05, col = 'red') +\n  gghighlight::gghighlight(n &gt; (mean(n) + sd(n)))"
  },
  {
    "objectID": "slides/Untitled.html#thats-not-bad",
    "href": "slides/Untitled.html#thats-not-bad",
    "title": "Week 3",
    "section": "Thats not bad‚Ä¶",
    "text": "Thats not bad‚Ä¶\n\n# How many seconds per point?\n(point_per_sec = 4.3 / (nrow(counties) * nrow(starbucks)))\n#&gt; [1] 1.036916e-07\n\n\n# How will this scale to our dams data\n# (assuming the process is linear)\n\npoint_per_sec * (nrow(counties) * 91000)\n#&gt; [1] 29.31745\n\n\n~ 30 seconds to test ~282,100,000 point/polygon relations is not bad, but could be a bottle neck in analysis\nLets look at a common means for improvement‚Ä¶"
  },
  {
    "objectID": "slides/functions.html#tools-for-reading-and-writing-data",
    "href": "slides/functions.html#tools-for-reading-and-writing-data",
    "title": "Week 3",
    "section": "tools for reading and writing data:",
    "text": "tools for reading and writing data:\n\nread_sf / write_sf\nread_csv / write_csv\nread_excel"
  },
  {
    "objectID": "slides/functions.html#tools-for-data-maniputation",
    "href": "slides/functions.html#tools-for-data-maniputation",
    "title": "Week 3",
    "section": "tools for data maniputation",
    "text": "tools for data maniputation\n\nfilter\nselect\nmutate\ngroup_by\nsummarize\nrollmean, lag"
  },
  {
    "objectID": "slides/functions.html#tools-to-managemanipulate-data-typestructure",
    "href": "slides/functions.html#tools-to-managemanipulate-data-typestructure",
    "title": "Week 3",
    "section": "tools to manage/manipulate data type/structure",
    "text": "tools to manage/manipulate data type/structure\n\nas.numeric\nas.factor\nst_as_sf\ndata.frame"
  },
  {
    "objectID": "slides/functions.html#tools-for-merging-and-shaping-data",
    "href": "slides/functions.html#tools-for-merging-and-shaping-data",
    "title": "Week 3",
    "section": "tools for merging and shaping data",
    "text": "tools for merging and shaping data\n\ninner_join\nleft_join\nright_join\nfull_join\npivot_longer\npivot_wider"
  },
  {
    "objectID": "slides/functions.html#tools-for-measuring-geometries",
    "href": "slides/functions.html#tools-for-measuring-geometries",
    "title": "Week 3",
    "section": "tools for measuring geometries:",
    "text": "tools for measuring geometries:\n\nst_distance\nst_area\nst_length\nset_units / drop_units"
  },
  {
    "objectID": "slides/functions.html#these-tools-are-all-functions",
    "href": "slides/functions.html#these-tools-are-all-functions",
    "title": "Week 3",
    "section": "These tools are all functions",
    "text": "These tools are all functions\n\nR is a statistical computing language that provides features as functions\nEven with just its base installation, R provides hundreds of functions:\n\n\nlength(lsf.str(\"package:base\")) + length(lsf.str(\"package:stats\")) + length(lsf.str(\"package:utils\"))\n#&gt; [1] 1947\n\n\nsf provides over 100 more\n\n\nlength(lsf.str(\"package:sf\")) \n#&gt; [1] 150\n\n\nand the core tidyverse packages (that we use) an additional ~750\n\n\nlength(lsf.str(\"package:dplyr\")) +\nlength(lsf.str(\"package:ggplot2\")) +\nlength(lsf.str(\"package:tidyr\")) +\nlength(lsf.str(\"package:forcats\")) +\nlength(lsf.str(\"package:purrr\")) \n#&gt; [1] 1025"
  },
  {
    "objectID": "slides/functions.html#to-date",
    "href": "slides/functions.html#to-date",
    "title": "Week 3",
    "section": "To date ‚Ä¶",
    "text": "To date ‚Ä¶\n\nWe have been using functions written for us - mostly by sf and the tidyverse\nThis how any commercial GIS suite operates as well\nAnalysis and workflows are limited to the tools kits and options exposed to the user\nIn R, a lot more is actually exposed!\nEvery time we install a new package, we download code that provides new specific features (as functions)\nEvery time we attach a package to a working session (library()) we are making those functions available/visible"
  },
  {
    "objectID": "slides/functions.html#functions-are-objects",
    "href": "slides/functions.html#functions-are-objects",
    "title": "Week 3",
    "section": "Functions are objects",
    "text": "Functions are objects\n\nJust like x = 10 binds the value of 10 to the name x creating an object visible in the environment,\nfunctions are objects that can be called by name to execute a set of directions over defined arguments.\n\n\nclass(sf::st_intersects)\n#&gt; [1] \"function\"\nclass(sf::st_as_sf)\n#&gt; [1] \"function\""
  },
  {
    "objectID": "slides/functions.html#our-own-functions-are-visable-as-objects-in-the-environemnt",
    "href": "slides/functions.html#our-own-functions-are-visable-as-objects-in-the-environemnt",
    "title": "Week 3",
    "section": "Our own functions are visable as objects in the environemnt",
    "text": "Our own functions are visable as objects in the environemnt\n\nx = 10\ny = data.frame(x = 1:10, y = 10:1)\nf = function(x,y){ x  + y }"
  },
  {
    "objectID": "slides/functions.html#advancing-your-programming-skills",
    "href": "slides/functions.html#advancing-your-programming-skills",
    "title": "Week 3",
    "section": "Advancing your programming skills",
    "text": "Advancing your programming skills\n\nOne of the best ways to improve your skills as a data scientist is to write functions.\nFunctions allow you to automate common tasks in a more general way than copy-and-pasting.\nThe more times you apply a function, the more incentive you have to optimize it for speed/accuracy\nThe more creative/unique your analyses and questions can be"
  },
  {
    "objectID": "slides/functions.html#so-why-write-functions-opposed-to-scripts",
    "href": "slides/functions.html#so-why-write-functions-opposed-to-scripts",
    "title": "Week 3",
    "section": "So why write functions opposed to scripts?",
    "text": "So why write functions opposed to scripts?\n\nThe process can be named\nAs requirements change, you only need to update code in one place\nYou eliminate the chance of making incidental mistakes when you copy and paste (forgetting to change dist_to_state to dist_to_border).\nfunctions can be ‚Äòsourced‚Äô into Qmd/Rmd files and R Scripts\nYou save yourself time"
  },
  {
    "objectID": "slides/functions.html#rule-of-thumb",
    "href": "slides/functions.html#rule-of-thumb",
    "title": "Week 3",
    "section": "Rule of thumb  ",
    "text": "Rule of thumb  \n\nData is the first argument (better for pipes!)\nWhenever you have copy-and pasted code more than twice you should write a function\n\nFor example how many times have we coded:\n\nstates = USAboundaries::us_states() |&gt; \n  filter(!name %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n\n\nOr made the same table with knitr/kableExtra only changing the col.names and data.frame\nOr calculated a distance, changed the units, and dropped them?\nAll of these task are repetitive and prone to making errors that could impact our analysis but not break our code‚Ä¶"
  },
  {
    "objectID": "slides/functions.html#the-form-of-a-function",
    "href": "slides/functions.html#the-form-of-a-function",
    "title": "Week 3",
    "section": "The form of a function:",
    "text": "The form of a function:\nCreating a function follows the form:\n\nname = function(arg1, arg2, *){\n  code\n  ..\n  return(...)\n}\n\nWhere:\n\nname is the function name (e.g.¬†st_as_sf)\n\nThis is the name on which R is able to call the object\n\narg1 is the first input\narg2 is the second input\n* is any other argument you want to define\ncode ... defines the instructions to carry out on arg1 and arg2\nreturn(...) is what the function returns"
  },
  {
    "objectID": "slides/functions.html#defining-a-function",
    "href": "slides/functions.html#defining-a-function",
    "title": "Week 3",
    "section": "Defining a function",
    "text": "Defining a function\n\nTo define a function we need to identify the code we have, and what can/should generalized for future uses?\n\n\nstates = USAboundaries::us_states() |&gt; \n  filter(!name %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n\n\nHere the input data (us_states) could change\nSo could the variable name we filter by (name)"
  },
  {
    "objectID": "slides/functions.html#function-signiture",
    "href": "slides/functions.html#function-signiture",
    "title": "Week 3",
    "section": "Function Signiture",
    "text": "Function Signiture\nSo, lets start with a function that takes general input data and a variable name\n\nget_conus = function(data, var){\n\n}"
  },
  {
    "objectID": "slides/functions.html#function-arguments",
    "href": "slides/functions.html#function-arguments",
    "title": "Week 3",
    "section": "Function arguments",
    "text": "Function arguments\nFunction arguments typically include two two broad sets: - the data to compute on, - arguments that control the details of the calculation\n\nIn st_transform x is the data, crs is the proj4string/EPSG code\nIn ms_simplify input is the data, keep defines the directions\nIn get_conus: data provides the data, var defines the column to filter"
  },
  {
    "objectID": "slides/functions.html#section",
    "href": "slides/functions.html#section",
    "title": "Week 3",
    "section": "",
    "text": "Generally, data arguments should come first.\nDetail arguments should go on the end\nIt can be useful - and good practice - to define default values.\n\nshould almost always be the most common value.\nThe exceptions to this rule are to do with safety of the process.\n\ne.g.¬†na.rm = FALSE"
  },
  {
    "objectID": "slides/functions.html#code-body",
    "href": "slides/functions.html#code-body",
    "title": "Week 3",
    "section": "Code body",
    "text": "Code body\nWe then have to carry these generalizations into the function directions using the arguments as our operators:\n\nget_conus = function(data, var){\n  conus = filter(data, !get(var) %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n  return(conus)\n}\n\n\nhere, we replace us_states() with data\nwe use get() to return the value of a named object\nWe assign our filtered object to the name conus\nAnd explicitly return the conus object from the function\nThe value returned by the function is usually the last evaluated statement, if we don‚Äôt specify return we can take advantage of this default:\n\n\nget_conus = function(data, var){\n  filter(data, !get(var) %in% c(\"Hawaii\", \"Puerto Rico\", \"Alaska\"))\n}"
  },
  {
    "objectID": "slides/functions.html#using-our-function",
    "href": "slides/functions.html#using-our-function",
    "title": "Week 3",
    "section": "Using our function:",
    "text": "Using our function:\n\nLike any object, we have to run the lines of code to save it as an object before we can use it directly in our code:\nBut then ‚Ä¶\n\n\n\nStates\n\nx = get_conus(data = us_states(), var = \"name\")\nplot(x$geometry)\n\n\n\n\n\n\n\n\n\nCounties\n\nx2 = get_conus(data = us_counties()[,-9], var = \"state_name\")\nplot(x2$geometry)"
  },
  {
    "objectID": "slides/functions.html#cities",
    "href": "slides/functions.html#cities",
    "title": "Week 3",
    "section": "Cities",
    "text": "Cities\n\ncities = read_csv(\"../labs/data/uscities.csv\") |&gt;\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  get_conus(\"state_name\")\n\nplot(cities$geometry, pch = 16, cex = .1)"
  },
  {
    "objectID": "slides/functions.html#its-ok-to-be-more-detailed",
    "href": "slides/functions.html#its-ok-to-be-more-detailed",
    "title": "Week 3",
    "section": "It‚Äôs ok to be more detailed",
    "text": "It‚Äôs ok to be more detailed\n\nAnother advantage of functions is that if our requirements change, we only need to make the change our code in one place.\nThis also means we can spend more time fine-tuning our code since we know it will be recycled.\nHere we can be more focused and make sure to remove other potential ‚Äúnon-conus‚Äù states from any input object:\n\n\nget_conus = function(data, var){\n  filter(data, !get(var) %in% \n           c(\"Hawaii\", \"Puerto Rico\", \"Alaska\",\n             \"Guam\", \"District of Columbia\"))\n}"
  },
  {
    "objectID": "slides/functions.html#using-our-function-1",
    "href": "slides/functions.html#using-our-function-1",
    "title": "Week 3",
    "section": "Using our function",
    "text": "Using our function\n\nconus = get_conus(us_states(), \"name\")\nnrow(conus)\n#&gt; [1] 48"
  },
  {
    "objectID": "slides/functions.html#point-in-polygon-case-study",
    "href": "slides/functions.html#point-in-polygon-case-study",
    "title": "Week 3",
    "section": "Point-in-Polygon Case Study",
    "text": "Point-in-Polygon Case Study\n\nThe power of GIS lies in analyzing multiple data sources together.\nOften the answer you want lies in many different layers and you need to do some analysis to extract and compile information.\nOne common analysis is Points-in-Polygon (PIP).\nPIP is useful when you want to know how many - or what kind of - points fall within the bounds of each polygon"
  },
  {
    "objectID": "slides/functions.html#data",
    "href": "slides/functions.html#data",
    "title": "Week 3",
    "section": "Data",
    "text": "Data\nCONUS counties\n\ncounties = st_transform(us_counties()[,-9], 5070) |&gt; \n  select(name, geoid, state_name) |&gt; \n  get_conus(\"state_name\")\n\nCONUS Starbucks\n\nstarbucks = read_csv('data/directory.csv') |&gt; \n  filter(!is.na(Latitude), Country == \"US\") |&gt; \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |&gt; \n  st_transform(5070) |&gt; \n  st_filter(counties) |&gt; \n  select(store_name = `Store Name`)\n\nColorado Counties\n\nco = filter(counties, state_name == \"Colorado\")"
  },
  {
    "objectID": "slides/functions.html#step-1-spatial-join",
    "href": "slides/functions.html#step-1-spatial-join",
    "title": "Week 3",
    "section": "Step 1: Spatial Join",
    "text": "Step 1: Spatial Join\nTo count the Starbucks locations in CA counties, we start by joining the CA counties to the locations:\n\nHere we uses the counties as the x table and the locations as the y table\nThis is because we want to add the starbucks information to the county sf object.\nRemember the default of st_join is a left_join on the st_intersects predicate\n\n\n(starbucks1 = st_join(co, starbucks))\n#&gt; Simple feature collection with 511 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1146480 ymin: 1566911 xmax: -504612.5 ymax: 2073715\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt; First 10 features:\n#&gt;         name geoid state_name                        store_name\n#&gt; 1       Yuma 08125   Colorado                              &lt;NA&gt;\n#&gt; 2   San Juan 08111   Colorado                              &lt;NA&gt;\n#&gt; 3       Baca 08009   Colorado                              &lt;NA&gt;\n#&gt; 4    Prowers 08099   Colorado                              &lt;NA&gt;\n#&gt; 5     Custer 08027   Colorado                              &lt;NA&gt;\n#&gt; 6    Fremont 08043   Colorado     City Market - Canon City #417\n#&gt; 7       Mesa 08077   Colorado    I-70 Business Loop & 32 Rd - C\n#&gt; 7.1     Mesa 08077   Colorado    Hwy 6 & 25 Rd - Grand Junction\n#&gt; 7.2     Mesa 08077   Colorado   City Market-Grand Junction #444\n#&gt; 7.3     Mesa 08077   Colorado City Market - Grand Junction #432\n#&gt;                           geometry\n#&gt; 1   MULTIPOLYGON (((-575240.5 1...\n#&gt; 2   MULTIPOLYGON (((-1042591 16...\n#&gt; 3   MULTIPOLYGON (((-617878.8 1...\n#&gt; 4   MULTIPOLYGON (((-587200.4 1...\n#&gt; 5   MULTIPOLYGON (((-847581.3 1...\n#&gt; 6   MULTIPOLYGON (((-863875.9 1...\n#&gt; 7   MULTIPOLYGON (((-1114392 18...\n#&gt; 7.1 MULTIPOLYGON (((-1114392 18...\n#&gt; 7.2 MULTIPOLYGON (((-1114392 18...\n#&gt; 7.3 MULTIPOLYGON (((-1114392 18..."
  },
  {
    "objectID": "slides/functions.html#step-2-point-counts-by-polygon",
    "href": "slides/functions.html#step-2-point-counts-by-polygon",
    "title": "Week 3",
    "section": "Step 2: Point counts by Polygon",
    "text": "Step 2: Point counts by Polygon\ncount() is a dplyr function that ‚Äúlets you quickly count the unique values of one or more variables: df |&gt; count(a, b) is roughly equivalent to df |&gt; group_by(a, b) |&gt; summarize(n = n())‚Äù\n\n(count(starbucks1, geoid))\n#&gt; Simple feature collection with 64 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1146480 ymin: 1566911 xmax: -504612.5 ymax: 2073715\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt; First 10 features:\n#&gt;    geoid  n                       geometry\n#&gt; 1  08001 35 MULTIPOLYGON (((-765835.6 1...\n#&gt; 2  08003  1 MULTIPOLYGON (((-878691.4 1...\n#&gt; 3  08005 58 MULTIPOLYGON (((-769015.2 1...\n#&gt; 4  08007  1 MULTIPOLYGON (((-1004101 16...\n#&gt; 5  08009  1 MULTIPOLYGON (((-617878.8 1...\n#&gt; 6  08011  1 MULTIPOLYGON (((-640691.8 1...\n#&gt; 7  08013 36 MULTIPOLYGON (((-819541.3 1...\n#&gt; 8  08014  9 MULTIPOLYGON (((-775389.1 1...\n#&gt; 9  08015  1 MULTIPOLYGON (((-905015.4 1...\n#&gt; 10 08017  1 MULTIPOLYGON (((-616732 176..."
  },
  {
    "objectID": "slides/functions.html#step-3-combine-the-processes",
    "href": "slides/functions.html#step-3-combine-the-processes",
    "title": "Week 3",
    "section": "Step 3: Combine the processes ‚Ä¶",
    "text": "Step 3: Combine the processes ‚Ä¶\n\nstarbucks1 = st_join(co, starbucks) |&gt; \n   count(geoid)\n\nplot(starbucks1['n'])"
  },
  {
    "objectID": "slides/functions.html#now-for-colorado",
    "href": "slides/functions.html#now-for-colorado",
    "title": "Week 3",
    "section": "Now for Colorado?",
    "text": "Now for Colorado?\nWe can anticipate that PIP is a useful process we want to implement over variable points and polygons pairs\nSo, lets make a function named point_in_polygon, that takes a point dataset and a polygon dataset\n\npoint_in_polygon = function(points, polygon){\n st_join(polygon, points) |&gt; \n   count(geoid)\n}"
  },
  {
    "objectID": "slides/functions.html#test",
    "href": "slides/functions.html#test",
    "title": "Week 3",
    "section": "Test",
    "text": "Test\n\n\n\nco_sb = point_in_polygon(starbucks, co)\nplot(co_sb['n'])\n\n\n\n\n\n\n\n\n\nca_sb = point_in_polygon(starbucks, \n                         filter(counties, state_name == \"California\"))\nplot(co_sb['n'])\n\n\n\n\n\n\n\n\n\n\nor_sb = point_in_polygon(starbucks, \n                         filter(counties, \n                                state_name == \"Oregon\"))\nplot(or_sb['n'])\n\n\n\n\n\n\n\n\n\nny_sb = point_in_polygon(starbucks, \n                         filter(counties, \n                                state_name == \"New York\"))\nplot(ny_sb['n'])"
  },
  {
    "objectID": "slides/functions.html#generalizing-the-count-variable",
    "href": "slides/functions.html#generalizing-the-count-variable",
    "title": "Week 3",
    "section": "Generalizing the count variable",
    "text": "Generalizing the count variable\n\nIn its current form, point_in_polygon only counts on geoid\nLets modify that by making the variable name an input\n\nagain, we use get() to return the value of a named object\nwe call this, variable id\n\n\n\npoint_in_polygon2 = function(points, polygon, var){\n  st_join(polygon, points) |&gt; \n    count(get(var))\n}"
  },
  {
    "objectID": "slides/functions.html#applying-the-new-function",
    "href": "slides/functions.html#applying-the-new-function",
    "title": "Week 3",
    "section": "Applying the new function",
    "text": "Applying the new function\n\nHere, we can apply the PIP function over states and count by name\n\n\nstates = get_conus(us_states(), \"name\") |&gt; \n  st_transform(5070)\n\nstate_sb = point_in_polygon2(starbucks, states, 'name')\n\nplot(state_sb['n'])"
  },
  {
    "objectID": "slides/functions.html#optimizing-functions",
    "href": "slides/functions.html#optimizing-functions",
    "title": "Week 3",
    "section": "Optimizing functions",
    "text": "Optimizing functions\n\nLets apply our function over the counties and see how long it takes\nWe can check the time it takes by wrapping our function in system.time\n\n\nsystem.time({\n  us = point_in_polygon(starbucks, counties)\n})\n\n# user    system  elapsed \n# 3.719   0.354   4.309"
  },
  {
    "objectID": "slides/functions.html#thats-not-bad",
    "href": "slides/functions.html#thats-not-bad",
    "title": "Week 3",
    "section": "Thats not bad‚Ä¶",
    "text": "Thats not bad‚Ä¶\n\n# How many seconds per point?\n(point_per_sec = 4.3 / (nrow(counties) * nrow(starbucks)))\n#&gt; [1] 1.036916e-07\n\n\n# How will this scale to our dams data\n# (assuming the process is linear)\n\npoint_per_sec * (nrow(counties) * 91000)\n#&gt; [1] 29.31745\n\n\n~ 30 seconds to test ~282,100,000 point/polygon relations is not bad, but could be a bottle neck in analysis\nLets look at a common means for improvement‚Ä¶"
  },
  {
    "objectID": "slides/functions.html#to-keep-geometery-or-not",
    "href": "slides/functions.html#to-keep-geometery-or-not",
    "title": "Week 3",
    "section": "To keep geometery or not?",
    "text": "To keep geometery or not?\n\nRemember our geometries are sticky, that means they carry through all calculations - whether they are needed or not\nWe can ease alot of computational overhead by being mindful of when we retain our geometry data with our attribute data.\n\n\n\n\nsystem.time({\n  st_join(counties, starbucks) |&gt; \n    count(geoid) \n})\n\n#user    system  elapsed \n#3.970   0.421   5.521\n\n\n\nsystem.time({\n  st_join(counties, starbucks) |&gt; \n    st_drop_geometry() |&gt; \n    count(geoid) |&gt; \n    left_join(counties, by = 'geoid') |&gt; \n    st_as_sf() \n})\n\n# user    system  elapsed \n# 0.396   0.017   0.598"
  },
  {
    "objectID": "slides/functions.html#section-1",
    "href": "slides/functions.html#section-1",
    "title": "Week 3",
    "section": "",
    "text": "# How many seconds per point?\n# How many seconds per point?\n(point_per_sec = .598 / (nrow(counties) * nrow(starbucks)))\n#&gt; [1] 1.442037e-08\n\n# How will this scale to our dams data\n# (assuming the process is linear)\n\npoint_per_sec * (nrow(counties) * 91000)\n#&gt; [1] 4.077171\n\nAwesome!\nEffectively a 86% decrease in time needed ((29-4) / 29)\n‚ÄúFunction-izing‚Äù our improvements\n\npoint_in_polygon3 = function(points, polygon, var){\n  st_join(polygon, points) |&gt; \n    st_drop_geometry() |&gt; \n    count(get(var)) |&gt; \n    setNames(c(var, \"n\")) |&gt; #&lt;&lt; \n    left_join(polygon, by = var) |&gt; \n    st_as_sf() \n}"
  },
  {
    "objectID": "slides/functions.html#what-else-can-we-wrap",
    "href": "slides/functions.html#what-else-can-we-wrap",
    "title": "Week 3",
    "section": "What else can we wrap?",
    "text": "What else can we wrap?\n\nWhat about a really nice, clean, informative plot?\nggplots look great but can be time consuming to program‚Ä¶\nA function would allow us to take care of the groundwork\n\n\nplot_pip = function(data){\n  ggplot() + \n    geom_sf(data = data, aes(fill = log(n)), alpha = .9, size = .2) + \n    scale_fill_gradient(low = \"white\", high = \"darkgreen\") + \n    theme_void() + \n    theme(legend.position = 'none',\n          plot.title = element_text(face = \"bold\", color = \"darkgreen\", hjust = .5, size = 24)) +\n    labs(title = \"Starbucks Locations\",\n         caption = paste0(sum(data$n), \" locations represented\")) \n}\n\n\nThis is great because we can devote the time to making a nice plot and we will be able to recycle the work over other cases‚Ä¶"
  },
  {
    "objectID": "slides/functions.html#test-1",
    "href": "slides/functions.html#test-1",
    "title": "Week 3",
    "section": "Test",
    "text": "Test\n\n\n\npoint_in_polygon3(starbucks, filter(counties, state_name == \"California\"), \"geoid\") |&gt; \nplot_pip()\n\n\n\n\n\n\n\n\n\npoint_in_polygon3(starbucks, filter(counties, state_name == \"New York\"), \"geoid\") |&gt; \nplot_pip()\n\n\n\n\n\n\n\n\n\n\npoint_in_polygon3(starbucks, counties, \"geoid\") |&gt; \n  plot_pip()\n\n\n\n\n\n\n\n\n\npoint_in_polygon3(starbucks, states, var = \"name\") |&gt; \n  plot_pip()"
  },
  {
    "objectID": "slides/functions.html#moving-beyond-starbucks",
    "href": "slides/functions.html#moving-beyond-starbucks",
    "title": "Week 3",
    "section": "Moving beyond Starbucks?",
    "text": "Moving beyond Starbucks?\n\nHere is a nice tutorial of point-in-polygon in QGIS. It is looking at earthquakes by country.\nJust like us they use naturalearth data for the polygon areas\nAnd they are looking at earthquake data maintained by NOAA.\nIn R, we can read the NOAA data directly from a URL.\nThe data is a tab delimited txt file so we use readr::read_delim()"
  },
  {
    "objectID": "slides/functions.html#section-2",
    "href": "slides/functions.html#section-2",
    "title": "Week 3",
    "section": "",
    "text": "sf::sf_use_s2(FALSE)\ncountries = st_as_sf(rnaturalearth::countries110)\n\nquakes = 'data/earthquakes-2025-03-29_21-55-22_-0600.tsv' |&gt; \n  read_delim(delim = \"\\t\") |&gt; \n  filter(!is.na(Latitude), !is.na(Longitude)) |&gt; \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |&gt;     \n  st_transform(st_crs(countries))\n\nnrow(countries)\n#&gt; [1] 177\nnrow(quakes)\n#&gt; [1] 6443\nnrow(st_intersection(quakes, countries))\n#&gt; [1] 4317"
  },
  {
    "objectID": "slides/functions.html#pip-plotting-method",
    "href": "slides/functions.html#pip-plotting-method",
    "title": "Week 3",
    "section": "PIP ‚Äì> Plotting Method",
    "text": "PIP ‚Äì&gt; Plotting Method\n\nWe can use our functions right out of the box for this data\nBut‚Ä¶ somethings are not quite right..\n\n\npoint_in_polygon3(quakes, countries, var = \"ADMIN\") |&gt; \n  plot_pip()"
  },
  {
    "objectID": "slides/functions.html#modify-for-our-analysis",
    "href": "slides/functions.html#modify-for-our-analysis",
    "title": "Week 3",
    "section": "Modify for our analysis ‚Ä¶",
    "text": "Modify for our analysis ‚Ä¶\n\npoint_in_polygon3(quakes, countries, var = \"ADMIN\") |&gt; \n  plot_pip() + #&lt;&lt;\n  labs(title = \"Earthquake Locations\") + \n  scale_fill_viridis_c() + \n  geom_sf(data = quakes, size = .25, alpha = .05, col = 'red')"
  },
  {
    "objectID": "slides/functions.html#improve-the-anaylsis",
    "href": "slides/functions.html#improve-the-anaylsis",
    "title": "Week 3",
    "section": "Improve the anaylsis‚Ä¶",
    "text": "Improve the anaylsis‚Ä¶\n\npoint_in_polygon3(quakes, countries, var = \"ADMIN\") |&gt; \n  plot_pip() + \n  labs(title = \"Earthquake Locations\",\n       subtitle = \"Most impacted countries\") + \n  theme(plot.subtitle = element_text(hjust = .5),\n        plot.title = element_text(color = \"navy\")) + \n  scale_fill_viridis_c() + \n  geom_sf(data = quakes, size = .3, alpha = .05, col = 'red') +\n  gghighlight::gghighlight(n &gt; (mean(n) + sd(n)))"
  },
  {
    "objectID": "slides/week-2.html#todays-data-colorado-counties",
    "href": "slides/week-2.html#todays-data-colorado-counties",
    "title": "Week 2",
    "section": "Todays Data: Colorado Counties!",
    "text": "Todays Data: Colorado Counties!\n\n#&gt; Simple feature collection with 64 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    geoid       name      aland state_nm                       geometry\n#&gt; 1  08001      Adams 3021840487 Colorado MULTIPOLYGON (((-105.0532 3...\n#&gt; 2  08003    Alamosa 1871643028 Colorado MULTIPOLYGON (((-105.4855 3...\n#&gt; 3  08005   Arapahoe 2066438714 Colorado MULTIPOLYGON (((-103.7065 3...\n#&gt; 4  08007  Archuleta 3496712164 Colorado MULTIPOLYGON (((-107.1287 3...\n#&gt; 5  08009       Baca 6617400567 Colorado MULTIPOLYGON (((-102.0416 3...\n#&gt; 6  08011       Bent 3918255148 Colorado MULTIPOLYGON (((-102.7476 3...\n#&gt; 7  08013    Boulder 1881325109 Colorado MULTIPOLYGON (((-105.3978 3...\n#&gt; 8  08014 Broomfield   85386685 Colorado MULTIPOLYGON (((-105.1092 3...\n#&gt; 9  08015    Chaffee 2624715692 Colorado MULTIPOLYGON (((-105.9698 3...\n#&gt; 10 08017   Cheyenne 4605713960 Colorado MULTIPOLYGON (((-103.1729 3..."
  },
  {
    "objectID": "slides/week-2.html#section-8",
    "href": "slides/week-2.html#section-8",
    "title": "Week 2",
    "section": "",
    "text": "We can set units if we do manipulations as well using the units package\n\nunits::set_units(l, \"km\")\n#&gt; 94980.15 [km]\nunits::set_units(l, \"mile\")\n#&gt; 59017.93 [mile]\n\nunits::set_units(a, \"ha\")\n#&gt; 783760974 [ha]\nunits::set_units(a, \"km2\")\n#&gt; 7837610 [km^2]\nunits::set_units(a, \"in2\")\n#&gt; 1.214832e+16 [in^2]"
  },
  {
    "objectID": "slides/week-2.html#conus",
    "href": "slides/week-2.html#conus",
    "title": "Week 2",
    "section": "CONUS",
    "text": "CONUS\n\n\n\nconus &lt;-  USAboundaries::us_states() |&gt;\n  filter(!state_name %in% c(\"Puerto Rico\", \n                            \"Alaska\", \n                            \"Hawaii\"))\n\nlength(st_geometry(conus))\n#&gt; [1] 49\n\n\n\nggplot() + \n  geom_sf(data = conus, aes(fill = state_name)) + \n  theme_linedraw() + \n  theme(legend.position = \"none\") + \n  labs(title = paste(\"CONUS:\", length(st_geometry(conus)), \"feature(s)\") ) + \n  scale_fill_viridis_d()"
  },
  {
    "objectID": "slides/week-2.html#what-makes-spatial-data-spatial",
    "href": "slides/week-2.html#what-makes-spatial-data-spatial",
    "title": "Week 2",
    "section": "What makes spatial data spatial?",
    "text": "What makes spatial data spatial?\n\nWhat makes a feature geometry spatial is the reference system‚Ä¶"
  },
  {
    "objectID": "slides/week-2.html#geometrycollection-1",
    "href": "slides/week-2.html#geometrycollection-1",
    "title": "Week 2",
    "section": "GEOMETRYCOLLECTION",
    "text": "GEOMETRYCOLLECTION\n\nIn case we end up with GEOMETRYCOLLECTION objects, the next question is often what to do with them. One thing we can do is extract elements from them:\n\n\n\nst_collection_extract(j, \"POLYGON\")\n#&gt; Geometry set for 3 features \n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 5.5 ymin: -0.5 xmax: 8 ymax: 1.5\n#&gt; CRS:           NA\n#&gt; MULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5...\n#&gt; MULTIPOLYGON (((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1)))\n#&gt; MULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5...\n\nst_collection_extract(j, \"POINT\")\n#&gt; POINT (1 0)\n\nst_collection_extract(j, \"LINESTRING\")\n#&gt; LINESTRING (4 0, 3 0)"
  },
  {
    "objectID": "slides/week-2.html#conversion-between-types",
    "href": "slides/week-2.html#conversion-between-types",
    "title": "Week 2",
    "section": "Conversion between types",
    "text": "Conversion between types\nWe can convert simple feature geometries using the st_cast generic (up to the extent that a conversion is feasible):\n\nmethods(st_cast)\n#&gt;  [1] st_cast.CIRCULARSTRING*     st_cast.COMPOUNDCURVE*     \n#&gt;  [3] st_cast.CURVE*              st_cast.GEOMETRYCOLLECTION*\n#&gt;  [5] st_cast.LINESTRING*         st_cast.MULTICURVE*        \n#&gt;  [7] st_cast.MULTILINESTRING*    st_cast.MULTIPOINT*        \n#&gt;  [9] st_cast.MULTIPOLYGON*       st_cast.MULTISURFACE*      \n#&gt; [11] st_cast.POINT*              st_cast.POLYGON*           \n#&gt; [13] st_cast.sf*                 st_cast.sfc*               \n#&gt; [15] st_cast.sfc_CIRCULARSTRING*\n#&gt; see '?methods' for accessing help and source code"
  },
  {
    "objectID": "slides/week-2.html#conversion-between-types-1",
    "href": "slides/week-2.html#conversion-between-types-1",
    "title": "Week 2",
    "section": "Conversion between types",
    "text": "Conversion between types\nLets take the Larimer County in our Colorado sf object:\n\n(co1 = filter(co, name == \"Larimer\")$geometry)\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -106.1954 ymin: 40.25778 xmax: -104.9431 ymax: 40.99844\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTIPOLYGON (((-105.6533 40.26046, -105.6094 4...\n(co_ls = st_cast(co1, \"MULTILINESTRING\"))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -106.1954 ymin: 40.25778 xmax: -104.9431 ymax: 40.99844\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTILINESTRING ((-105.6533 40.26046, -105.6094..."
  },
  {
    "objectID": "slides/week-2.html#smaller",
    "href": "slides/week-2.html#smaller",
    "title": "Week 2",
    "section": ".{smaller}",
    "text": ".{smaller}\n\n\n\n(co_c = st_combine(co_geom))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTIPOLYGON (((-105.0532 39.79106, -104.976 39...\n\n\n\n\n\n\n\n\n\n\n\n(co_u = st_union(co_geom))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; POLYGON ((-105.155 36.99526, -105.1208 36.99543...\n\n\n\n\n\n\n\n\n\n\n\n\n(co_c_ml = st_combine(co_geom) |&gt; \n   st_cast(\"MULTILINESTRING\"))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTILINESTRING ((-105.0532 39.79106, -104.976 ...\n\n\n\n\n\n\n\n\n\n\n\n(co_u_ml = st_union(co_geom)  |&gt; \n    st_cast(\"MULTILINESTRING\"))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\n#&gt; MULTILINESTRING ((-105.155 36.99526, -105.1208 ..."
  },
  {
    "objectID": "slides/week-2.html#conversion-between-types-2",
    "href": "slides/week-2.html#conversion-between-types-2",
    "title": "Week 2",
    "section": "Conversion between types",
    "text": "Conversion between types\nIt is often convenient to analyze the the points that make up a LINESTRING However ‚Ä¶\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt; \n  st_linestring() |&gt; \n  st_cast(\"POINT\")\n#&gt; Warning in st_cast.LINESTRING(st_linestring(rbind(c(0, 0), c(1, 1), c(1, :\n#&gt; point from first coordinate only\n#&gt; POINT (0 0)\n\n\ndoes not do what we expect, because it will convert a single geometry into a new single geometry (one line to one point)\n\n\nInstead, we must recognize that a collection of points is what defines a LINSETRING and a collection of POINTs, operating as a single unit, is a MULTIPOINT\n\nrbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt; \n  st_linestring() |&gt; \n  st_cast(\"MULTIPOINT\")\n#&gt; MULTIPOINT ((0 0), (1 1), (1 0), (0 1))\n\n\n\nIf we really wanted the individual POINT geometries, we need to work with sets:\n\n(p &lt;- rbind(c(0,0), c(1,1), c(1,0), c(0,1)) |&gt; \n   st_linestring() |&gt; \n   st_sfc() |&gt; #&lt;&lt;\n   st_cast(\"POINT\"))\n#&gt; Geometry set for 4 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n#&gt; CRS:           NA\n#&gt; POINT (0 0)\n#&gt; POINT (1 1)\n#&gt; POINT (1 0)\n#&gt; POINT (0 1)"
  },
  {
    "objectID": "labs/keys/lab3-key.html",
    "href": "labs/keys/lab3-key.html",
    "title": "Lab 3: Tesselations, Point-in-Polygon",
    "section": "",
    "text": "Background\nIn this lab we will an explore the impacts of tessellated surfaces and the modifiable areal unit problem (MAUP) using the National Dam Inventory maintained by the United States Army Corps of Engineers. Doing this will require repetitive tasks that we will write as functions and careful consideration of feature aggregation/simplification, spatial joins, and data visualization. The end goal is to visualize the distribution of dams and there purposes across the country.\nDISCLAIMER: This lab will be crunching a TON of data, in some cases 562,590,604 values for a single process! Therefore, I encourage you to run your code chuck-by-chunk rather then regularly knitting. Your final knit may take a couple of minutes to process. I know this is painful but be proud that, all said, your report will be analyzing billions of meaningful data and geometric relations.\n\nThis labs covers 4 main skills:\n\nTessellating Surfaces to discritized space\nGeometry Simplification: to expedite expensive intersections\nWriting functions to expedite repetitious reporting and mapping tasks\nPoint-in-polygon counts to aggregate point data\n\n\nLibraries\n\n\n\n\nQuestion 1:\nHere we will prepare five tessellated surfaces from CONUS and write a function to plot them in a descriptive way.\n\nStep 1.1\nFirst, we need a spatial file of CONUS counties. For future area calculations we want these in an equal area projection (EPSG:5070).\nTo achieve this:\n\nget an sf object of US counties (AOI::aoi_get(state = \"conus\", county = \"all\"))\ntransform the data to EPSG:5070\n\n\ncounties &lt;- AOI::aoi_get(state = \"conus\", county = \"all\") %&gt;% \n  st_transform(5070)\n\n\n\nStep 1.2\nFor triangle based tessellations we need point locations to serve as our ‚Äúanchors‚Äù.\nTo achieve this:\n\ngenerate county centroids using st_centroid\nSince, we can only tessellate over a feature we need to combine or union the resulting 3,108 POINT features into a single MULTIPOINT feature\nSince these are point objects, the difference between union/combine is mute\n\n\ncentroids = st_centroid(counties) %&gt;% \n  st_combine()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\n\n\nStep 1.3\nTessellations/Coverage‚Äôs describe the extent of a region with geometric shapes, called tiles, with no overlaps or gaps.\nTiles can range in size, shape, area and have different methods for being created.\nSome methods generate triangular tiles across a set of defined points (e.g.¬†voroni and delauny triangulation)\nOthers generate equal area tiles over a known extent (st_make_grid)\nFor this lab, we will create surfaces of CONUS using using 4 methods, 2 based on an extent and 2 based on point anchors:\nTessellations :\n\nst_voroni: creates voroni tessellation\nst_traingulate: triangulates set of points (not constrained)\n\nCoverage‚Äôs:\n\nst_make_grid: Creates a square grid covering the geometry of an sf or sfc object\nst_make_grid(square = FALSE): Create a hexagonal grid covering the geometry of an sf or sfc object\nThe side of coverage tiles can be defined by a cell resolution or a specified number of cell in the X and Y direction\n\n\nFor this step:\n\nMake a voroni tessellation over your county centroids (MULTIPOINT)\nMake a triangulated tessellation over your county centroids (MULTIPOINT)\nMake a gridded coverage with n = 70, over your counties object\nMake a hexagonal coverage with n = 70, over your counties object\n\nIn addition to creating these 4 coverage‚Äôs we need to add an ID to each tile.\nTo do this:\n\nadd a new column to each tessellation that spans from 1:n().\nRemember that ALL tessellation methods return an sfc GEOMETRYCOLLECTION, and to add attribute information - like our ID - you will have to coerce the sfc list into an sf object (st_sf or st_as_sf)\n\nLast, we want to ensure that our surfaces are topologically valid/simple.\n\nTo ensure this, we can pass our surfaces through st_cast.\nRemember that casting an object explicitly (e.g.¬†st_cast(x, \"POINT\")) changes a geometry\nIf no output type is specified (e.g.¬†st_cast(x)) then the cast attempts to simplify the geometry.\nIf you don‚Äôt do this you might get unexpected ‚ÄúTopologyException‚Äù errors.\n\n\ngrid = st_make_grid(counties, n = 70) %&gt;% \n  st_as_sf() %&gt;% \n  st_cast() %&gt;% \n  mutate(id = 1:n())\n\nhex = st_make_grid(counties, n = 70, square = FALSE) %&gt;% \n  st_as_sf() %&gt;% \n  st_cast() %&gt;% \n  mutate(id = 1:n())\n\n# Triangulation (of centroids)\ntri = st_triangulate(centroids) %&gt;% \n  st_as_sf() %&gt;% \n  st_cast() %&gt;% \n  mutate(id = 1:n())\n\n# Voroni tessellation (of centroids)\nvor = st_voronoi(centroids)  %&gt;%\n  st_as_sf() %&gt;% \n  st_cast() %&gt;% \n  mutate(id = 1:n())\n\n\n\nStep 1.4\nIf you plot the above tessellations you‚Äôll see the triangulated surfaces produce regions far beyond the boundaries of CONUS.\nWe need to cut these boundaries to CONUS border.\nTo do this, we will call on st_intersection, but will first need a geometry of CONUS to serve as our differencing feature. We can get this by unioning our existing county boundaries.\n\n# Generate simplified CONUS boundary\nusa_u = st_union(counties) \n\n\n\nStep 1.5\nWith a single feature boundary, we must carefully consider the complexity of the geometry. Remember, the more points our geometry contains, the more computations needed for spatial predicates our differencing. For a task like ours, we do not need a finely resolved coastal boarder.\nTo achieve this:\n\nSimplify your unioned border using the Visvalingam algorithm provided by rmapshaper::ms_simplify.\nChoose what percentage of vertices to retain using the keep argument and work to find the highest number that provides a shape you are comfortable with for the analysis:\n\n\nusa = rmapshaper::ms_simplify(usa_u, keep = .05)\n\n\nOnce you are happy with your simplification, use the mapview::npts function to report the number of points in your original object, and the number of points in your simplified object.\nHow many points were you able to remove? What are the consequences of doing this computationally?\n\n\nmapview::npts(usa_u)\n\n[1] 11292\n\nmapview::npts(usa)\n\n[1] 577\n\n\n\nFinally, use your simplified object to crop the two triangulated tessellations with st_intersection:\n\n\nvor_crop  = st_intersection(vor, usa) \n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\ntri_crop  = st_intersection(tri, usa) \n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nhex_crop  = st_filter(hex, usa) \ngrid_crop = st_filter(grid, usa) \n\n\n\nStep 1.6\nThe last step is to plot your tessellations. We don‚Äôt want to write out 5 ggplots (or mindlessly copy and paste üòÑ)\nInstead, lets make a function that takes an sf object as arg1 and a character string as arg2 and returns a ggplot object showing arg1 titled with arg2.\n\nThe form of a function is:\n\nname = function(arg1, arg2) {\n  \n  ... code goes here ...\n  \n  }\n\n\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nIn your function, the code should follow our standard ggplot practice where your data is arg1, and your title is arg2\nThe function should also enforce the following:\n\na white fill\na navy border\na size of 0.2\n`theme_void``\na caption that reports the number of features in arg1\n\nYou will need to paste character stings and variables together.\n\n\n\n\nplot_tess = function(data, title){\n  ggplot() + \n    geom_sf(data = data, fill = \"white\", col = \"navy\", size = .2) +   \n    theme_void() +\n    labs(title = title, caption = paste(\"This tesselation has:\", nrow(data), \"tiles\" )) +\n    theme(plot.title = element_text(hjust = .5, color =  \"navy\", face = \"bold\"))\n}\n\n\n\nStep 1.7\nUse your new function to plot each of your tessellated surfaces and the original county data (5 plots in total):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2:\nIn this question, we will write out a function to summarize our tessellated surfaces. Most of this should have been done in your daily assignments.\n\nStep 2.1\nFirst, we need a function that takes a sf object and a character string and returns a data.frame.\nFor this function:\n\nThe function name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string describing the object\nIn your function, calculate the area of arg1; convert the units to km2; and then drop the units\nNext, create a data.frame containing the following:\n\ntext from arg2\nthe number of features in arg1\nthe mean area of the features in arg1 (km2)\nthe standard deviation of the features in arg1\nthe total area (km2) of arg1\n\nReturn this data.frame\n\n\nsum_tess = function(data, type){\n  areakm2 = drop_units(set_units(st_area(data), \"km2\"))\n  data.frame(type = type, count = length(areakm2),  mean = round(mean(areakm2), 2),  sd = round(sd(areakm2),2), tot = round(sum(areakm2), 2))\n}\n\n\n\nStep 2.2\nUse your new function to summarize each of your tessellations and the original counties.\n\n\nStep 2.3\nMultiple data.frame objects can bound row-wise with bind_rows into a single data.frame\nFor example, if your function is called sum_tess, the following would bind your summaries of the triangulation and voroni object.\n\ntess_summary = bind_rows(\n  sum_tess(triangulation ,\"triangulation\"),\n  sum_tess(voroni, \"voroni\"))\n\n\n\nStep 2.4\nOnce your 5 summaries are bound (2 tessellations, 2 coverage‚Äôs, and the raw counties) print the data.frame as a nice table using knitr/kableExtra.\n\n\ntypecountmeansdtottriangulation6,1981,290.371,598.407,997,700voroni3,1082,604.432,917.828,094,557counties3,1082,605.053,443.718,096,496grid3,1302,761.290.008,642,833Hexagon2,3083,781.180.008,726,961\n\n\n\n\nStep 2.5\nComment on the traits of each tessellation. Be specific about how these traits might impact the results of a point-in-polygon analysis in the contexts of the modifiable areal unit problem and with respect computational requirements.\n\n\n\n\nQuestion 3:\nThe data we are going to analysis in this lab is from US Army Corp of Engineers National Dam Inventory (NID). This dataset documents ~91,000 dams in the United States and a variety of attribute information including design specifications, risk level, age, and purpose.\nFor the remainder of this lab we will analysis the distributions of these dams (Q3) and their purpose (Q4) through using a point-in-polygon analysis.\n\nStep 3.1\nIn the tradition of this class - and true to data science/GIS work - you need to find, download, and manage raw data. While the raw NID data is no longer easy to get with the transition of the USACE services to ESRI Features Services, I staged the data in the resources directory of this class. To get it, navigate to that location and download the raw file into you lab data directory.\n\nReturn to your RStudio Project and read the data in using the readr::read_csv\n\nAfter reading the data in, be sure to remove rows that don‚Äôt have location values (!is.na())\nConvert the data.frame to a sf object by defining the coordinates and CRS\nTransform the data to a CONUS AEA (EPSG:5070) projection - matching your tessellation\nFilter to include only those within your CONUS boundary\n\n\n\ndams = readr::read_csv('../data/NID2019_U.csv') \n\nRows: 91457 Columns: 69\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (44): DAM_NAME, OTHER_DAM_NAME, DAM_FORMER_NAME, NIDID, SECTION, COUNTY,...\ndbl (24): RECORDID, STATEID, LONGITUDE, LATITUDE, DISTANCE, YEAR_COMPLETED, ...\nlgl  (1): URL_ADDRESS\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndams2 = dams %&gt;% \n  filter(!is.na(LATITUDE) ) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4236) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(usa)\n\n\n\nStep 3.2\nFollowing the in-class examples develop an efficient point-in-polygon function that takes:\n\npoints as arg1,\npolygons as arg2,\nThe name of the id column as arg3\n\nThe function should make use of spatial and non-spatial joins, sf coercion and dplyr::count. The returned object should be input sf object with a column - n - counting the number of points in each tile.\n\npoint_in_polygon = function(points, polygon, id){\n  st_join(polygon, points) %&gt;% \n    st_drop_geometry() %&gt;% \n    count(.data[[id]]) %&gt;% \n    left_join(polygon) %&gt;% \n    st_as_sf() \n}\n\n\n\nStep 3.3\nApply your point-in-polygon function to each of your five tessellated surfaces where:\n\nYour points are the dams\nYour polygons are the respective tessellation\nThe id column is the name of the id columns you defined.\n\n\nvor_pts    &lt;-  point_in_polygon(points = dams2, polygon = vor_crop, id = \"id\")\n\nJoining with `by = join_by(id)`\n\ngrid_pts   &lt;-  point_in_polygon(dams2, grid_crop, \"id\")\n\nJoining with `by = join_by(id)`\n\ntri_pts    &lt;-  point_in_polygon(dams2, tri_crop, \"id\")\n\nJoining with `by = join_by(id)`\n\ncount_pts  &lt;-  point_in_polygon(dams2, counties, \"fip_code\")\n\nJoining with `by = join_by(fip_code)`\n\nhex_pts    &lt;-  point_in_polygon(dams2, hex_crop, \"id\")\n\nJoining with `by = join_by(id)`\n\n\n\n\nStep 3.4\nLets continue the trend of automating our repetitive tasks through function creation. This time make a new function that extends your previous plotting function.\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nThe function should also enforce the following:\n\nthe fill aesthetic is driven by the count column n\nthe col is NA\nthe fill is scaled to a continuous viridis color ramp\ntheme_void\na caption that reports the number of dams in arg1 (e.g.¬†sum(n))\n\nYou will need to paste character stings and variables together.\n\n\n\n\nmake_plot2 = function(data, title){\n ggplot() + \n    geom_sf(data = data, aes(fill = n), col = NA) +\n    scale_fill_viridis_c() + \n    theme_void() + \n    labs(fill = title,\n         caption = paste(\"There are\", sum(data$n), \" total dams counted\"))\n}\n\n\n\nStep 3.5\nApply your plotting function to each of the 5 tessellated surfaces with Point-in-Polygon counts:\n\n\nStep 3.6\nComment on the influence of the tessellated surface in the visualization of point counts. How does this related to the MAUP problem. Moving forward you will only use one tessellation, which will you chose and why?\nWhile there is not ‚Äúright‚Äù answer, justify your selection here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\nThe NID provides a comprehensive data dictionary here. In it we find that dam purposes are designated by a character code.\nThese are shown below for convenience (built using knitr on a data.frame called nid_classifier):\n\n\n\nNID 2019: Dam Purposes\n\n\nabbr\npurpose\n\n\n\n\nI\nIrrigation\n\n\nH\nHydroelectric\n\n\nC\nFlood Control\n\n\nN\nNavigation\n\n\nS\nWater Supply\n\n\nR\nRecreation\n\n\nP\nFire Protection\n\n\nF\nFish and Wildlife\n\n\nD\nDebris Control\n\n\nT\nTailings\n\n\nG\nGrade Stabilization\n\n\nO\nOther\n\n\n\n\n\n\n\n\nIn the data dictionary, we see a dam can have multiple purposes.\nIn these cases, the purpose codes are concatenated in order of decreasing importance. For example, SCR would indicate the primary purposes are Water Supply, then Flood Control, then Recreation.\nA standard summary indicates there are over 400 unique combinations of dam purposes:\n\n\nunique(dams2$PURPOSES) %&gt;% length\n\n[1] 493\n\n\n\nBy storing dam codes as a concatenated string, there is no easy way to identify how many dams serve any one purpose‚Ä¶ for example where are the hydro electric dams?\n\n\nTo overcome this data structure limitation, we can identify how many dams serve each purpose by splitting the PURPOSES values (strsplit) and tabulating the unlisted results as a data.frame. Effectively this is double/triple/quadruple counting dams bases on how many purposes they serve:\n\n\nJoining with `by = join_by(abbr)`\n\n\nThe result of this would indicate:\n\n\n\n\n\n\n\n\n\n\nStep 4.1\n\nYour task is to create point-in-polygon counts for at least 4 of the above dam purposes:\nYou will use grepl to filter the complete dataset to those with your chosen purpose\nRemember that grepl returns a boolean if a given pattern is matched in a string\ngrepl is vectorized so can be used in dplyr::filter\n\n\nFor example:\n\n# Find flood control dams in the first 5 records:\ndams2$PURPOSES[1:5]\n\n[1] \"FR\" \"R\"  \"C\"  \"FR\" \"R\" \n\ngrepl(\"F\", dams2$PURPOSES[1:5])\n\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\n\n\nFor your analysis, choose at least four of the above codes, and describe why you chose them. Then for each of them, create a subset of dams that serve that purpose using dplyr::filter and grepl\nFinally, use your point-in-polygon function to count each subset across your elected tessellation\n\n\nJoining with `by = join_by(id)`\nJoining with `by = join_by(id)`\nJoining with `by = join_by(id)`\nJoining with `by = join_by(id)`\nJoining with `by = join_by(id)`\nJoining with `by = join_by(id)`\nJoining with `by = join_by(id)`\n\n\n\n\nStep 4.2\n\nNow use your plotting function from Q3 to map these counts.\nBut! you will use gghighlight to only color those tiles where the count (n) is greater then the (mean + 1 standard deviation) of the set\nSince your plotting function returns a ggplot object already, the gghighlight call can be added ‚Äú+‚Äù directly to the function.\nThe result of this exploration is to highlight the areas of the country with the most\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 4.3\nComment of geographic distribution of dams you found. Does it make sense? How might the tessellation you chose impact your findings? How does the distribution of dams coincide with other geographic factors such as river systems, climate, ect?\n\n\n\nQuestion 5:\nYou have also been asked to identify the largest, at risk, flood control dams in the country\nYou must also map the Mississippi River System - This data is available here under the ‚ÄòData & Resources‚Äô tab - Download the shapefile and unzip it into your data directory. - Use read_sf to import this data and filter it to only include the Mississippi SYSTEM\nTo achieve this:\nCreate an interactive map using leaflet to show the largest (NID_STORAGE); high-hazard (HAZARD == ‚ÄúH‚Äù) dam in each state\n\nThe markers should be drawn as opaque, circle markers, filled red with no border, and a radius set equal to the (NID_Storage / 1,500,000)\nThe map tiles should be selected from any of the tile providers\nA popup table should be added using leafem::popup and should only include the dam name, storage, purposes, and year completed.\nThe Mississippi system should be added at a Polyline feature.\n\n\nlibrary(leaflet)\n\nmiss = read_sf('../../slides/data/majorrivers_0_0/MajorRivers.shp') %&gt;% \n  filter(SYSTEM== \"Mississippi\")\n\nbiggest_dams = dams2 %&gt;% \n  filter(HAZARD == \"H\", grepl(\"C\", PURPOSES)) %&gt;% \n  group_by(STATE) %&gt;% \n  slice_max(NID_STORAGE, n = 2 ) %&gt;% \n  st_transform(4326) %&gt;% \n  mutate(label = paste(DAM_NAME, \"\\n\", PURPOSES)) %&gt;% \n  select(DAM_NAME, NID_STORAGE, PURPOSES, YEAR_COMPLETED)\n\nleaflet() %&gt;% \n  addProviderTiles(providers$CartoDB.Positron) %&gt;% \n  addCircleMarkers(data = biggest_dams, \n                   fillOpacity = 1, \n                   fillColor = \"red\", \n                   color = NA, \n                   radius  = ~NID_STORAGE/15e5,\n                   popup = leafpop::popupTable(st_drop_geometry(biggest_dams),\n                    row.numbers = F, \n                    feature.id  = F)) %&gt;% \n  addPolylines(data = miss)\n\n\n\n\n\n\n\n\nRubric\n\nQuestion 1: Tessellations (30)\nQuestion 2: Tessellation Comparison (30)\nQuestion 3: PIP (30)\nQuestion 4: Conditional PIP (30)\nQuestion 5: Dam Age (20)\nWell Structured and appealing Qmd deployed as web page (10)\n\nTotal: 150\n\n\nSubmission\nYou will submit a URL to your web page deployed with GitHub pages.\nTo do this:\n\nRender your lab document\nStage/commit/push your files\nIf you followed the naming conventions in the ‚ÄúSet Up‚Äù of lab 1, your lab 3 link will be available at:\n\nhttps://USERNAME.github.io/csu-523c/lab-03.html\nSubmit this URL in the appropriate Canvas dropbox. Also take a moment to update your personal webpage with this link and some bullet points of what you learned. While not graded as part of this lab, it will be extra credit!"
  },
  {
    "objectID": "labs/keys/lab4.html#libraries",
    "href": "labs/keys/lab4.html#libraries",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(rstac) # STAC API\nlibrary(terra) # Raster Data handling\nlibrary(sf) # Vector data processing\nlibrary(mapview) # Rapid Interactive visualization\n\nAlmost all remote sensing / image analysis begins with the same basic steps:\n\nIdentify an area of interest (AOI)\nIdentify the temporal range of interest\nIdentify the relevant images\nDownload the images\nAnalyze the products"
  },
  {
    "objectID": "labs/keys/lab4.html#step-1-aoi-identification",
    "href": "labs/keys/lab4.html#step-1-aoi-identification",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 1: AOI identification",
    "text": "Step 1: AOI identification\nFirst we need to identify an AOI. We want to be able to extract the flood extents for Palo, Iowa and its surroundings. To do this we will use the geocoding capabilities within the AOI package.\n\npalo &lt;- AOI::geocode(\"Palo, Iowa\", bbox = TRUE)\n\nThis region defines the AOI for this analysis."
  },
  {
    "objectID": "labs/keys/lab4.html#step-2-temporal-identification",
    "href": "labs/keys/lab4.html#step-2-temporal-identification",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 2: Temporal identification",
    "text": "Step 2: Temporal identification\nThe flood event occurred on September 26, 2016. A primary challenge with remote sensing is the fact that all satellite imagery is not available at all times. In this case Landsat 8 has an 8 day revisit time. To ensure we capture an image within the date of the flood, lets set our time range to the period between September 24th - 29th of 2016. We will define this duration in the form YYYY-MM-DD/YYYY-MM-DD.\n\ntemporal_range &lt;- \"2016-09-24/2016-09-29\""
  },
  {
    "objectID": "labs/keys/lab4.html#step-3-identifying-the-relevant-images",
    "href": "labs/keys/lab4.html#step-3-identifying-the-relevant-images",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 3: Identifying the relevant images",
    "text": "Step 3: Identifying the relevant images\nThe next step is to identify the images that are available for our AOI and time range. This is where the rstac package comes in. The rstac package provides a simple interface to the SpatioTemporal Asset Catalog (STAC) API, which is a standard for discovering and accessing geospatial data.\nSTAC is a specification for describing geospatial data in a consistent way, making it easier to discover and access datasets. It provides a standardized way to describe the metadata of geospatial assets, including their spatial and temporal extents, data formats, and other relevant information.\n\nCatalog: A catalog is a collection of STAC items and collections. It serves as a top-level container for organizing and managing geospatial data. A catalog can contain multiple collections, each representing a specific dataset or group of related datasets.\nItems: The basic unit of data in STAC. Each item represents a single asset, such as a satellite image or a vector dataset. Items contain metadata that describes the asset, including its spatial and temporal extents, data format, and other relevant information.\nAsset: An asset is a specific file or data product associated with an item. For example, a single satellite image may have multiple assets, such as different bands or processing levels. Assets are typically stored in a cloud storage system and can be accessed via URLs.\n\nFor this project we are going to use a STAC catalog to identify the data available for our analysis. We want data from the Landsat 8 collection which is served by the USGS (via AWS), Google, and Microsoft Planetary Computer (MPC). MPC is the one that provides free access so we will use that data store.\nIf you go to this link you see the JSON representation of the full data holdings. If you CMD/CTL+F on that page for Landsat you‚Äôll find the references for the available data stores.\nWithin R, we can open a connection to this endpoint with the stac function:\n\n# Open a connection to the MPC STAC API\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\"))\n\n###rstac_query\n- url: https://planetarycomputer.microsoft.com/api/stac/v1/\n- params:\n- field(s): version, base_url, endpoint, params, verb, encode\n\n\nThat connection will provide an open entry to ALL data hosted by MPC. The stac_search function allows us to reduce the catalog to assets that match certain criteria (just like dplyr::filter reduces a data.frame). The get_request() function sends your search to the STAC API returning the metadata about the objects that match a criteria. The service implementation at MPC sets a return limit of 250 items (but it could be overridden with the limit parameter).\nHere, we are interested in the ‚ÄúLandsat Collection 2 Level-2‚Äù data. From the JSON file (seen in the browser). To start, lets search for that collection using the stac -&gt; stac_search ‚Äì&gt; get_request workflow:\n\n(stac_query &lt;-stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\") |&gt; \n  get_request())\n\n###Items\n- features (250 item(s)):\n  - LC09_L2SP_015248_20250414_02_T1\n  - LC09_L2SP_015247_20250414_02_T1\n  - LC09_L2SP_015054_20250414_02_T1\n  - LC09_L2SP_015053_20250414_02_T1\n  - LC09_L2SP_015052_20250414_02_T2\n  - LC09_L2SP_015051_20250414_02_T2\n  - LC09_L2SP_015050_20250414_02_T1\n  - LC09_L2SP_015049_20250414_02_T1\n  - LC09_L2SP_015048_20250414_02_T2\n  - LC09_L2SP_015047_20250414_02_T1\n  - ... with 240 more feature(s).\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nAwesome! So the first 250 items from the Level-2 Landsat collection were returned. Within each item, there are a number of assets (e.g.¬†the red, green, blue bands) and all items have some associated fields like the sub item assets, the bounding box, etc. We can now refine our search to limit the returned results to those that cover our AOI and time range of interest:\n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo)) |&gt; \n  get_request())\n\n###Items\n- features (2 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n  - LE07_L2SP_026031_20160925_02_T1\n- assets: \nang, atmos_opacity, atran, blue, cdist, cloud_qa, coastal, drad, emis, emsd, green, lwir, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nBy adding these constraints, we now see just two items. One from the Landsat 7 Level 2 dataset, and one from the Landsat 8 Level 2 dataset. For this lab, lets focus on the Landsat 8 item. We can use either the item or the id search criteria to elect this:\n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo),\n    limit = 1) |&gt; \n  get_request())\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n## OR ## \n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    id = 'LC08_L2SP_025031_20160926_02_T1',\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo)) |&gt; \n  get_request())\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nThe last thing we need to do, is sign this request. In rstac, items_sign(sign_planetary_computer()) signs STAC item asset URLs retrieved from Microsoft‚Äôs Planetary Computer, ensuring they include authentication tokens for access. sign_planetary_computer() generates the necessary signing function, and items_sign() applies it to STAC items. This is essential for accessing datasets hosted on the Planetary Computer, and other catalog were data access might be requester-paid or limited.\n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo),\n    limit = 1) |&gt; \n  get_request() |&gt; \n  items_sign(sign_planetary_computer()))\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type"
  },
  {
    "objectID": "labs/keys/lab4.html#step-4-downloading-needed-images",
    "href": "labs/keys/lab4.html#step-4-downloading-needed-images",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 4: Downloading needed images",
    "text": "Step 4: Downloading needed images\nOK! Now that we have identified the item we want, we are ready to download the data using assets_download(). In total, a Landsat 8 item has the following 11 bands:\n\nknitr::include_graphics(\"../images/lsat8-bands.jpg\")\n\n\n\n\n\n\n\n\nFor this lab, lets just get the first 6 bands. Assets are extracted from a STAC item by the asset name (look at the print statements of the stac_query). Let‚Äôs define a vector of the assets we want:\n\n# Bands 1-6\nbands &lt;- c('coastal', 'blue', 'green', 'red', 'nir08', 'swir16')\n\nNow we can use the assets_download() function to download the data. The output_dir argument specifies where to save the files, and the overwrite argument specifies whether to overwrite existing files with the same name.\n\nassets_download(items = stac_query,\n                asset_names = bands, \n                output_dir = '../data', \n                overwrite = TRUE)\n\nAnd that does it! You now have the process needed to get you data.\nWith a set of local files, you can create a raster object! Remember your files need to be in the order of the bands (double check step 2).\n\nlist.files() can search a directory for a pattern and return a list of files. The recursive argument will search all sub-directories. The full.names argument will return the full path to the files.\nThe rast() function will read the files into a raster object.\nThe setNames() function will set the names of the bands to the names we defined above."
  },
  {
    "objectID": "labs/keys/lab4.html#step-5-analyize-the-images",
    "href": "labs/keys/lab4.html#step-5-analyize-the-images",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 5: Analyize the images",
    "text": "Step 5: Analyize the images\nWe only want to analyze our image for the regions surrounding Palo (our AOI). Transform your AOI to the CRS of the landsat stack and use it to crop your raster stack.\nAwesome! We have now (1) identified, (2) downloaded, and (3) saved our images.\nWe have loaded them as a multiband SpatRast object and cropped the domain to our AOI. Lets make a few RGB plots to see what these images reveal.\n\n\n\n\n\n\nNote\n\n\n\nThere are many online examples that discuss the applications of the available bands in Landsat. What we want to do here is simply show how loading different combinations into the RGB channels make different features stand out. A useful reference of popular band combinations is here."
  },
  {
    "objectID": "labs/keys/lab5-key.html#libraries",
    "href": "labs/keys/lab5-key.html#libraries",
    "title": "Lab 5: Terrain Processing and OSM",
    "section": "Libraries",
    "text": "Libraries\nAttach your needed libraries:\n\nlibrary(sf)       # vector manipulation\nlibrary(terra)    # raster manipulation\n\nterra 1.8.29\n\nlibrary(whitebox) # terrain analysis\n\n# Data libraries\nlibrary(osmdata)  # OSM API\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright\n\nlibrary(elevatr)  # Elevation Web Tiles\n\nelevatr v0.99.0 NOTE: Version 0.99.0 of 'elevatr' uses 'sf' and 'terra'.  Use \nof the 'sp', 'raster', and underlying 'rgdal' packages by 'elevatr' is being \ndeprecated; however, get_elev_raster continues to return a RasterLayer.  This \nwill be dropped in future versions, so please plan accordingly."
  },
  {
    "objectID": "labs/keys/lab5-key.html#flooding-jargon",
    "href": "labs/keys/lab5-key.html#flooding-jargon",
    "title": "Lab 5: Terrain Processing and OSM",
    "section": "Flooding Jargon",
    "text": "Flooding Jargon\n\nstage = height of the water in a channel\nstreamflow = the rate of water flowing in a channel\nbasin = an area in which all cells contribute to a common outlet (an area or ridge of land that separates waters flowing to different rivers, basins, or seas)\nflowpath = the linear path over which water flows (river)\nHAND = Height Above Nearest Drainage, ‚Äúhow high is a cell above its nearest river cell?‚Äù"
  },
  {
    "objectID": "labs/keys/lab6-key.html",
    "href": "labs/keys/lab6-key.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "In this lab, we will explore predictive modeling in hydrology using the tidymodels framework and the CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) dataset.\n\n\n\n\n\n\n\n\n\n\n\ntidymodels is an R framework designed for machine learning and statistical modeling. Built on the principles of the tidyverse, tidymodels provides a consistent and modular approach to tasks like data preprocessing, model training, evaluation, and validation. By leveraging the strengths of packages such as recipes, parsnip, and yardstick, tidymodels streamlines the modeling workflow, making it easier to experiment with different models while maintaining reproducibility and interpretability.\n\n\n\nThe CAMELS dataset is a widely used resource in hydrology and environmental science, providing data on over 500 self-draining river basins across the United States. It includes meteorological forcings, streamflow observations, and catchment attributes such as land cover, topography, and soil properties. This dataset is particularly valuable for large-sample hydrology studies, enabling researchers to develop and test models across diverse climatic and physiographic conditions.\nIn this lab, we will focus on predicting mean streamflow for these basins using their associated characteristics. CAMELS has been instrumental in various hydrologic and machine learning applications, including:\n\nCalibrating Hydrologic Models ‚Äì Used for parameter tuning in models like SAC-SMA, VIC, and HBV, improving regional and large-sample studies.\nTraining Machine Learning Models ‚Äì Supports deep learning (e.g., LSTMs) and regression-based streamflow predictions, often outperforming traditional methods.\nUnderstanding Model Behavior ‚Äì Assists in assessing model generalization, uncertainty analysis, and the role of catchment attributes.\nBenchmarking & Regionalization ‚Äì Facilitates large-scale model comparisons and parameter transfer to ungauged basins.\nHybrid Modeling ‚Äì Enhances physics-based models with machine learning for bias correction and improved hydrologic simulations.\n\nA notable study by Kratzert et al.¬†(2019) demonstrated that LSTMs can outperform conceptual models in streamflow prediction. As part of this lab, we will explore how to programmatically download and load the data into R."
  },
  {
    "objectID": "labs/keys/lab6-key.html#what-is-tidymodels",
    "href": "labs/keys/lab6-key.html#what-is-tidymodels",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "tidymodels is an R framework designed for machine learning and statistical modeling. Built on the principles of the tidyverse, tidymodels provides a consistent and modular approach to tasks like data preprocessing, model training, evaluation, and validation. By leveraging the strengths of packages such as recipes, parsnip, and yardstick, tidymodels streamlines the modeling workflow, making it easier to experiment with different models while maintaining reproducibility and interpretability."
  },
  {
    "objectID": "labs/keys/lab6-key.html#what-is-the-camels-dataset",
    "href": "labs/keys/lab6-key.html#what-is-the-camels-dataset",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "The CAMELS dataset is a widely used resource in hydrology and environmental science, providing data on over 500 self-draining river basins across the United States. It includes meteorological forcings, streamflow observations, and catchment attributes such as land cover, topography, and soil properties. This dataset is particularly valuable for large-sample hydrology studies, enabling researchers to develop and test models across diverse climatic and physiographic conditions.\nIn this lab, we will focus on predicting mean streamflow for these basins using their associated characteristics. CAMELS has been instrumental in various hydrologic and machine learning applications, including:\n\nCalibrating Hydrologic Models ‚Äì Used for parameter tuning in models like SAC-SMA, VIC, and HBV, improving regional and large-sample studies.\nTraining Machine Learning Models ‚Äì Supports deep learning (e.g., LSTMs) and regression-based streamflow predictions, often outperforming traditional methods.\nUnderstanding Model Behavior ‚Äì Assists in assessing model generalization, uncertainty analysis, and the role of catchment attributes.\nBenchmarking & Regionalization ‚Äì Facilitates large-scale model comparisons and parameter transfer to ungauged basins.\nHybrid Modeling ‚Äì Enhances physics-based models with machine learning for bias correction and improved hydrologic simulations.\n\nA notable study by Kratzert et al.¬†(2019) demonstrated that LSTMs can outperform conceptual models in streamflow prediction. As part of this lab, we will explore how to programmatically download and load the data into R."
  },
  {
    "objectID": "labs/keys/lab6-key.html#lab-goals",
    "href": "labs/keys/lab6-key.html#lab-goals",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Lab Goals",
    "text": "Lab Goals\nIn this lab, you will:\n\nLearn how to programatically download and access data.\nPractice using tidymodels for predictive modeling.\nTrain and evaluate models to predict mean streamflow across the country.\nInterpret and compare model performance using workflows.\n\nBy the end of this lab, you will have hands-on experience applying machine learning techniques to real-world data, helping to bridge the gap between statistical modeling and environmental science."
  },
  {
    "objectID": "labs/keys/lab6-key.html#data-download",
    "href": "labs/keys/lab6-key.html#data-download",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Data Download",
    "text": "Data Download\nThe CAMELS dataset is hosted by NCAR and can be accessed here under the ‚ÄúIndividual Files‚Äù section. The root URL for all data seen on the ‚ÄúIndividual Files‚Äù page is:\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\nNear the bottom of that page, there are many .txt files that contain the data we want. Some hold climate data for each basin, some hold geology data, some hold soil data, etc. There is also a PDF with descriptions of the columns in each file. We are going to download all of the .txt files and the PDF."
  },
  {
    "objectID": "labs/keys/lab6-key.html#getting-the-documentation-pdf",
    "href": "labs/keys/lab6-key.html#getting-the-documentation-pdf",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Getting the documentation PDF",
    "text": "Getting the documentation PDF\nWe can download the documentation PDF which provides a descriptions for the various columns as many are not self-explanatory. Here we can use download.file to download the PDF to our data directory.\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              '../data/camels_attributes_v2.0.pdf')"
  },
  {
    "objectID": "labs/keys/lab6-key.html#getting-basin-characteristics",
    "href": "labs/keys/lab6-key.html#getting-basin-characteristics",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Getting Basin characteristics",
    "text": "Getting Basin characteristics\nNow we want to download the .txt files that store the actual data documented in the PDF. Doing this file by file (like we did with the PDF) is possible, but lets look at a better/easier way‚Ä¶\n\nLets create a vector storing the data types/file names we want to download:\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n\n\n\n\n\n\nglue\n\n\n\n\n\nThe glue package provides an efficient way to interpolate and manipulate strings. It is particularly useful for dynamically constructing text, formatting outputs, and embedding R expressions within strings.\n\nKey Features of glue:\n\nString Interpolation: Embed R expressions inside strings using {}.\nImproved Readability: Eliminates the need for cumbersome paste(), paste0() and sprintf() commands.\nMulti-line Strings: Easily handle multi-line text formatting.\nSafe and Efficient: Optimized for performance and readability.\n\n\n\nBasic Usage\n\nTo use glue, you need to load the package and then call the glue() function with the desired string template. You can embed R expressions within curly braces {} to interpolate values into the string.\n\n\nclass &lt;- \"ESS 330\"\nyear  &lt;- 2024\n\nglue(\"We are taking {class} together in {year}\")\n\nWe are taking ESS 330 together in 2024\n\n\n\n\nMultiples\n\nYou can also use glue to interpolate multiple values at once\n\n\nclasses &lt;- c(\"ESS 330\", \"ESS 523c\")\n\nglue(\"We are taking {classes} together in {year}\")\n\nWe are taking ESS 330 together in 2024\nWe are taking ESS 523c together in 2024\n\n\n\n\n\n\n\nUsing glue, we can construct the needed URLs and file names for the data we want to download:\n\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('../data/camels_{types}.txt')\n\n\nNow we can download the data: walk2 comes from the purrr package and is used to apply a function to multiple arguments in parallel (much like map2 works over paired lists). Here, we are asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.\n\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\nOnce downloaded, the data can be read it into R using readr::read_delim(), again instead of applying this to each file individually, we can use map to apply the function to each element of the local_files list.\n\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n\nThis gives us a list of data.frames, one for each file that we want to merge into a single table. So far in class we have focused on *_join functions to merge data based on a primary and foreign key relationship.\n\nIn this current list, we have &gt;2 tables, but, have a shared column called gauge_id that we can use to merge the data. However, since we have more then a left and right hand table, we need a more robust tool. We will use the powerjoin package to merge the data into a single data frame. powerjoin is a flexible package for joining lists of data.frames. It provides a wide range of join types, including inner, left, right, full, semi, anti, and cross joins making it a versatile tool for data manipulation and analysis, and one that should feel familiar to users of dplyr.\nIn this case, we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, we could have read straight form the urls. Strongly consider the implications of this approach as the longevity and persistence of the data is not guaranteed.\n\n# Read and merge data\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')"
  },
  {
    "objectID": "labs/keys/lab6-key.html#exploratory-data-analysis",
    "href": "labs/keys/lab6-key.html#exploratory-data-analysis",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirst, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\nWe can take a moment here to are learn a few new things about ggplot2!\n\n\n\n\n\n\nColor scales\n\n\n\nIn ggplot, sometimes you want different things (like bars, dots, or lines) to have different colors. But how does R know which colors to use? That‚Äôs where color scales come in!\nscales can be used to map data values to colors (scale_color_*) or fill aesthetics (scale_fill_*). There are two main types of color scales:\n\nDiscrete color scales ‚Äì for things that are categories, like ‚Äúapples,‚Äù ‚Äúbananas,‚Äù and ‚Äúcherries.‚Äù Each gets its own separate color.\n\n\nscale_color_manual(values = c(\"red\", \"yellow\", \"pink\")) #lets you pick your own colors.\n\nOr\n\nscale_color_brewer(palette = \"Set1\") #uses a built-in color set.\n\n\nContinuous color scales ‚Äì for numbers, like temperature (cold to hot) or height (short to tall). The color changes smoothly.\n\n\nscale_color_gradient(low = \"blue\", high = \"red\") #makes small numbers blue and big numbers red."
  },
  {
    "objectID": "labs/keys/lab6-key.html#visual-eda",
    "href": "labs/keys/lab6-key.html#visual-eda",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Visual EDA",
    "text": "Visual EDA\n\nLets start by looking that the 3 dimensions (variables) of this data. We‚Äôll start with a XY plot of aridity and rainfall. We are going to use the scale_color_viridis_c() function to color the points by the q_mean column. This scale functions maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color_* function is applied, it maps to the known color aesthetic in the plot.\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOk! so it looks like there is a relationship between rainfall, aridity, and rainfall but it looks like an exponential decay function and is certainly not linear.\nTo test a transformation, we can log transform the x and y axes using the scale_x_log10() and scale_y_log10() functions:\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nGreat! We can see a log-log relationship between aridity and rainfall provides a more linear relationship. This is a common relationship in hydrology and is often used to estimate rainfall in ungauged basins. However, once the data is transformed, the lack of spread in the streamflow data is quite evident with high mean flow values being compressed to the low end of aridity/high end of rainfall.\nTo address this, we can visualize how a log transform may benifit the q_mean data as well. Since the data is represented by color, rather then an axis, we can use the trans (transform) argument in the scale_color_viridis_c() function to log transform the color scale.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nExcellent! Treating these three right skewed variables as log transformed, we can see a more evenly spread relationship between aridity, rainfall, and mean flow. This is a good sign for building a model to predict mean flow using aridity and rainfall."
  },
  {
    "objectID": "labs/keys/lab6-key.html#lets-start-by-splitting-the-data",
    "href": "labs/keys/lab6-key.html#lets-start-by-splitting-the-data",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Lets start by splitting the data",
    "text": "Lets start by splitting the data\nFirst, we set a seed for reproducabilty, then transform the q_mean column to a log scale. Remember it is error prone to apply transformations to the outcome variable within a recipe. So, we‚Äôll do it a prioi.\nOnce set, we can split the data into a training and testing set. We are going to use 80% of the data for training and 20% for testing with no stratification.\nAdditionally, we are going to create a 10-fold cross validation dataset to help us evaluate multi-model setups.\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)"
  },
  {
    "objectID": "labs/keys/lab6-key.html#preprocessor-recipe",
    "href": "labs/keys/lab6-key.html#preprocessor-recipe",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Preprocessor: recipe",
    "text": "Preprocessor: recipe\nIn lecture, we have focused on using formulas as a workflow preprocessor. Separately we have used the recipe function to define a series of data preprocessing steps. Here, we are going to use the recipe function to define a series of data preprocessing steps.\nWe learned quite a lot about the data in the visual EDA. We know that the q_mean, aridity and p_mean columns are right skewed and can be helped by log transformations. We also know that the relationship between aridity and p_mean is non-linear and can be helped by adding an interaction term to the model. To implement these, lets build a recipe!\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())"
  },
  {
    "objectID": "labs/keys/lab6-key.html#naive-base-lm-approach",
    "href": "labs/keys/lab6-key.html#naive-base-lm-approach",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Naive base lm approach:",
    "text": "Naive base lm approach:\nOk, to start, lets do what we are comfortable with ‚Ä¶ fitting a linear model to the data. First, we use prep and bake on the training data to apply the recipe. Then, we fit a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/keys/lab6-key.html#where-things-get-a-little-messy",
    "href": "labs/keys/lab6-key.html#where-things-get-a-little-messy",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Where things get a little messy‚Ä¶",
    "text": "Where things get a little messy‚Ä¶\nOk so now we have our trained model lm_base and want to validate it on the test data.\n\nRemember a models ability to predict on new data is the most important part of the modeling process. It really doesnt matter how well it does on data it has already seen!\n\nWe have to be careful about how we do this with the base R approach:\n\nWrong version 1: augment\nThe broom package provides a convenient way to extract model predictions and residuals. We can use the augment function to add predicted values to the test data. However, if we use augment directly on the test data, we will get incorrect results because the preprocessing steps defined in the recipe object have not been applied to the test data.\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\nbroom::augment(lm_base, data = camels_test)\n\nError in `$&lt;-`:\n! Assigned data `predict(x, na.action = na.pass, ...) %&gt;% unname()` must\n  be compatible with existing data.\n‚úñ Existing data has 135 rows.\n‚úñ Assigned data has 535 rows.\n‚Ñπ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 535 to size 135.\n\n\n\n\nWrong version 2: predict\nThe predict function can be used to make predictions on new data. However, if we use predict directly on the test data, we will get incorrect results because the preprocessing steps defined in the recipe object have not been applied to the test data.\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCorrect version: prep -&gt; bake -&gt; predict\nTo correctly evaluate the model on the test data, we need to apply the same preprocessing steps to the test data that we applied to the training data. We can do this using the prep and bake functions with the recipe object. This ensures the test data is transformed in the same way as the training data before making predictions.\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)"
  },
  {
    "objectID": "labs/keys/lab6-key.html#model-evaluation-statistical-and-visual",
    "href": "labs/keys/lab6-key.html#model-evaluation-statistical-and-visual",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nNow that we have the predicted values, we can evaluate the model using the metrics function from the yardstick package. This function calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values.\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\nOk so that was a bit burdensome, is really error prone (fragile), and is worthless if we wanted to test a different algorithm‚Ä¶ lets look at a better approach!"
  },
  {
    "objectID": "labs/keys/lab6-key.html#using-a-workflow-instead",
    "href": "labs/keys/lab6-key.html#using-a-workflow-instead",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Using a workflow instead",
    "text": "Using a workflow instead\ntidymodels provides a framework for building and evaluating models using a consistent and modular workflow. The workflows package allows you to define a series of modeling steps, including data preprocessing, model fitting, and model fitting, in a single object. This makes it easier to experiment with different models, compare performance, and ensure reproducibility.\nworkflows are built from a model, a preprocessor, and a execution. Here, we are going to use the linear_reg function to define a linear regression model, set the engine to lm, and the mode to regression. We then add our recipe to the workflow, fit the model to the training data, and extract the model coefficients.\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\nLets ensure we replicated the results from the lm_base model. How do they look to you?\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01"
  },
  {
    "objectID": "labs/keys/lab6-key.html#making-predictions",
    "href": "labs/keys/lab6-key.html#making-predictions",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Making Predictions",
    "text": "Making Predictions\nNow that lm_wf is a workflow, data is not embedded in the model, we can use augment with the new_data argument to make predictions on the test data.\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62"
  },
  {
    "objectID": "labs/keys/lab6-key.html#model-evaluation-statistical-and-visual-1",
    "href": "labs/keys/lab6-key.html#model-evaluation-statistical-and-visual-1",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nAs with EDA, applying for graphical and statistical evaluation of the model is a key Here, we use the metrics function to extract the default metrics (rmse, rsq, mae) between the observed and predicted mean streamflow values.\nWe then create a scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()"
  },
  {
    "objectID": "labs/keys/lab6-key.html#switch-it-up",
    "href": "labs/keys/lab6-key.html#switch-it-up",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Switch it up!",
    "text": "Switch it up!\nThe real power of this approach is that we can easily switch out the models/recipes and see how it performs. Here, we are going to instead use a random forest model to predict mean streamflow. We define a random forest model using the rand_forest function, set the engine to ranger, and the mode to regression. We then add the recipe, fit the model, and evaluate the skill.\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)"
  },
  {
    "objectID": "labs/keys/lab6-key.html#predictions",
    "href": "labs/keys/lab6-key.html#predictions",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Predictions",
    "text": "Predictions\n\nMake predictions on the test data using the augment function and the new_data argument.\n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61"
  },
  {
    "objectID": "labs/keys/lab6-key.html#model-evaluation-statistical-and-visual-2",
    "href": "labs/keys/lab6-key.html#model-evaluation-statistical-and-visual-2",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nEvaluate the model using the metrics function and create a scatter plot of the observed vs predicted values, colored by aridity.\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nAwesome! We just set up a completely new model and were able to utilize all of the things we had done for the linear model. This is the power of the tidymodels framework!\nThat said, we still can reduce some to the repetition. Further, we are not really able to compare these models to one another as they"
  },
  {
    "objectID": "labs/keys/lab6-key.html#a-workflowset-approach",
    "href": "labs/keys/lab6-key.html#a-workflowset-approach",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "A workflowset approach",
    "text": "A workflowset approach\nworkflow_set is a powerful tool for comparing multiple models on the same data. It allows you to define a set of workflows, fit them to the same data, and evaluate their performance using a common metric. Here, we are going to create a workflow_set object with the linear regression and random forest models, fit them to the training data, and compare their performance using the autoplot and rank_results functions.\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 √ó 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore‚Ä¶ Prepro‚Ä¶ rmse    0.563  0.0247    10 recipe       rand‚Ä¶     1\n2 recipe_rand_fore‚Ä¶ Prepro‚Ä¶ rsq     0.771  0.0259    10 recipe       rand‚Ä¶     1\n3 recipe_linear_reg Prepro‚Ä¶ rmse    0.569  0.0260    10 recipe       line‚Ä¶     2\n4 recipe_linear_reg Prepro‚Ä¶ rsq     0.770  0.0223    10 recipe       line‚Ä¶     2\n\n\nOverall it seems the random forest model is outperforming the linear model. This is not surprising given the non-linear relationship between the predictors and the outcome :)"
  },
  {
    "objectID": "labs/keys/lab6-key.html#hyperparameter-tuning",
    "href": "labs/keys/lab6-key.html#hyperparameter-tuning",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\n\nrf_tune &lt;- rand_forest(mtry = tune(), trees = tune()) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nset.seed(123)\n(rf_res &lt;- tune_grid(rf_tune, rec, resamples = camels_cv, grid = 5))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 √ó 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [482/54]&gt; Fold01 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 2 &lt;split [482/54]&gt; Fold02 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 3 &lt;split [482/54]&gt; Fold03 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 4 &lt;split [482/54]&gt; Fold04 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 5 &lt;split [482/54]&gt; Fold05 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 6 &lt;split [482/54]&gt; Fold06 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 7 &lt;split [483/53]&gt; Fold07 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 8 &lt;split [483/53]&gt; Fold08 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n 9 &lt;split [483/53]&gt; Fold09 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n10 &lt;split [483/53]&gt; Fold10 &lt;tibble [10 √ó 6]&gt; &lt;tibble [0 √ó 3]&gt;\n\nshow_best(rf_res, metric = \"rsq\")\n\n# A tibble: 5 √ó 8\n   mtry trees .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     3  1000 rsq     standard   0.772    10  0.0237 Preprocessor1_Model5\n2     2  2000 rsq     standard   0.772    10  0.0259 Preprocessor1_Model4\n3     1   500 rsq     standard   0.771    10  0.0262 Preprocessor1_Model1\n4     1  1500 rsq     standard   0.770    10  0.0265 Preprocessor1_Model2\n5     2     1 rsq     standard   0.678    10  0.0374 Preprocessor1_Model3"
  },
  {
    "objectID": "labs/keys/lab6-key.html#final-model",
    "href": "labs/keys/lab6-key.html#final-model",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Final Model",
    "text": "Final Model\n\nrf_fin &lt;- rand_forest(mtry = 3, trees = 1000) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nfinal &lt;- workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(rf_fin) |&gt; \n  fit(data = camels_train)"
  },
  {
    "objectID": "labs/keys/lab6-key.html#evaluation",
    "href": "labs/keys/lab6-key.html#evaluation",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Evaluation",
    "text": "Evaluation\nAs a last step, lets evaluate the Random Forest model‚Äôs performance in predicting streamflow using the vip, augment, and ggplot2. We‚Äôll starts by computing variable importance (vip::vip()) to understand which predictors most influence the model.\nNext, we‚Äôll apply the trained model (final) to the test dataset using augment to append predictions to the test data.\nModel performance is then assessed using metrics(), comparing the actual (logQmean) and predicted (.pred) log-transformed mean streamflow values.\nFinally, a scatter plot is generated , visualizing the observed vs.¬†predicted values, color-coded by aridity. The plot includes a 1:1 reference line (geom_abline()) to indicate perfect predictions and uses the viridis color scale to improve readability.\n\n# VIP: \nvip::vip(final)\n\n\n\n\n\n\n\n## Predcition\nrf_data &lt;- augment(final, new_data = camels_test)\n\n## Evaluation\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.619\n2 rsq     standard       0.716\n3 mae     standard       0.385\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  geom_smooth(method = \"lm\", col = 'red', lty = 2, se = FALSE) +\n  theme_linedraw() + \n  labs(title = \"Random Forest Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "labs/keys/lab6-key.html#data-spliting-15",
    "href": "labs/keys/lab6-key.html#data-spliting-15",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Data Spliting (15)",
    "text": "Data Spliting (15)\n\nSet a seed for reproducible\nCreate an initial split with 75% used for training and 25% for testing\nExtract your training and testing sets\nBuild a 10-fold CV dataset as well"
  },
  {
    "objectID": "labs/keys/lab6-key.html#recipe-15",
    "href": "labs/keys/lab6-key.html#recipe-15",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Recipe (15)",
    "text": "Recipe (15)\n\nDefine a formula you want to use to predict logQmean\nDescribe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision.\nBuild a recipe that you feel handles the predictors chosen well"
  },
  {
    "objectID": "labs/keys/lab6-key.html#define-3-models-25",
    "href": "labs/keys/lab6-key.html#define-3-models-25",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Define 3 models (25)",
    "text": "Define 3 models (25)\n\nDefine a random forest model using the rand_forest function\nSet the engine to ranger and the mode to regression\nDefine two other models of your choice"
  },
  {
    "objectID": "labs/keys/lab6-key.html#workflow-set",
    "href": "labs/keys/lab6-key.html#workflow-set",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "workflow set ()",
    "text": "workflow set ()\nWith your preprocessing steps and models defined, you can now build a workflow_set object to fit and evaluate your models. This will allow you to compare the performance of different models on the same data.\n\nCreate a workflow object\nAdd the recipe\nAdd the model(s)\nFit the model to the resamples"
  },
  {
    "objectID": "labs/keys/lab6-key.html#evaluation-1",
    "href": "labs/keys/lab6-key.html#evaluation-1",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Evaluation",
    "text": "Evaluation\n\nUse autoplot and rank_results to compare the models.\nDescribe what model you think is best and why!\n\n\nTune the best model\n\nUse the tune_grid function to tune at least one of the model hyperparameters\nUse show_best to find the best hyperparameter values for the metric of your choic\nUse a workflow to fit your final, tuned, model\n\n\n\nLook at VIP\n\nUse the vip::vip package to visualize the variable importance of your final model\nDescribe what you think of the results and if they make sense\nIf the model you elect cant provide VIP, instead discuss the pros and cons of a less interpretable model"
  },
  {
    "objectID": "labs/keys/lab6-key.html#extact-and-evaluate",
    "href": "labs/keys/lab6-key.html#extact-and-evaluate",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Extact and Evaluate",
    "text": "Extact and Evaluate\n\nUse augment to make predictions on the test data\nUse metrics to evaluate the model performance on the test data\nCreate a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale\nDescribe what you think of the results!"
  },
  {
    "objectID": "slides/week-3.html#geometric-interiors-boundaries-and-exteriors",
    "href": "slides/week-3.html#geometric-interiors-boundaries-and-exteriors",
    "title": "Week 3",
    "section": "Geometric Interiors, Boundaries and Exteriors",
    "text": "Geometric Interiors, Boundaries and Exteriors\nAll geometries have interior, boundary and exterior regions.\nThe terms interior and boundary are used in the context of algebraic topology and manifold theory and not general topology\nThe OGC has define these states for the common geometry types in the simple features standard:\n\n\n\n\n\n\n\n\n\nDimension\nGeometry Type\nInterior (I)\nBoundary (B)\n\n\n\n\nPoint, MultiPoint\n0\nPoint, Points\nEmpty\n\n\nLineString, Line\n1\nPoints that are left when the boundary points are removed.\nTwo end points.\n\n\nPolygon\n2\nPoints within the rings.\nSet of rings."
  },
  {
    "objectID": "slides/week-3.html#interior-boundary-and-exterior-points",
    "href": "slides/week-3.html#interior-boundary-and-exterior-points",
    "title": "Week 3",
    "section": "Interior, Boundary and Exterior: POINTS",
    "text": "Interior, Boundary and Exterior: POINTS"
  },
  {
    "objectID": "slides/week-3.html#interior-boundary-and-exterior-linestring",
    "href": "slides/week-3.html#interior-boundary-and-exterior-linestring",
    "title": "Week 3",
    "section": "Interior, Boundary and Exterior: LINESTRING",
    "text": "Interior, Boundary and Exterior: LINESTRING"
  },
  {
    "objectID": "slides/week-3.html#predicates",
    "href": "slides/week-3.html#predicates",
    "title": "Week 3",
    "section": "Predicates",
    "text": "Predicates\nIn the following, we are interested in the resulting geometry that occurs when 2 geometries are overlaid‚Ä¶"
  },
  {
    "objectID": "slides/week-3.html#de-9im",
    "href": "slides/week-3.html#de-9im",
    "title": "Week 3",
    "section": "DE-9IM",
    "text": "DE-9IM\n\nDimensionally Extended 9-Intersection Model (DE-9IM)\nThe DE-9IM is a topological model and (standard) used to describe the spatial relations of two geometries\nUsed in geometry, point-set topology, geospatial topology\nThe DE-9IM matrix provides a way to classify geometry relations using the set {0,1,2,F} or {T,F}\nWith a {T,F} matrix domain, there are 512 possible relations that can be grouped into binary classification schemes.\nAbout 10 of these, have been given a common name such as intersects, touches, and within.\nWhen testing two geometries against a scheme, the result is a spatial predicate named by the scheme.\nThe model was developed by Clementini and others based on the seminal works of Egenhofer\nProvides the primary basis for queries and assertions in GIS and spatial databases (PostGIS)."
  },
  {
    "objectID": "slides/week-3.html#binary-logical-operations",
    "href": "slides/week-3.html#binary-logical-operations",
    "title": "Week 3",
    "section": "Binary logical operations",
    "text": "Binary logical operations\nReturns either a sparse matrix\n\nst_intersects(x,y)\n#&gt; Sparse geometry binary predicate list of length 3, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1, 3\n#&gt;  2: 2, 3\n#&gt;  3: 3\n\nor a dense matrix\n\nst_intersects(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2] [,3]  [,4]\n#&gt; [1,]  TRUE FALSE TRUE FALSE\n#&gt; [2,] FALSE  TRUE TRUE FALSE\n#&gt; [3,] FALSE FALSE TRUE FALSE\n\nst_disjoint(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3] [,4]\n#&gt; [1,] FALSE  TRUE FALSE TRUE\n#&gt; [2,]  TRUE FALSE FALSE TRUE\n#&gt; [3,]  TRUE  TRUE FALSE TRUE\n\nst_touches(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE\n\nst_within(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "slides/week-3.html#intersecting-counties",
    "href": "slides/week-3.html#intersecting-counties",
    "title": "Week 3",
    "section": "Intersecting Counties",
    "text": "Intersecting Counties\n\n\n\nggplot()"
  },
  {
    "objectID": "slides/week-3.html#intersecting-counties-1",
    "href": "slides/week-3.html#intersecting-counties-1",
    "title": "Week 3",
    "section": "Intersecting Counties",
    "text": "Intersecting Counties\n\n\n\nggplot() +\n  geom_sf(data = counties, lty = 3)"
  },
  {
    "objectID": "slides/week-3.html#intersecting-counties-2",
    "href": "slides/week-3.html#intersecting-counties-2",
    "title": "Week 3",
    "section": "Intersecting Counties",
    "text": "Intersecting Counties\n\n\n\nggplot() +\n  geom_sf(data = counties, lty = 3) +\n  geom_sf(data = states, fill = NA, size = 1)"
  },
  {
    "objectID": "slides/week-3.html#intersecting-counties-3",
    "href": "slides/week-3.html#intersecting-counties-3",
    "title": "Week 3",
    "section": "Intersecting Counties",
    "text": "Intersecting Counties\n\n\n\nggplot() +\n  geom_sf(data = counties, lty = 3) +\n  geom_sf(data = states, fill = NA, size = 1) +\n  geom_sf(data = misscount, fill = 'red', alpha = .5)"
  },
  {
    "objectID": "slides/week-3.html#intersecting-counties-4",
    "href": "slides/week-3.html#intersecting-counties-4",
    "title": "Week 3",
    "section": "Intersecting Counties",
    "text": "Intersecting Counties\n\n\n\nggplot() +\n  geom_sf(data = counties, lty = 3) +\n  geom_sf(data = states, fill = NA, size = 1) +\n  geom_sf(data = misscount, fill = 'red', alpha = .5) +\n  geom_sf(data = miss, col = \"blue\")"
  },
  {
    "objectID": "slides/week-3.html#intersecting-counties-5",
    "href": "slides/week-3.html#intersecting-counties-5",
    "title": "Week 3",
    "section": "Intersecting Counties",
    "text": "Intersecting Counties\n\n\n\nggplot() +\n  geom_sf(data = counties, lty = 3) +\n  geom_sf(data = states, fill = NA, size = 1) +\n  geom_sf(data = misscount, fill = 'red', alpha = .5) +\n  geom_sf(data = miss, col = \"blue\") +\n  theme_linedraw()"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations",
    "href": "slides/week-3.html#intersecting-populations",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within)\n\n\n\n#&gt; Simple feature collection with 3576 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -112.6693 ymin: 29.2135 xmax: -77.6454 ymax: 48.952\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 3,576 √ó 4\n#&gt;    city        state_name   population           geometry\n#&gt;  * &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;        &lt;POINT [¬∞]&gt;\n#&gt;  1 Minneapolis Minnesota       2906807 (-93.2678 44.9635)\n#&gt;  2 St. Louis   Missouri        2127843 (-90.2451 38.6359)\n#&gt;  3 Pittsburgh  Pennsylvania    1712828 (-79.9763 40.4397)\n#&gt;  4 Cincinnati  Ohio            1704916  (-84.506 39.1413)\n#&gt;  5 Kansas City Missouri        1686807 (-94.5541 39.1238)\n#&gt;  6 Memphis     Tennessee       1033394 (-89.9663 35.1087)\n#&gt;  7 Louisville  Kentucky         965005 (-85.6485 38.1663)\n#&gt;  8 New Orleans Louisiana        918752 (-89.9288 30.0687)\n#&gt;  9 Omaha       Nebraska         826161 (-96.0529 41.2627)\n#&gt; 10 Tulsa       Oklahoma         740620 (-95.9042 36.1283)\n#&gt; # ‚Ñπ 3,566 more rows"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-1",
    "href": "slides/week-3.html#intersecting-populations-1",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name)\n\n\n\n#&gt; Simple feature collection with 3576 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -112.6693 ymin: 29.2135 xmax: -77.6454 ymax: 48.952\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 3,576 √ó 4\n#&gt; # Groups:   state_name [23]\n#&gt;    city        state_name   population           geometry\n#&gt;    &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;        &lt;POINT [¬∞]&gt;\n#&gt;  1 Minneapolis Minnesota       2906807 (-93.2678 44.9635)\n#&gt;  2 St. Louis   Missouri        2127843 (-90.2451 38.6359)\n#&gt;  3 Pittsburgh  Pennsylvania    1712828 (-79.9763 40.4397)\n#&gt;  4 Cincinnati  Ohio            1704916  (-84.506 39.1413)\n#&gt;  5 Kansas City Missouri        1686807 (-94.5541 39.1238)\n#&gt;  6 Memphis     Tennessee       1033394 (-89.9663 35.1087)\n#&gt;  7 Louisville  Kentucky         965005 (-85.6485 38.1663)\n#&gt;  8 New Orleans Louisiana        918752 (-89.9288 30.0687)\n#&gt;  9 Omaha       Nebraska         826161 (-96.0529 41.2627)\n#&gt; 10 Tulsa       Oklahoma         740620 (-95.9042 36.1283)\n#&gt; # ‚Ñπ 3,566 more rows"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-2",
    "href": "slides/week-3.html#intersecting-populations-2",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6)\n\n\n\n#&gt; Simple feature collection with 23 features and 2 fields\n#&gt; Geometry type: MULTIPOINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -112.6693 ymin: 29.2135 xmax: -77.6454 ymax: 48.952\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 23 √ó 3\n#&gt;    state_name    tot                                                    geometry\n#&gt;    &lt;chr&gt;       &lt;dbl&gt;                                            &lt;MULTIPOINT [¬∞]&gt;\n#&gt;  1 Arkansas    1.16  ((-90.7867 34.4605), (-90.8533 34.3087), (-90.9445 34.2181‚Ä¶\n#&gt;  2 Colorado    0.286 ((-102.1247 38.0554), (-102.2217 38.1212), (-102.3115 38.0‚Ä¶\n#&gt;  3 Illinois    0.583 ((-90.0004 38.1306), (-90.0977 38.0816), (-90.2128 38.1644‚Ä¶\n#&gt;  4 Indiana     0.422 ((-84.8544 38.9531), (-84.9236 38.7837), (-84.8301 38.8352‚Ä¶\n#&gt;  5 Iowa        0.792 ((-95.6757 42.4651), (-95.7831 42.4775), (-95.8667 42.5437‚Ä¶\n#&gt;  6 Kansas      1.07  ((-101.7782 37.962), (-100.9936 37.9859), (-101.1332 37.98‚Ä¶\n#&gt;  7 Kentucky    1.87  ((-84.2636 38.9184), (-84.2481 38.89), (-83.8844 38.7563),‚Ä¶\n#&gt;  8 Louisiana   2.41  ((-90.0308 29.2135), (-90.1036 29.7495), (-90.1217 29.7092‚Ä¶\n#&gt;  9 Minnesota   5.90  ((-93.2657 47.1367), (-93.1376 47.1636), (-93.0828 47.4005‚Ä¶\n#&gt; 10 Mississippi 0.284 ((-91.2992 31.1029), (-91.4152 31.496), (-91.3867 31.5437)‚Ä¶\n#&gt; # ‚Ñπ 13 more rows"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-3",
    "href": "slides/week-3.html#intersecting-populations-3",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6) |&gt;\n  st_drop_geometry()\n\n\n\n#&gt; # A tibble: 23 √ó 2\n#&gt;    state_name    tot\n#&gt;  * &lt;chr&gt;       &lt;dbl&gt;\n#&gt;  1 Arkansas    1.16 \n#&gt;  2 Colorado    0.286\n#&gt;  3 Illinois    0.583\n#&gt;  4 Indiana     0.422\n#&gt;  5 Iowa        0.792\n#&gt;  6 Kansas      1.07 \n#&gt;  7 Kentucky    1.87 \n#&gt;  8 Louisiana   2.41 \n#&gt;  9 Minnesota   5.90 \n#&gt; 10 Mississippi 0.284\n#&gt; # ‚Ñπ 13 more rows"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-4",
    "href": "slides/week-3.html#intersecting-populations-4",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6) |&gt;\n  st_drop_geometry() |&gt;\n  ggplot()"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-5",
    "href": "slides/week-3.html#intersecting-populations-5",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6) |&gt;\n  st_drop_geometry() |&gt;\n  ggplot() +\n  geom_col(aes(x = reorder(state_name, -tot), y = tot))"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-6",
    "href": "slides/week-3.html#intersecting-populations-6",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6) |&gt;\n  st_drop_geometry() |&gt;\n  ggplot() +\n  geom_col(aes(x = reorder(state_name, -tot), y = tot)) +\n  theme_linedraw()"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-7",
    "href": "slides/week-3.html#intersecting-populations-7",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6) |&gt;\n  st_drop_geometry() |&gt;\n  ggplot() +\n  geom_col(aes(x = reorder(state_name, -tot), y = tot)) +\n  theme_linedraw() +\n  theme(axis.text.x = element_text(angle = 90) )"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-8",
    "href": "slides/week-3.html#intersecting-populations-8",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6) |&gt;\n  st_drop_geometry() |&gt;\n  ggplot() +\n  geom_col(aes(x = reorder(state_name, -tot), y = tot)) +\n  theme_linedraw() +\n  theme(axis.text.x = element_text(angle = 90) ) +\n  labs(title = \"Population living along Mississippi River System\",\n       x = \"\",\n       y = \"Population (millions)\")"
  },
  {
    "objectID": "slides/week-3.html#intersecting-populations-9",
    "href": "slides/week-3.html#intersecting-populations-9",
    "title": "Week 3",
    "section": "Intersecting Populations",
    "text": "Intersecting Populations\n\n\n\nst_filter(cities, misscount, .predicate = st_within) |&gt;\n  group_by(state_name) |&gt;\n  summarize(tot = sum(population)/1e6) |&gt;\n  st_drop_geometry() |&gt;\n  ggplot() +\n  geom_col(aes(x = reorder(state_name, -tot), y = tot)) +\n  theme_linedraw() +\n  theme(axis.text.x = element_text(angle = 90) ) +\n  labs(title = \"Population living along Mississippi River System\",\n       x = \"\",\n       y = \"Population (millions)\")\n\nggsave(filename = \"images/lecture-12-pop-count.png\", height = 4)\n\n\n\n\n\n\n\n\n\n\n#&gt; Saving 10 x 4 in image"
  },
  {
    "objectID": "slides/week-3.html#section",
    "href": "slides/week-3.html#section",
    "title": "Week 3",
    "section": "",
    "text": "A simpler (binary) version of this matrix can be created by mapping all non-empty intersections {0,1,2} to TRUE.\n\nWhere II would state: ‚ÄúDoes the Interior of‚Äùa‚Äù overlaps with the Interior of ‚Äúb‚Äù in a way that produces a point (0), line (1), or polygon (2)‚Äù\nWhere IB would state: ‚ÄúDoes the Interior of‚Äùa‚Äù overlap with the Boundary of ‚Äúb‚Äù in a way that produces a point (0), line (1), or polygon (2)‚Äù\nBoth matrix forms: - dimensional {0,1,2,F} - Boolean {T,F}\n\nCan be serialize as a ‚ÄúDE-9IM string code‚Äù representing the matrix in a single string element (standardized format for data interchange)\nThe OGC has standardized the typical spatial predicates (Contains, Crosses, Intersects, Touches, etc.) as Boolean functions, and the DE-9IM model as a function that returns the DE-9IM code, with domain of {0,1,2,F}"
  },
  {
    "objectID": "slides/week-3.html#illustration",
    "href": "slides/week-3.html#illustration",
    "title": "Week 3",
    "section": "Illustration",
    "text": "Illustration\n\nReading from left-to-right and top-to-bottom, the DE-9IM(a,b) string code is ‚Äò212101212‚Äô"
  },
  {
    "objectID": "slides/week-3.html#example-dataset",
    "href": "slides/week-3.html#example-dataset",
    "title": "Week 3",
    "section": "Example Dataset",
    "text": "Example Dataset\n\nGeometry X is a 3 feature polygon colored in red\nGeometry Y is a 4 feature polygon colored in blue"
  },
  {
    "objectID": "slides/week-3.html#st_filter",
    "href": "slides/week-3.html#st_filter",
    "title": "Week 3",
    "section": "st_filter",
    "text": "st_filter\n\nst_filter is a generic function that can be used to filter geometries based on a predicate\nThe function is a wrapper for the st_intersects function and can be used to filter geometries based on a predicate\nThe function can be used to filter geometries based on a predicate and return the geometries that satisfy the predicate\n\nFor example, the following code will return all geometries in x that intersect with y:\n\n# filter x by those that intersect y\nst_filter(st_as_sf(x), st_as_sf(y), .predicate = st_intersects)\n#&gt; Simple feature collection with 3 features and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.2 ymin: -1 xmax: 3 ymax: 3\n#&gt; CRS:           NA\n#&gt;                                x\n#&gt; 1 POLYGON ((-1 -1, 1 -1, 1 1,...\n#&gt; 2 POLYGON ((1 1, 3 1, 3 3, 1 ...\n#&gt; 3 POLYGON ((-1.2 1, 0.8 1, 0....\nst_filter(st_as_sf(x), st_as_sf(y), .predicate = st_disjoint)\n#&gt; Simple feature collection with 3 features and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1.2 ymin: -1 xmax: 3 ymax: 3\n#&gt; CRS:           NA\n#&gt;                                x\n#&gt; 1 POLYGON ((-1 -1, 1 -1, 1 1,...\n#&gt; 2 POLYGON ((1 1, 3 1, 3 3, 1 ...\n#&gt; 3 POLYGON ((-1.2 1, 0.8 1, 0....\nst_filter(st_as_sf(x), st_as_sf(y), .predicate = st_touches)\n#&gt; Simple feature collection with 0 features and 0 fields\n#&gt; Bounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\n#&gt; CRS:           NA\n#&gt; [1] x\n#&gt; &lt;0 rows&gt; (or 0-length row.names)"
  },
  {
    "objectID": "slides/week-3.html#example",
    "href": "slides/week-3.html#example",
    "title": "Week 3",
    "section": "Example",
    "text": "Example\n\nYou have been tasked by the USACE of identify the population living along the Mississippi River systems\nYou are interested in the county level since most flood control measures and flood response efforts are enforced by county EMAs\nFederal funding however is administered at the state level so you need population counts aggregated to state‚Ä¶\n\n\n# Global River Shapefile filtered to the Mississippi System\nmiss = read_sf('data/majorrivers_0_0/MajorRivers.shp') |&gt; \n  filter(SYSTEM == \"Mississippi\") \n\n# Which counties intersect this system\nmisscount = st_filter(aoi_get(state = \"conus\", county = \"all\"), miss,  .predicate = st_intersects) \n\n# Find all counties in the intersecting states\ncounties =  filter(aoi_get(state = \"conus\", county = \"all\"), state_name %in% c(misscount$state_name) )\n\n# Find all impacted states\nstates   =  filter(aoi_get(state = \"conus\"), state_name %in% c(misscount$state_name))"
  },
  {
    "objectID": "slides/week-3.html#result",
    "href": "slides/week-3.html#result",
    "title": "Week 3",
    "section": "Result",
    "text": "Result\n\nggplot(states) + \n  geom_sf() + \n  geom_sf(data = wa, fill = \"blue\", alpha = .3) +\n  geom_sf(data = st_filter(states, wa, .predicate = st_touches), fill = \"red\", alpha = .5) + \n  theme_void()"
  },
  {
    "objectID": "slides/week-3.html#distance-example-additional-parameter",
    "href": "slides/week-3.html#distance-example-additional-parameter",
    "title": "Week 3",
    "section": "Distance Example (additional parameter)",
    "text": "Distance Example (additional parameter)\n\ncities = read_csv(\"../labs/data/uscities.csv\") |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  select(city, population, state_name) |&gt; \n  st_transform(5070)\n\n\nst_filter(cities, \n          filter(cities, city == \"Fort Collins\"), \n          .predicate = st_is_within_distance, 10000) \n#&gt; Simple feature collection with 2 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -760147.5 ymin: 1982249 xmax: -751658.3 ymax: 1984621\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt; # A tibble: 2 √ó 4\n#&gt;   city         population state_name            geometry\n#&gt; * &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;              &lt;POINT [m]&gt;\n#&gt; 1 Fort Collins     339256 Colorado   (-760147.5 1984621)\n#&gt; 2 Timnath            8007 Colorado   (-751658.3 1982249)"
  },
  {
    "objectID": "slides/week-3.html#st_join",
    "href": "slides/week-3.html#st_join",
    "title": "Week 3",
    "section": "st_join",
    "text": "st_join\n\nIn sf st_join provides this joining capacity\nBy default, st_join performs a left join (Returns all records from x, and the matched records from y)\n\n\n\n\n\n\n\n\n\n\n\nIt can also do inner joins by setting left = FALSE.\n\n\n\n\n\n\n\n\n\n\n\nThe default predicate for st_join (and st_filter) is st_intersects\nThis can be changed with the join argument (see ?st_join for details)."
  },
  {
    "objectID": "slides/week-3.html#ramerdouglaspeucker",
    "href": "slides/week-3.html#ramerdouglaspeucker",
    "title": "Week 3",
    "section": "Ramer‚ÄìDouglas‚ÄìPeucker",
    "text": "Ramer‚ÄìDouglas‚ÄìPeucker\n\nMark the first and last points as kept\nFind the point, p that is the farthest from the first-last line segment. If there are no points between first and last we are done (the base case)\nIf p is closer than tolerance units to the line segment then everything between first and last can be discarded\nOtherwise, mark p as kept and repeat steps 1-4 using the points between first and p and between p and last (the call to recursion)"
  },
  {
    "objectID": "slides/week-3.html#st_simplify",
    "href": "slides/week-3.html#st_simplify",
    "title": "Week 3",
    "section": "st_simplify",
    "text": "st_simplify"
  },
  {
    "objectID": "slides/week-3.html#st_simplify-1",
    "href": "slides/week-3.html#st_simplify-1",
    "title": "Week 3",
    "section": "st_simplify",
    "text": "st_simplify\n\nsf provides st_simplify, which uses the GEOS implementation of the Douglas-Peucker algorithm to reduce the vertex count.\nst_simplify uses the dTolerance to control the level of generalization in map units (see Douglas and Peucker 1973 for details)."
  },
  {
    "objectID": "slides/week-3.html#section-1",
    "href": "slides/week-3.html#section-1",
    "title": "Week 3",
    "section": "",
    "text": "topSB &lt;- st_join(counties, starbucks) |&gt; \n  group_by(name, state_name) |&gt; \n  summarise(n = n()) |&gt; \n  group_by(state_name) |&gt; \n  slice_max(n, n = 1) |&gt; \n  ungroup()"
  },
  {
    "objectID": "slides/week-3.html#visvalingam",
    "href": "slides/week-3.html#visvalingam",
    "title": "Week 3",
    "section": "Visvalingam",
    "text": "Visvalingam\n\nThe Visvalingam algorithm overcomes some limitations of the Douglas-Peucker algorithm (Visvalingam and Whyatt 1993).\nit progressively removes points with the least-perceptible change.\nSimplification often allows the elimination of 95% or more points while retaining sufficient detail for visualization and often analysis"
  },
  {
    "objectID": "slides/week-3.html#section-2",
    "href": "slides/week-3.html#section-2",
    "title": "Week 3",
    "section": "",
    "text": "Neither joining nor filtering spatially alter the underlying geometry of the features\nIn cases where we seek to alter a geometry based on another, we need clipping methods"
  },
  {
    "objectID": "slides/week-3.html#section-3",
    "href": "slides/week-3.html#section-3",
    "title": "Week 3",
    "section": "",
    "text": "In cases where we want to reduce the complexity in a geometry we can use simplification algorithms\nThe most common algorithms are Ramer‚ÄìDouglas‚ÄìPeucker and Visvalingam-Whyatt."
  },
  {
    "objectID": "slides/week-3.html#section-4",
    "href": "slides/week-3.html#section-4",
    "title": "Week 3",
    "section": "",
    "text": "usa = aoi_get(state = \"conus\") |&gt; \n  st_union() |&gt; \n  st_transform(5070)\n\nusa1000   = st_simplify(usa, dTolerance = 10000)\nusa10000  = st_simplify(usa, dTolerance = 100000)\nusa100000 = st_simplify(usa, dTolerance = 1000000)"
  },
  {
    "objectID": "slides/week-3.html#core-concepts-of-spatial-data-kuhn-2012",
    "href": "slides/week-3.html#core-concepts-of-spatial-data-kuhn-2012",
    "title": "Week 3",
    "section": "Core Concepts of Spatial Data: (Kuhn 2012)",
    "text": "Core Concepts of Spatial Data: (Kuhn 2012)"
  },
  {
    "objectID": "slides/week-3.html#core-concepts-of-spatial-data-kuhn-2012-1",
    "href": "slides/week-3.html#core-concepts-of-spatial-data-kuhn-2012-1",
    "title": "Week 3",
    "section": "Core Concepts of Spatial Data: (Kuhn 2012)",
    "text": "Core Concepts of Spatial Data: (Kuhn 2012)\n\nOne Base Concept: Location\nFour Content Concepts: Field, Object, Network, Event\nTwo Quality Concepts: Granularity, Accuracy"
  },
  {
    "objectID": "slides/week-3.html#core-concepts-of-spatial-data-kuhn-2012-2",
    "href": "slides/week-3.html#core-concepts-of-spatial-data-kuhn-2012-2",
    "title": "Week 3",
    "section": "Core Concepts of Spatial Data: (Kuhn 2012)",
    "text": "Core Concepts of Spatial Data: (Kuhn 2012)\n\nOne Base Concept: Location (coordinates)\nFour Content Concepts: Field (raster), Object (simple feature), Network, Event\nTwo Quality Concepts: Granularity (simplification), Accuracy (taken for granted)"
  },
  {
    "objectID": "slides/week-3.html#object-view",
    "href": "slides/week-3.html#object-view",
    "title": "Week 3",
    "section": "Object View",
    "text": "Object View\n\nObjects describe individuals that have an identity (id) as well as spatial, temporal, and thematic properties.\nAnswers questions about properties and relations of objects.\nResults from fixing theme, controlling time, and measuring space.\nFeatures, such as surfaces, depend on objects (but are also objects)"
  },
  {
    "objectID": "slides/week-3.html#object-view-1",
    "href": "slides/week-3.html#object-view-1",
    "title": "Week 3",
    "section": "Object View",
    "text": "Object View\n\nObject implies boundedness\n\nboundaries may not be known or even knowable, but have limits.\n\nCrude examples of such limits are the minimal bounding boxes used for indexing and querying objects in databases.\nMany objects (particularly natural ones) do not have crisp boundaries (watersheds)\nDifferences between spatial information from multiple sources are often caused by more or less arbitrary delineations through context-dependent boundaries.\nMany questions about objects and features can be answered without boundaries, using simple point representations (centroids) with thematic attributes."
  },
  {
    "objectID": "slides/week-3.html#field-view",
    "href": "slides/week-3.html#field-view",
    "title": "Week 3",
    "section": "Field View",
    "text": "Field View\n\nFields describe phenomena that have a scalar or vector attribute everywhere in a space of interest\n\nfor example, air temperatures, rainfall, elevation, land cover\n\nField information answers the question what is here?, where here can be anywhere in the space considered.\nField-based spatial information can also represent attributes that are computed rather than measured, such as probabilities or densities.\n\n\nTogether, fields and objects are the two fundamental ways of structuring spatial information."
  },
  {
    "objectID": "slides/week-3.html#objects-can-provide-coverage",
    "href": "slides/week-3.html#objects-can-provide-coverage",
    "title": "Week 3",
    "section": "Objects can provide coverage:",
    "text": "Objects can provide coverage:\n\nBoth objects and fields can cover space continuously - the primary difference is that objects prescribe bounds.\n\nCounties are one form of object that covers the USA seamlessly\nState objects are another ‚Ä¶"
  },
  {
    "objectID": "slides/week-3.html#landsat-path-row",
    "href": "slides/week-3.html#landsat-path-row",
    "title": "Week 3",
    "section": "LANDSAT Path Row",
    "text": "LANDSAT Path Row\n\nServes tiles based on a path/row index"
  },
  {
    "objectID": "slides/week-3.html#modis-sinisoial-grid",
    "href": "slides/week-3.html#modis-sinisoial-grid",
    "title": "Week 3",
    "section": "MODIS Sinisoial Grid",
    "text": "MODIS Sinisoial Grid\n\nServes tiles based on a path/row index"
  },
  {
    "objectID": "slides/week-3.html#uber-hex-addressing",
    "href": "slides/week-3.html#uber-hex-addressing",
    "title": "Week 3",
    "section": "Uber Hex Addressing",
    "text": "Uber Hex Addressing\n\nBreaks the world into Hexagons‚Ä¶"
  },
  {
    "objectID": "slides/week-3.html#what3word",
    "href": "slides/week-3.html#what3word",
    "title": "Week 3",
    "section": "what3word",
    "text": "what3word\n\nBreaks the world into 3m grids encoded with unique 3 word strings"
  },
  {
    "objectID": "slides/week-3.html#tesselation-plotting-function",
    "href": "slides/week-3.html#tesselation-plotting-function",
    "title": "Week 3",
    "section": "Tesselation plotting function",
    "text": "Tesselation plotting function\n\nplot_tess = function(data, title){\n  ggplot() + \n    geom_sf(data = data, fill = \"white\", col = \"navy\", size = .2) +   \n    theme_void() +\n    labs(title = title, caption = paste(\"This tesselation has:\", nrow(data), \"tiles\" )) +\n    theme(plot.title = element_text(hjust = .5, color =  \"navy\", face = \"bold\"))\n}"
  },
  {
    "objectID": "slides/week-3.html#section-5",
    "href": "slides/week-3.html#section-5",
    "title": "Week 3",
    "section": "",
    "text": "A limitation with Douglas-Peucker (therefore st_simplify) is that it simplifies objects on a per-geometry basis.\nThis means the ‚Äòtopology‚Äô is lost, resulting in overlapping and disconnected geometries.\n\n\nstates = st_transform(states, 5070)\nsimp_states   = st_simplify(states, dTolerance = 20000)\nplot(simp_states$geometry)"
  },
  {
    "objectID": "slides/week-3.html#hexagonal-grid",
    "href": "slides/week-3.html#hexagonal-grid",
    "title": "Week 3",
    "section": "Hexagonal Grid",
    "text": "Hexagonal Grid\n\nHexagonal tessellations (honey combs) offer an alternative to square grids\nThey are created in the same way but by setting square = FALSE\n\n\nhex_grid = st_make_grid(south_counties, n = c(70, 50), square = FALSE) |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())"
  },
  {
    "objectID": "slides/week-3.html#advantages-square-grids",
    "href": "slides/week-3.html#advantages-square-grids",
    "title": "Week 3",
    "section": "Advantages Square Grids",
    "text": "Advantages Square Grids\n\nSimple definition and data storage\n\nOnly need the origin (lower left), cell size (XY) and grid dimensions\n\nEasy to aggregate and dissaggregate (resample)\nAnalogous to raster data\nRelationship between cells is given\nCombining layers is easy with traditional matrix algebra"
  },
  {
    "objectID": "slides/week-3.html#triangulations",
    "href": "slides/week-3.html#triangulations",
    "title": "Week 3",
    "section": "Triangulations",
    "text": "Triangulations\n\nAn alternative to creating equal area tiles is to create triangulations from known anchor points\nTriangulation requires a set of input points and seeks to partition the interior into a partition of triangles.\nIn GIS contexts you‚Äôll hear:\n\nThiessen Polygon\nVoronoi Regions\nDelunay Triangulation\nTIN (Triangular irregular networks)\netc,.."
  },
  {
    "objectID": "slides/week-3.html#voronoi-polygons",
    "href": "slides/week-3.html#voronoi-polygons",
    "title": "Week 3",
    "section": "Voronoi Polygons",
    "text": "Voronoi Polygons\n\nVoronoi/Thiessen polygon boundaries define the area closest to each anchor point relative to all others\nThey are defined by the perpendicular bisectors of the lines between all points."
  },
  {
    "objectID": "slides/week-3.html#voronoi-polygons-1",
    "href": "slides/week-3.html#voronoi-polygons-1",
    "title": "Week 3",
    "section": "Voronoi Polygons",
    "text": "Voronoi Polygons"
  },
  {
    "objectID": "slides/week-3.html#voronoi-polygons-2",
    "href": "slides/week-3.html#voronoi-polygons-2",
    "title": "Week 3",
    "section": "Voronoi Polygons",
    "text": "Voronoi Polygons\n\nUsefull for tasks such as:\n\nnearest neighbor search,\nfacility location (optimization),\nlargest empty areas,\npath planning‚Ä¶\n\nAlso useful for simple interpolation of values such as rain gauges,"
  },
  {
    "objectID": "slides/week-3.html#often-used-in-numerical-models-and-simulations",
    "href": "slides/week-3.html#often-used-in-numerical-models-and-simulations",
    "title": "Week 3",
    "section": "Often used in numerical models and simulations",
    "text": "Often used in numerical models and simulations"
  },
  {
    "objectID": "slides/week-3.html#st_voronoi",
    "href": "slides/week-3.html#st_voronoi",
    "title": "Week 3",
    "section": "st_voronoi",
    "text": "st_voronoi\n\nst_voronoi creates voronoi tesselation in sf\nIt works over a MULTIPOINT collection\nShould always be simplified after creation (st_cast())\nIf to be treated as an object for analysis, should be id‚Äôd\n\n\nsouth_cent_u = st_union(south_cent)\n\nv_grid = st_voronoi(south_cent_u) |&gt; \n  st_cast() |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())"
  },
  {
    "objectID": "slides/week-3.html#delaunay-triangulation",
    "href": "slides/week-3.html#delaunay-triangulation",
    "title": "Week 3",
    "section": "Delaunay triangulation",
    "text": "Delaunay triangulation\n\nA Delaunay triangulation for a given set of points (P) in a plane, is a triangulation DT(P), where no point is inside the circumcircle of any triangle in DT(P)."
  },
  {
    "objectID": "slides/week-3.html#delaunay-triangulation-1",
    "href": "slides/week-3.html#delaunay-triangulation-1",
    "title": "Week 3",
    "section": "Delaunay triangulation",
    "text": "Delaunay triangulation\n\nThe Delaunay triangulation of a discrete POINT set corresponds to the dual graph of the Voronoi diagram.\nThe circumcenters (center of circles) of Delaunay triangles are the vertices of the Voronoi diagram."
  },
  {
    "objectID": "slides/week-3.html#used-in-landscape-evaluation-and-terrian-modeling",
    "href": "slides/week-3.html#used-in-landscape-evaluation-and-terrian-modeling",
    "title": "Week 3",
    "section": "Used in landscape evaluation and terrian modeling",
    "text": "Used in landscape evaluation and terrian modeling"
  },
  {
    "objectID": "slides/week-3.html#st_triangulate",
    "href": "slides/week-3.html#st_triangulate",
    "title": "Week 3",
    "section": "st_triangulate",
    "text": "st_triangulate\n\nst_triangulate creates Delaunay triangulation in sf\nIt works over a MULTIPOINT collection\nShould always be simplified after creation (st_cast())\nIf to be treated as an object for analysis, should be id‚Äôd\n\n\nt_grid = st_triangulate(south_cent_u) |&gt; \n  st_cast() |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())"
  },
  {
    "objectID": "slides/week-3.html#topologic-diminsion",
    "href": "slides/week-3.html#topologic-diminsion",
    "title": "Week 3",
    "section": "Topologic Diminsion",
    "text": "Topologic Diminsion\nA POINT is shape with a dimension of 0 that occupies a single location in coordinate space.\n\n# POINT defined as numeric vector\n(st_dimension(st_point(c(0,1))))\n#&gt; [1] 0\n\nA LINESTRING is shape that has a dimension of 1 (length)\n\n# LINESTRING defined by matrix\n(st_dimension(st_linestring(matrix(1:4, nrow = 2))))\n#&gt; [1] 1\n\nA POLYGON is surface stored as a list of its exterior and interior rings. It has a dimension of 2. (area)\n\n# POLYGON defined by LIST (interior and exterior rings)\n(st_dimension(st_polygon(list(matrix(c(1:4, 1,2), nrow = 3, byrow = TRUE)))))\n#&gt; [1] 2"
  },
  {
    "objectID": "slides/week-3.html#predicates-1",
    "href": "slides/week-3.html#predicates-1",
    "title": "Week 3",
    "section": "Predicates",
    "text": "Predicates\nIn the following, we are interested in the resulting geometry that occurs when 2 geometries are overlaid‚Ä¶"
  },
  {
    "objectID": "slides/week-3.html#overlap-is-a-point-0d",
    "href": "slides/week-3.html#overlap-is-a-point-0d",
    "title": "Week 3",
    "section": "Overlap is a POINT: 0D",
    "text": "Overlap is a POINT: 0D"
  },
  {
    "objectID": "slides/week-3.html#overlap-is-a-linestring-1d",
    "href": "slides/week-3.html#overlap-is-a-linestring-1d",
    "title": "Week 3",
    "section": "Overlap is a LINESTRING: 1D",
    "text": "Overlap is a LINESTRING: 1D"
  },
  {
    "objectID": "slides/week-3.html#overlap-is-a-polygon-2d",
    "href": "slides/week-3.html#overlap-is-a-polygon-2d",
    "title": "Week 3",
    "section": "Overlap is a POLYGON: 2D",
    "text": "Overlap is a POLYGON: 2D"
  },
  {
    "objectID": "slides/week-3.html#no-overlap-false",
    "href": "slides/week-3.html#no-overlap-false",
    "title": "Week 3",
    "section": "No Overlap = FALSE",
    "text": "No Overlap = FALSE"
  },
  {
    "objectID": "slides/week-3.html#dimensionally-extended-9-intersection-model-de-9im",
    "href": "slides/week-3.html#dimensionally-extended-9-intersection-model-de-9im",
    "title": "Week 3",
    "section": "Dimensionally Extended 9-Intersection Model (DE-9IM)",
    "text": "Dimensionally Extended 9-Intersection Model (DE-9IM)\n\nThe DE-9IM is a topological model and (standard) used to describe the spatial relations of two geometries\nUsed in geometry, point-set topology, geospatial topology\nThe DE-9IM matrix provides a way to classify geometry relations using the set {0,1,2,F} or {T,F}\nWith a {T,F} matrix domain, there are 512 possible relations that can be grouped into binary classification schemes.\nAbout 10 of these, have been given a common name such as intersects, touches, and within.\nWhen testing two geometries against a scheme, the result is a spatial predicate named by the scheme.\nProvides the primary basis for queries and assertions in GIS and spatial databases (PostGIS)."
  },
  {
    "objectID": "slides/week-3.html#the-matrix-model",
    "href": "slides/week-3.html#the-matrix-model",
    "title": "Week 3",
    "section": "The Matrix Model",
    "text": "The Matrix Model\nThe DE-9IM matrix is based on a 3x3 intersection matrix:\n\nWhere:\n\ndim is the dimension of the intersection and\nI is the interior\nB is the boundary\nE is the exterior\n\n\nEmpty sets are denoted as F\nnon-empty sets are denoted with the maximum dimension of the intersection {0,1,2}"
  },
  {
    "objectID": "slides/week-3.html#spatial-predicates",
    "href": "slides/week-3.html#spatial-predicates",
    "title": "Week 3",
    "section": "Spatial Predicates",
    "text": "Spatial Predicates\n\n‚Äúnamed spatial predicates‚Äù have been defined for some common relations.\nA few spatial predicate functions that can be derived (expressed by masks) from DE-9IM include (* = wildcard):\n\n\n\n\n\n\n\n\n\nPredicate\nDE-9IM String Code\nDescription\n\n\n\n\nIntersects\nT*F**FFF*\n‚ÄúTwo geometries intersect if they share any portion of space‚Äù\n\n\nOverlaps\nT*F**FFF*\n‚ÄúTwo geometries overlap if they share some but not all of the same space‚Äù\n\n\nEquals\nT*F**FFF*\n‚ÄúTwo geometries are topologically equal if their interiors intersect and no part of the interior or boundary of one geometry intersects the exterior of the other‚Äù\n\n\nDisjoint\nFF*FF*****\n‚ÄúTwo geometries are disjoint: they have no point in common. They form a set of disconnected geometries.‚Äù\n\n\nTouches\nFT*******\nF**T*****\n\n\nContains\nT*****FF**\n‚ÄúA contains B: geometry B lies in A, and the interiors intersect‚Äù\n\n\nCovers\nT*****FF*\n*T****FF*\n\n\nWithin\n*T*****FF*\n**T****FF*\n\n\nCovered by\n*T*****FF*\n**T****FF*"
  },
  {
    "objectID": "slides/week-3.html#spatial-predicates-in-r",
    "href": "slides/week-3.html#spatial-predicates-in-r",
    "title": "Week 3",
    "section": "Spatial Predicates in R",
    "text": "Spatial Predicates in R\n\nGeometry X is a 3 feature polygon colored in red\nGeometry Y is a 4 feature polygon colored in blue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nst_relate(x,y)\n#&gt;      [,1]        [,2]        [,3]        [,4]       \n#&gt; [1,] \"212FF1FF2\" \"FF2FF1212\" \"212101212\" \"FF2FF1212\"\n#&gt; [2,] \"FF2FF1212\" \"212101212\" \"212101212\" \"FF2FF1212\"\n#&gt; [3,] \"FF2FF1212\" \"FF2FF1212\" \"212101212\" \"FF2FF1212\"\nst_relate(x,x)\n#&gt;      [,1]        [,2]        [,3]       \n#&gt; [1,] \"2FFF1FFF2\" \"FF2F01212\" \"FF2F11212\"\n#&gt; [2,] \"FF2F01212\" \"2FFF1FFF2\" \"FF2FF1212\"\n#&gt; [3,] \"FF2F11212\" \"FF2FF1212\" \"2FFF1FFF2\"\nst_relate(y,y)\n#&gt;      [,1]        [,2]        [,3]        [,4]       \n#&gt; [1,] \"2FFF1FFF2\" \"FF2FF1212\" \"212101212\" \"FF2FF1212\"\n#&gt; [2,] \"FF2FF1212\" \"2FFF1FFF2\" \"212101212\" \"FF2FF1212\"\n#&gt; [3,] \"212101212\" \"212101212\" \"2FFF1FFF2\" \"FF2FF1212\"\n#&gt; [4,] \"FF2FF1212\" \"FF2FF1212\" \"FF2FF1212\" \"2FFF1FFF2\"\n\n\nSpatial Predicates\n\n‚Äúnamed spatial predicates‚Äù have been defined for some common relations.\nA few spatial predicate functions that can be derived (expressed by masks) from DE-9IM include (* = wildcard):\n\n\n\n\n\n\n\n\n\nPredicate\nDE-9IM String Code\nDescription\n\n\n\n\nIntersects\nT*F**FFF*\n‚ÄúTwo geometries intersect if they share any portion of space‚Äù\n\n\nOverlaps\nT*F**FFF*\n‚ÄúTwo geometries overlap if they share some but not all of the same space‚Äù\n\n\nEquals\nT*F**FFF*\n‚ÄúTwo geometries are topologically equal if their interiors intersect and no part of the interior or boundary of one geometry intersects the exterior of the other‚Äù\n\n\nDisjoint\nFF*FF*****\n‚ÄúTwo geometries are disjoint: they have no point in common. They form a set of disconnected geometries.‚Äù\n\n\nTouches\nFT*******\nF**T*****\n\n\nContains\nT*****FF**\n‚ÄúA contains B: geometry B lies in A, and the interiors intersect‚Äù\n\n\nCovers\nT*****FF*\n*T****FF*\n\n\nWithin\n*T*****FF*\n**T****FF*\n\n\nCovered by\n*T*****FF*\n**T****FF*\n\n\n\n\nBinary logical operations\nReturns either a sparse matrix\n\nst_intersects(x,y)\n#&gt; Sparse geometry binary predicate list of length 3, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1, 3\n#&gt;  2: 2, 3\n#&gt;  3: 3\n\nor a dense matrix\n\nst_intersects(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2] [,3]  [,4]\n#&gt; [1,]  TRUE FALSE TRUE FALSE\n#&gt; [2,] FALSE  TRUE TRUE FALSE\n#&gt; [3,] FALSE FALSE TRUE FALSE\n\nst_disjoint(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3] [,4]\n#&gt; [1,] FALSE  TRUE FALSE TRUE\n#&gt; [2,]  TRUE FALSE FALSE TRUE\n#&gt; [3,]  TRUE  TRUE FALSE TRUE\n\nst_touches(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE\n\nst_within(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE\n\n\nst_realtes vs.¬†predicate calls‚Ä¶\nFT*******; F**T*****; F***T****\n\nstates = filter(aoi_get(state = \"all\"), state_abbr %in% c(\"WA\", \"OR\", \"MT\", \"ID\")) |&gt;\n  select(name)\n\nwa = filter(states, name == \"Washington\")\n\n\nplot(states$geometry, col = \"white\", border = \"black\")\n\n\n(mutate(states, \n        deim9 = st_relate(states, wa),\n        touch = st_touches(states, wa, sparse = F)))\n#&gt; Simple feature collection with 4 features and 3 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 41.98818 xmax: -104.0397 ymax: 49.00244\n#&gt; Geodetic CRS:  WGS 84\n#&gt;         name                       geometry     deim9 touch\n#&gt; 1      Idaho MULTIPOLYGON (((-111.0455 4... FF2F11212  TRUE\n#&gt; 2    Montana MULTIPOLYGON (((-109.7985 4... FF2FF1212 FALSE\n#&gt; 3     Oregon MULTIPOLYGON (((-117.22 44.... FF2F11212  TRUE\n#&gt; 4 Washington MULTIPOLYGON (((-121.5237 4... 2FFF1FFF2 FALSE\n\n\nBinary Predicates\n\nCollectively, predicates define the type of relationship each 2D object has with another.\nOf the ~ 512 unique relationships offered by the DE-9IM models a selection of ~ 10 have been named.\nThese are include in PostGIS/GEOS and are made accessible via R sf\n\n\n\nSo‚Ä¶\n\nbinary predicates return conditional {T,F} relations based on predefined masks of the DE-9IM strings\nUp to this point in class we have been using Boolean {T,F} masks to filter data.frames by columns:\n\n\nfruits = c(\"apple\", \"orange\", \"lemon\", \"watermelon\")\n\n\nfruits == \"apple\"\n#&gt; [1]  TRUE FALSE FALSE FALSE\n\n\nfruits %in% c(\"apple\", \"lemon\")\n#&gt; [1]  TRUE FALSE  TRUE FALSE\n\n\n\nFiltering on data.frames\n\nWe have used dplyr::filter to subset a data frame, retaining all rows that satisfy a boolean condition.\n\n\n\n\nmutate(states, equalsWA = (name == \"Washington\")) |&gt; \n  st_drop_geometry()\n#&gt;         name equalsWA\n#&gt; 1      Idaho    FALSE\n#&gt; 2    Montana    FALSE\n#&gt; 3     Oregon    FALSE\n#&gt; 4 Washington     TRUE\n\n\n\nfilter(states, name == \"Washington\") |&gt; \n  st_drop_geometry()\n#&gt;         name\n#&gt; 1 Washington\n\n\n\n\n\nSpatial Filtering\n\nWe can filter spatially, using st_filter as the function call\nHere the boolean condition is not passed (e.g.¬†stusps == WA)\nBut instead, is defined by a spatial predicate\nThe default predicate is st_intersects but can be changed with the .predicate argument:\n\n\n\n\nmutate(states, \n       touch = st_touches(states, wa, sparse = FALSE)) \n#&gt; Simple feature collection with 4 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 41.98818 xmax: -104.0397 ymax: 49.00244\n#&gt; Geodetic CRS:  WGS 84\n#&gt;         name                       geometry touch\n#&gt; 1      Idaho MULTIPOLYGON (((-111.0455 4...  TRUE\n#&gt; 2    Montana MULTIPOLYGON (((-109.7985 4... FALSE\n#&gt; 3     Oregon MULTIPOLYGON (((-117.22 44....  TRUE\n#&gt; 4 Washington MULTIPOLYGON (((-121.5237 4... FALSE\n\n\n\nst_filter(states, wa, .predicate = st_touches) \n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7035 ymin: 41.98818 xmax: -111.0435 ymax: 49.00085\n#&gt; Geodetic CRS:  WGS 84\n#&gt;     name                       geometry\n#&gt; 1  Idaho MULTIPOLYGON (((-111.0455 4...\n#&gt; 2 Oregon MULTIPOLYGON (((-117.22 44....\n\n\n\n\n\nResult\n\nggplot(states) + \n  geom_sf() + \n  geom_sf(data = wa, fill = \"blue\", alpha = .3) +\n  geom_sf(data = st_filter(states, wa, .predicate = st_touches), fill = \"red\", alpha = .5) + \n  theme_void()\n\n\n\n\nDistance Example (additional parameter)\n\ncities = read_csv(\"../labs/data/uscities.csv\") |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  select(city, population, state_name) |&gt; \n  st_transform(5070)\n\n\nfoco = filter(cities, city == \"Fort Collins\")\n\n\nst_filter(cities, foco, .predicate = st_is_within_distance, 10000) \n#&gt; Simple feature collection with 2 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -760147.5 ymin: 1982249 xmax: -751658.3 ymax: 1984621\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt; # A tibble: 2 √ó 4\n#&gt;   city         population state_name            geometry\n#&gt; * &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;              &lt;POINT [m]&gt;\n#&gt; 1 Fort Collins     339256 Colorado   (-760147.5 1984621)\n#&gt; 2 Timnath            8007 Colorado   (-751658.3 1982249)\n\n\n\nThink back to week 1: Data Science\n\nFiltering is a nice way to reduce the dimensions of a single dataset\nOften ‚Äú‚Ä¶ one table is not enough‚Ä¶ ‚Äù\nIn these cases we want to combine - or join - data\n\n\n\nSpatial Joining\n\nJoining two non-spatial datasets relies on a shared key that uniquely identifies each record in a table\n\n\n\nSpatially joining data relies on shared geographic relations rather then a shared key\nLike filter, these relations can be defined by a predicate\nAs with tabular data, mutating joins add data to the target object (x) from a source object (y).\n\n\n\nst_join\n\nIn sf st_join provides this joining capacity\nBy default, st_join performs a left join (Returns all records from x, and the matched records from y)\n\n\n\n\n\n\n\n\n\n\n\nIt can also do inner joins by setting left = FALSE.\n\n\n\n\n\n\n\n\n\n\n\nThe default topological predicate st_join (and st_filter) is st_intersects\nThis can be changed with the join argument (see ?st_join for details).\n\n\n\nEvery Starbucks in the World\nWhat county has the most Starbucks in each state?\n\nstarbucks = readr::read_csv('../labs/data/directory.csv') |&gt; \n  filter(!is.na(Latitude), Country == \"US\") |&gt; \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |&gt; \n  st_transform(5070)\n\n‚Äì\n\nnrow(starbucks)\n#&gt; [1] 13608\n\nnames(starbucks)\n#&gt;  [1] \"Brand\"          \"Store Number\"   \"Store Name\"     \"Ownership Type\"\n#&gt;  [5] \"Street Address\" \"City\"           \"State/Province\" \"Country\"       \n#&gt;  [9] \"Postcode\"       \"Phone Number\"   \"Timezone\"       \"geometry\"\n\n‚Äì\n\ncounties = aoi_get(state = \"conus\", county = \"all\") |&gt; \n  st_transform(5070) |&gt; \n  select(name, state_name)\n\n\n\ntopSB = st_join(counties, starbucks) |&gt; \n  group_by(name, state_name) |&gt; \n  summarise(n = n()) |&gt; \n  group_by(state_name) |&gt; \n  slice_max(n, n = 1) |&gt; \n  ungroup()\n\n\n\nNeither joining nor filtering spatially alter the underlying geometry of the features\nIn cases where we seek to alter a geometry bases on another, we need clipping methods\n\n\n\nClipping\n\nClipping is a form of subsetting that involves changing the geometry of at least some features.\nClipping can only apply to features more complex than points: (lines, polygons and their ‚Äòmulti‚Äô equivalents).\n\n\n\n\nSpatial Subsetting\n\nBy default the data.frame subsetting methods we‚Äôve seen (e.g [,]) implements st_intersection\n\n\nwa = st_transform(wa, 5070)\nwa_starbucks = starbucks[wa,] #&lt;&lt;\n\nggplot() + geom_sf(data = wa) + geom_sf(data = wa_starbucks) + theme_void()\n\n\n\n\nSimplification\n\n\n\nComputational Complexity\n\nIn all the cases we have looked at, the number of POINT (e.g geometries, nodes, or vertices) define the complexity of the predicate or the clip\nComputational needs increase with the number of POINTS / NODES / VERTICES\nSimplification is a process for generalization vector objects (lines and polygons)\nAnother reason for simplifying objects is to reduce the amount of memory, disk space and network bandwidth they consume\nOther times the level of detail captured in a geometry is either not needed, or, even counter productive the the scale/purpose of an analysis\nIf you are cropping features to a national border, how much detail do you need? The more points in your border, the long the clip operation will take.\nIn cases where we want to reduce th complexity in a geometry we can use simplification algorithms\n\n\n\nRamer‚ÄìDouglas‚ÄìPeucker\n\nMark the first and last points as kept\nFind the point, p that is the farthest from the first-last line segment. If there are no points between first and last we are done (the base case)\nIf p is closer than tolerance units to the line segment then everything between first and last can be discarded\nOtherwise, mark p as kept and repeat steps 1-4 using the points between first and p and between p and last (the call to recursion)\n\n\n\n\nst_simplify\n\n\n\nst_simplify\n\nsf provides st_simplify, which uses the GEOS implementation of the Douglas-Peucker algorithm to reduce the vertex count.\nst_simplify uses the dTolerance to control the level of generalization in map units (see Douglas and Peucker 1973 for details).\n\n\n\n\n\n\nusa = aoi_get(state = \"conus\") |&gt; \n  st_union() |&gt; \n  st_transform(5070)\n\nusa1000   = st_simplify(usa, dTolerance = 10000)\nusa10000  = st_simplify(usa, dTolerance = 100000)\nusa100000 = st_simplify(usa, dTolerance = 1000000)\n\n\n\n\n\n\n\n\n\n\n\nA limitation with Douglas-Peucker (therefore st_simplify) is that it simplifies objects on a per-geometry basis.\nThis means the ‚Äòtopology‚Äô is lost, resulting in overlapping and disconnected geometries.\n\n\nstates = st_transform(states, 5070)\nsimp_states   = st_simplify(states, dTolerance = 20000)\nplot(simp_states$geometry)\n\n\n\n\n\n\n\n\n\n\nVisvalingam\n\nThe Visvalingam algorithm overcomes some limitations of the Douglas-Peucker algorithm (Visvalingam and Whyatt 1993).\nit progressively removes points with the least-perceptible change.\nSimplification often allows the elimination of 95% or more points while retaining sufficient detail for visualization and often analysis\n\n\n\n\n\n\nlibrary(rmapshaper)\n\nusa10 = ms_simplify(usa, keep = .1)\nusa5  = ms_simplify(usa, keep = .05)\nusa1  = ms_simplify(usa, keep = .01)\n\n\n\n\n\n\nstates = st_transform(states, 5070)\nsimp_states   = ms_simplify(states, keep = .05)\nplot(simp_states$geometry)\n\n\n\n\n\nIn all cases, the number of points in a geometry can be calculated with mapview::npts()\n\nstates = st_transform(states, 5070)\nsimp_states_st   = st_simplify(states, dTolerance = 20000)\nsimp_states_ms   = ms_simplify(states, keep = .05)\n\nmapview::npts(states)\n#&gt; [1] 5930\nmapview::npts(simp_states_st)\n#&gt; [1] 53\nmapview::npts(simp_states_ms)\n#&gt; [1] 292\n\n\n\nTesselation\n\n\n\nSummary\n\nSo far we have spent the last two weeks looking at simple feature objects\nWhere a feature as a geometry define by a set of structured POINT(s)\nThese points have precision and define location ({X Y CRS})\nThe geometry defines a bounds: either 0D, 1D or 2D\nEach object is therefore bounded to those geometries implying a level of exactness.\n\n\n\nCore Concepts of Spatial Data: (Kuhn 2012)\n\n\n\nCore Concepts of Spatial Data: (Kuhn 2012)\n\nOne Base Concept: Location\nFour Content Concepts: Field, Object, Network, Event\nTwo Quality Concepts: Granularity, Accuracy\n\n\n\nCore Concepts of Spatial Data: (Kuhn 2012)\n\nOne Base Concept: Location (coordinates)\nFour Content Concepts: Field (raster), Object (simple feature), Network, Event\nTwo Quality Concepts: Granularity (simplification), Accuracy (taken for granted)\n\n\n\nObject View\n\nObjects describe individuals that have an identity (id) as well as spatial, temporal, and thematic properties.\nAnswers questions about properties and relations of objects.\nResults from fixing theme, controlling time, and measuring space.\nFeatures, such as surfaces, depend on objects (but are also objects)\n\n\n\nObject View\n\nObject implies boundedness\n\nboundaries may not be known or even knowable, but have limits.\n\nCrude examples of such limits are the minimal bounding boxes used for indexing and querying objects in databases.\nMany objects (particularly natural ones) do not have crisp boundaries (watersheds)\nDifferences between spatial information from multiple sources are often caused by more or less arbitrary delineations through context-dependent boundaries.\nMany questions about objects and features can be answered without boundaries, using simple point representations (centroids) with thematic attributes.\n\n\n\nField View\nFields describe phenomena that have a scalar or vector attribute everywhere in a space of interest\n\nfor example, air temperatures, rainfall, elevation, land cover\n\nField information answers the question what is here?, where here can be anywhere in the space considered.\nField-based spatial information can also represent attributes that are computed rather than measured, such as probabilities or densities.\n\nTogether, fields and objects are the two fundamental ways of structuring spatial information.\n\n\nObjects can provide coverage:\n\nBoth objects and fields can cover space continuously - the primary difference is that objects prescribe bounds.\n\nCounties are one form of object that covers the USA seamlessly\nState objects are another ‚Ä¶\n\n\n\n\nObject Coverages\n\n\n\nLANDSAT Path Row\n\nServes tiles based on a path/row index\n\n\n\n\nMODIS Sinisoial Grid\n\nServes tiles based on a path/row index\n\n\n\n\nUber Hex Addressing\n\nBreaks the world into Hexagons‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhat3word\n\nBreaks the world into 3m grids encoded with unique 3 word strings\n\n\n\n\nMap Tiles / slippy maps / Pyramids\n\nUse XYZ where Z is a zoom level ‚Ä¶\n\n\n\n\nOur Data for today ‚Ä¶\n\n\nSouthern Counties\n\nsouth_counties = aoi_get(state = \"south\", county = \"all\") |&gt; \n  st_transform(st_crs(cities))\n\nUnioned to States using dplyr\n\nsouth_states = south_counties |&gt; \n  group_by(state_name) |&gt; \n  summarise()\n\nSouth County Centroids\n\nsouth_cent = st_centroid(south_counties)\n\n\n\n\n\n\n\n\n\n\n\nTesselation plotting function\n\nplot_tess = function(data, title){\n  ggplot() + \n    geom_sf(data = data, fill = \"white\", col = \"navy\", size = .2) +   \n    theme_void() +\n    labs(title = title, caption = paste(\"This tesselation has:\", nrow(data), \"tiles\" )) +\n    theme(plot.title = element_text(hjust = .5, color =  \"navy\", face = \"bold\"))\n}\n\n\n\n\n\nplot_tess(south_counties, \"Counties\")\n\n\n\n\nRegular Tiles\n\nOne way to tile a surface is into regions of equal area\nTiles can be either square (rectilinear) or hexagonal\nst_make_grid generates a square or hexagonal grid covering the geometry of an sf or sfc object\nThe return object of st_make_grid is a new sfc object\nGrids can be specified by cellsize or number of grid cells (n) in the X and Y direction\n\n\n# Create a grid over the south with 70 rows and 50 columns\nsq_grid = st_make_grid(south_counties, n = c(70, 50)) |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())\n\n\nplot_tess(sq_grid, \"Square Coverage\")\n\n\n\n\nHexagonal Grid\n\nHexagonal tessellations (honey combs) offer an alternative to square grids\nThey are created in the same way but by setting square = FALSE\n\n\nhex_grid = st_make_grid(south_counties, n = c(70, 50), square = FALSE) |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())\n\n\n\nplot_tess(hex_grid, \"Hexegonal Coverage\")\n\n\n\n\nAdvantages Square Grids\n\nSimple definition and data storage\n\nOnly need the origin (lower left), cell size (XY) and grid dimensions\n\nEasy to aggregate and dissaggregate (resample)\nAnalogous to raster data\nRelationship between cells is given\nCombining layers is easy with traditional matrix algebra\n\n#$ Advantages Square Grids\n\nReduced Edge Effects\n\nLower perimeter to area ratio\nminimizes the amount line length needed to create a lattice of cells with a given area\n\nAll neighbors are identical\n\nNo rook vs queen neighbors\n\nBetter fit to curve surfaces (e.g.¬†the earth)\n\n\n\n\n\n\n\nTriangulations\n\nAn alternative to creating equal area tiles is to create triangulations from known anchor points\nTriangulation requires a set of input points and seeks to partition the interior into a partition of triangles.\nIn GIS contexts you‚Äôll hear:\n\nThiessen Polygon\nVoronoi Regions\nDelunay Triangulation\nTIN (Triangular irregular networks)\netc,..\n\n\n\n\nVoronoi Polygons\n\nVoronoi/Thiessen polygon boundaries define the area closest to each anchor point relative to all others\nThey are defined by the perpendicular bisectors of the lines between all points.\n\n\n\n\nVoronoi Polygons\n\n\n\nVoronoi Polygons\n\nUsefull for tasks such as:\n\nnearest neighbor search,\nfacility location (optimization),\nlargest empty areas,\npath planning‚Ä¶\n\nAlso useful for simple interpolation of values such as rain gauges,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOften used in numerical models and simulations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nst_voronoi\n\nst_voronoi creates voronoi tesselation in sf\nIt works over a MULTIPOINT collection\nShould always be simplified after creation (st_cast())\nIf to be treated as an object for analysis, should be id‚Äôd\n\n\nsouth_cent_u = st_union(south_cent)\n\nv_grid = st_voronoi(south_cent_u) |&gt; \n  st_cast() |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())\n\n\nplot_tess(v_grid, \"Voronoi Coverage\")\n\n\n\n\n\n\n\n\n\nv_grid = st_intersection(v_grid, st_union(south_states))\nplot_tess(v_grid, \"Voroni Coverage\") + \n  geom_sf(data = south_cent, col = \"darkred\", size = .2)\n\n\n\n\n\n\n\n\n\n\nDelaunay triangulation\n\nA Delaunay triangulation for a given set of points (P) in a plane, is a triangulation DT(P), where no point is inside the circumcircle of any triangle in DT(P).\n\n\n\n\nDelaunay triangulation\n\nThe Delaunay triangulation of a discrete POINT set corresponds to the dual graph of the Voronoi diagram.\nThe circumcenters (center of circles) of Delaunay triangles are the vertices of the Voronoi diagram.\n\n\n\n\nUsed in landscape evaluation and terrian modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nst_triangulate\n\nst_triangulate creates Delaunay triangulation in sf\nIt works over a MULTIPOINT collection\nShould always be simplified after creation (st_cast())\nIf to be treated as an object for analysis, should be id‚Äôd\n\n\nt_grid = st_triangulate(south_cent_u) |&gt; \n  st_cast() |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())\n\n\nplot_tess(t_grid, \"Square Coverage\")\n\n\n\n\n\n\n\n\n\nt_grid = st_intersection(t_grid, st_union(south_states))\nplot_tess(t_grid, \"Voroni Coverage\") + \n  geom_sf(data = south_cent, col = \"darkred\", size = .3)\n\n\n\n\n\n\n\n\n\n\nDifference in object coverages:\n\n\n\nTesselation Characteristics\n\n\nType\nElements\nMean Area (km2)\nStandard Deviation Area (km2)\nCoverage Area\n\n\n\n\ntriangulation\n2,828\n832\n557\n2,352,288\n\n\nvoroni\n1,421\n1,688\n1,046\n2,398,383\n\n\ncounties\n1,421\n1,688\n1,216\n2,398,383\n\n\ngrid\n3,500\n1,470\n0\n5,144,369\n\n\nHexagon\n3,789\n1,425\n0\n5,398,416\n\n\n\n\n\n\n\n\n\nModifiable areal unit problem (MAUP)\n\nThe modifiable areal unit problem (MAUP) is a source of statistical bias that can significantly impact the results of statistical hypothesis tests.\nMAUP affects results when point-based measures are aggregated into districts.\nThe resulting summary values (e.g., totals or proportions) are influenced by both the shape and scale of the aggregation unit.\n\n\n\n\nSummary\n\nThe power of GIS is the ability to integrate different layers and types of information\nThe scale of information can impact the analysis as can the grouping and zoning schemes chosen\nThe Modifiable Areal Unit Problem (MAUP) is an important issue for those who conduct spatial analysis using units of analysis at aggregations higher than incident level.\nThe MAUP occurs when the aggregate units of analysis are arbitrarily produced or not directly related to the underlying phenomena. A classic example of this problem is Gerrymandering.\nGerrymandering involves shaping and re-shaping voting districts based on the political affiliations of the resident citizenry.\n\n\n\nExamples"
  },
  {
    "objectID": "slides/week-3.html#for-example",
    "href": "slides/week-3.html#for-example",
    "title": "Week 3",
    "section": "For example ‚Ä¶",
    "text": "For example ‚Ä¶\n\nusa &lt;- st_cast(st_union(aoi_get(state = \"conus\")), \"MULTILINESTRING\")\n\nnrow(cities)\n#&gt; [1] 31254\n\n\n\n# Great Circle Distance in GCS\nsystem.time({x &lt;- st_distance(usa, cities)})\n# user      system elapsed \n# 103.560   1.390  117.128 \n\n# Euclidean Distance on PCS\nsystem.time({x &lt;- st_distance(usa, cities, which = \"Euclidean\")})\n# user    system  elapsed \n# 2.422   0.019   2.494"
  },
  {
    "objectID": "slides/week-3.html#units",
    "href": "slides/week-3.html#units",
    "title": "Week 3",
    "section": "Units",
    "text": "Units\nWhen possible measure operations report results with a units appropriate for the CRS:\n\nco &lt;- st_read(\"data/co.shp\")\n#&gt; Reading layer `co' from data source \n#&gt;   `/Users/mikejohnson/github/csu-ess-523c/slides/data/co.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 64 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -109.0602 ymin: 36.99246 xmax: -102.0415 ymax: 41.00342\n#&gt; Geodetic CRS:  WGS 84\na &lt;- st_area(co[1,])\nattributes(a) |&gt; unlist()\n#&gt; units.numerator1 units.numerator2            class \n#&gt;              \"m\"              \"m\"          \"units\"\n\nThe units package can be used to convert between units:\n\nunits::set_units(a, km^2) # result in square kilometers\n#&gt; 3062.85 [km^2]\nunits::set_units(a, ha) # result in hectares\n#&gt; 306285 [ha]\n\nand the results can be stripped of their attributes for cases where numeric values are needed (e.g.¬†math operators and ggplot):\n\nas.numeric(a)\n#&gt; [1] 3062849758"
  },
  {
    "objectID": "slides/week-3.html#spatial-filtering",
    "href": "slides/week-3.html#spatial-filtering",
    "title": "Week 3",
    "section": "Spatial Filtering",
    "text": "Spatial Filtering\n\nWe can filter spatially, use st_filter as the function call\nHere the boolean condition is not passed (e.g.¬†name == WA)\nBut instead, is defined by a spatial predicate\nThe default is st_intersects but can be changed with the .predicate argument:\n\n\n\n\nmutate(states, \n       touch = st_touches(states, wa, sparse = FALSE)) \n#&gt; Simple feature collection with 4 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 41.98818 xmax: -104.0397 ymax: 49.00244\n#&gt; Geodetic CRS:  WGS 84\n#&gt;         name                       geometry touch\n#&gt; 1      Idaho MULTIPOLYGON (((-111.0455 4...  TRUE\n#&gt; 2    Montana MULTIPOLYGON (((-109.7985 4... FALSE\n#&gt; 3     Oregon MULTIPOLYGON (((-117.22 44....  TRUE\n#&gt; 4 Washington MULTIPOLYGON (((-121.5237 4... FALSE\n\n\n\nst_filter(states, wa, .predicate = st_touches) \n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.7035 ymin: 41.98818 xmax: -111.0435 ymax: 49.00085\n#&gt; Geodetic CRS:  WGS 84\n#&gt;     name                       geometry\n#&gt; 1  Idaho MULTIPOLYGON (((-111.0455 4...\n#&gt; 2 Oregon MULTIPOLYGON (((-117.22 44...."
  },
  {
    "objectID": "slides/week-3.html#interior-boundary-and-exterior-polygon",
    "href": "slides/week-3.html#interior-boundary-and-exterior-polygon",
    "title": "Week 3",
    "section": "Interior, Boundary and Exterior: POLYGON",
    "text": "Interior, Boundary and Exterior: POLYGON"
  },
  {
    "objectID": "slides/week-3.html#summary",
    "href": "slides/week-3.html#summary",
    "title": "Week 3",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/week-3.html#dimensionally-extended-9-intersection-model",
    "href": "slides/week-3.html#dimensionally-extended-9-intersection-model",
    "title": "Week 3",
    "section": "Dimensionally Extended 9-Intersection Model",
    "text": "Dimensionally Extended 9-Intersection Model\n\nThe DE-9IM is a topological model and (standard) used to describe the spatial relations of two geometries\nUsed in geometry, point-set topology, geospatial topology\nThe DE-9IM matrix provides a way to classify geometry relations using the set {0,1,2,F} or {T,F}\nWith a {T,F} matrix domain, there are 512 possible relations that can be grouped into binary classification schemes.\nAbout 10 of these, have been given a common name such as intersects, touches, and within.\nWhen testing two geometries against a scheme, the result is a spatial predicate named by the scheme.\nProvides the primary basis for queries and assertions in GIS and spatial databases (PostGIS)."
  },
  {
    "objectID": "slides/week-3.html#st_realtes-vs.-predicate-calls",
    "href": "slides/week-3.html#st_realtes-vs.-predicate-calls",
    "title": "Week 3",
    "section": "st_realtes vs.¬†predicate calls‚Ä¶",
    "text": "st_realtes vs.¬†predicate calls‚Ä¶\n\nstates = filter(aoi_get(state = \"all\"), state_abbr %in% c(\"WA\", \"OR\", \"MT\", \"ID\")) |&gt;\n  select(name)\n\nwa = filter(states, name == \"Washington\")\n\n\n\n\nplot(states$geometry)\n\n\n\n\n\n\n\n\n\n\n(mutate(states, \n        deim9 = st_relate(states, wa),\n        touch = st_touches(states, wa, sparse = F)))\n#&gt; Simple feature collection with 4 features and 3 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 41.98818 xmax: -104.0397 ymax: 49.00244\n#&gt; Geodetic CRS:  WGS 84\n#&gt;         name                       geometry     deim9 touch\n#&gt; 1      Idaho MULTIPOLYGON (((-111.0455 4... FF2F11212  TRUE\n#&gt; 2    Montana MULTIPOLYGON (((-109.7985 4... FF2FF1212 FALSE\n#&gt; 3     Oregon MULTIPOLYGON (((-117.22 44.... FF2F11212  TRUE\n#&gt; 4 Washington MULTIPOLYGON (((-121.5237 4... 2FFF1FFF2 FALSE"
  },
  {
    "objectID": "slides/week-3.html#binary-predicates",
    "href": "slides/week-3.html#binary-predicates",
    "title": "Week 3",
    "section": "Binary Predicates",
    "text": "Binary Predicates\n\nCollectively, predicates define the type of relationship each 2D object has with another.\nOf the ~ 512 unique relationships offered by the DE-9IM models a selection of ~ 10 have been named.\nThese are include in PostGIS/GEOS and are made accessible via R sf"
  },
  {
    "objectID": "slides/week-3.html#filtering-on-data.frames",
    "href": "slides/week-3.html#filtering-on-data.frames",
    "title": "Week 3",
    "section": "Filtering on data.frames",
    "text": "Filtering on data.frames\n\nWe have used dplyr::filter to subset a data frame, retaining all rows that satisfy a boolean condition.\n\n\n\n\nmutate(states, equalsWA = (name == \"Washington\")) |&gt; \n  st_drop_geometry()\n#&gt;         name equalsWA\n#&gt; 1      Idaho    FALSE\n#&gt; 2    Montana    FALSE\n#&gt; 3     Oregon    FALSE\n#&gt; 4 Washington     TRUE\n\n\n\nfilter(states, name == \"Washington\")\n#&gt; Simple feature collection with 1 feature and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 45.54364 xmax: -116.9161 ymax: 49.00244\n#&gt; Geodetic CRS:  WGS 84\n#&gt;         name                       geometry\n#&gt; 1 Washington MULTIPOLYGON (((-121.5237 4..."
  },
  {
    "objectID": "slides/week-3.html#spatial-relations-in-r",
    "href": "slides/week-3.html#spatial-relations-in-r",
    "title": "Week 3",
    "section": "Spatial Relations in R",
    "text": "Spatial Relations in R\n\nGeometry X is a 3 feature polygon colored in red\nGeometry Y is a 4 feature polygon colored in blue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nst_relate(x,y)\n#&gt;      [,1]        [,2]        [,3]        [,4]       \n#&gt; [1,] \"212FF1FF2\" \"FF2FF1212\" \"212101212\" \"FF2FF1212\"\n#&gt; [2,] \"FF2FF1212\" \"212101212\" \"212101212\" \"FF2FF1212\"\n#&gt; [3,] \"FF2FF1212\" \"FF2FF1212\" \"212101212\" \"FF2FF1212\"\nst_relate(x,x)\n#&gt;      [,1]        [,2]        [,3]       \n#&gt; [1,] \"2FFF1FFF2\" \"FF2F01212\" \"FF2F11212\"\n#&gt; [2,] \"FF2F01212\" \"2FFF1FFF2\" \"FF2FF1212\"\n#&gt; [3,] \"FF2F11212\" \"FF2FF1212\" \"2FFF1FFF2\"\nst_relate(y,y)\n#&gt;      [,1]        [,2]        [,3]        [,4]       \n#&gt; [1,] \"2FFF1FFF2\" \"FF2FF1212\" \"212101212\" \"FF2FF1212\"\n#&gt; [2,] \"FF2FF1212\" \"2FFF1FFF2\" \"212101212\" \"FF2FF1212\"\n#&gt; [3,] \"212101212\" \"212101212\" \"2FFF1FFF2\" \"FF2FF1212\"\n#&gt; [4,] \"FF2FF1212\" \"FF2FF1212\" \"FF2FF1212\" \"2FFF1FFF2\""
  },
  {
    "objectID": "slides/week-3.html#predicates-in-r",
    "href": "slides/week-3.html#predicates-in-r",
    "title": "Week 3",
    "section": "Predicates in R",
    "text": "Predicates in R\nReturns either a sparse matrix\n\nst_intersects(x,y)\n#&gt; Sparse geometry binary predicate list of length 3, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1, 3\n#&gt;  2: 2, 3\n#&gt;  3: 3\n\nor a dense matrix\n\nst_intersects(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2] [,3]  [,4]\n#&gt; [1,]  TRUE FALSE TRUE FALSE\n#&gt; [2,] FALSE  TRUE TRUE FALSE\n#&gt; [3,] FALSE FALSE TRUE FALSE\n\nst_disjoint(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3] [,4]\n#&gt; [1,] FALSE  TRUE FALSE TRUE\n#&gt; [2,]  TRUE FALSE FALSE TRUE\n#&gt; [3,]  TRUE  TRUE FALSE TRUE\n\nst_touches(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE\n\nst_within(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "slides/week-3.html#named-predicates",
    "href": "slides/week-3.html#named-predicates",
    "title": "Week 3",
    "section": "Named Predicates",
    "text": "Named Predicates\n\n‚Äúnamed spatial predicates‚Äù have been defined for some common relations.\nA few functions can be derived (expressed by masks) from DE-9IM include (* = wildcard):\n\n\n\n\n\n\n\n\n\nPredicate\nDE-9IM String Code\nDescription\n\n\n\n\nIntersects\nT*F**FFF*\n‚ÄúTwo geometries intersect if they share any portion of space‚Äù\n\n\nOverlaps\nT*F**FFF*\n‚ÄúTwo geometries overlap if they share some but not all of the same space‚Äù\n\n\nEquals\nT*F**FFF*\n‚ÄúTwo geometries are topologically equal if their interiors intersect and no part of the interior or boundary of one geometry intersects the exterior of the other‚Äù\n\n\nDisjoint\nFF*FF*****\n‚ÄúTwo geometries are disjoint: they have no point in common. They form a set of disconnected geometries.‚Äù\n\n\nTouches\nFT*******\nF**T*****\n\n\nContains\nT*****FF**\n‚ÄúA contains B: geometry B lies in A, and the interiors intersect‚Äù\n\n\nCovers\nT*****FF*\n*T****FF*\n\n\nWithin\n*T*****FF*\n**T****FF*\n\n\nCovered by\n*T*****FF*\n**T****FF*"
  },
  {
    "objectID": "slides/week-3.html#named-predicates-in-r",
    "href": "slides/week-3.html#named-predicates-in-r",
    "title": "Week 3",
    "section": "Named predicates in R",
    "text": "Named predicates in R\n\nsf provides a set of functions that implement the DE-9IM model and named predicates. Each of these can return either a sparse or dense matrix.\n\nsparse matrix\n\nst_intersects(x,y)\n#&gt; Sparse geometry binary predicate list of length 3, where the predicate\n#&gt; was `intersects'\n#&gt;  1: 1, 3\n#&gt;  2: 2, 3\n#&gt;  3: 3\n\ndense matrix\n\nst_intersects(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2] [,3]  [,4]\n#&gt; [1,]  TRUE FALSE TRUE FALSE\n#&gt; [2,] FALSE  TRUE TRUE FALSE\n#&gt; [3,] FALSE FALSE TRUE FALSE\n\nst_disjoint(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3] [,4]\n#&gt; [1,] FALSE  TRUE FALSE TRUE\n#&gt; [2,]  TRUE FALSE FALSE TRUE\n#&gt; [3,]  TRUE  TRUE FALSE TRUE\n\nst_touches(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE\n\nst_within(x, y, sparse = FALSE)\n#&gt;       [,1]  [,2]  [,3]  [,4]\n#&gt; [1,] FALSE FALSE FALSE FALSE\n#&gt; [2,] FALSE FALSE FALSE FALSE\n#&gt; [3,] FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "slides/week-3.html#clipping",
    "href": "slides/week-3.html#clipping",
    "title": "Week 3",
    "section": "Clipping",
    "text": "Clipping\n\nClipping is a form of subsetting that involves changing the geometry of at least some features.\nClipping can only apply to features more complex than points: (lines, polygons and their ‚Äòmulti‚Äô equivalents)."
  },
  {
    "objectID": "slides/week-3.html#spatial-subsetting",
    "href": "slides/week-3.html#spatial-subsetting",
    "title": "Week 3",
    "section": "Spatial Subsetting",
    "text": "Spatial Subsetting\n\nBy default the data.frame subsetting methods we‚Äôve seen (e.g [,]) implements st_intersection\n\n\nwa = st_transform(wa, 5070)\nwa_starbucks = starbucks[wa,] #&lt;&lt;\n\nggplot() + geom_sf(data = wa) + geom_sf(data = wa_starbucks) + theme_void()"
  },
  {
    "objectID": "slides/week-3.html#rmapshaper",
    "href": "slides/week-3.html#rmapshaper",
    "title": "Week 3",
    "section": "rmapshaper",
    "text": "rmapshaper\nIn R, the rmapshaper package implements the Visvalingam algorithm in the ms_simplify function.\n\nThe ms_simplify function is a wrapper around the mapshaper JavaScript library (created by lead viz experts at the NYT)\n\n\nlibrary(rmapshaper)\n\nusa10 = ms_simplify(usa, keep = .1)\nusa5  = ms_simplify(usa, keep = .05)\nusa1  = ms_simplify(usa, keep = .01)"
  },
  {
    "objectID": "slides/week-3.html#section-6",
    "href": "slides/week-3.html#section-6",
    "title": "Week 3",
    "section": "",
    "text": "states = st_transform(states, 5070)\nsimp_states   = ms_simplify(states, keep = .05)\nplot(simp_states$geometry)"
  },
  {
    "objectID": "slides/week-3.html#section-7",
    "href": "slides/week-3.html#section-7",
    "title": "Week 3",
    "section": "",
    "text": "In all cases, the number of points in a geometry can be calculated with mapview::npts()\n\nstates = st_transform(states, 5070)\nsimp_states_st   = st_simplify(states, dTolerance = 20000)\nsimp_states_ms   = ms_simplify(states, keep = .05)\n\nmapview::npts(states)\n#&gt; [1] 5930\nmapview::npts(simp_states_st)\n#&gt; [1] 53\nmapview::npts(simp_states_ms)\n#&gt; [1] 292"
  },
  {
    "objectID": "slides/week-3.html#section-8",
    "href": "slides/week-3.html#section-8",
    "title": "Week 3",
    "section": "",
    "text": "plot_tess(south_counties, \"Counties\")"
  },
  {
    "objectID": "slides/week-3.html#our-data-for-today",
    "href": "slides/week-3.html#our-data-for-today",
    "title": "Week 3",
    "section": "Our Data for today ‚Ä¶",
    "text": "Our Data for today ‚Ä¶\nSouthern Counties\n\nsouth_counties = aoi_get(state = \"south\", county = \"all\") |&gt; \n  st_transform(st_crs(cities))\n\n\nUnioned to States using dplyr\n\nsouth_states = south_counties |&gt; \n  group_by(state_name) |&gt; \n  summarise()\n\n\nSouth County Centroids\n\nsouth_cent = st_centroid(south_counties)"
  },
  {
    "objectID": "slides/week-3.html#advantages-square-grids-1",
    "href": "slides/week-3.html#advantages-square-grids-1",
    "title": "Week 3",
    "section": "Advantages Square Grids",
    "text": "Advantages Square Grids\n\nReduced Edge Effects\n\nLower perimeter to area ratio\nminimizes the amount line length needed to create a lattice of cells with a given area\n\nAll neighbors are identical\n\nNo rook vs queen neighbors\n\nBetter fit to curve surfaces (e.g.¬†the earth)"
  },
  {
    "objectID": "slides/week-3.html#difference-in-object-coverages",
    "href": "slides/week-3.html#difference-in-object-coverages",
    "title": "Week 3",
    "section": "Difference in object coverages:",
    "text": "Difference in object coverages:\n\n\n\nTesselation Characteristics\n\n\nType\nElements\nMean Area (km2)\nStandard Deviation Area (km2)\nCoverage Area\n\n\n\n\ntriangulation\n2,828\n832\n557\n2,352,288\n\n\nvoroni\n1,421\n1,688\n1,046\n2,398,383\n\n\ncounties\n1,421\n1,688\n1,216\n2,398,383\n\n\ngrid\n3,500\n1,470\n0\n5,144,369\n\n\nHexagon\n3,789\n1,425\n0\n5,398,416"
  },
  {
    "objectID": "slides/week-3.html#modifiable-areal-unit-problem-maup",
    "href": "slides/week-3.html#modifiable-areal-unit-problem-maup",
    "title": "Week 3",
    "section": "Modifiable areal unit problem (MAUP)",
    "text": "Modifiable areal unit problem (MAUP)\n\nThe modifiable areal unit problem (MAUP) is a source of statistical bias that can significantly impact the results of statistical hypothesis tests.\nMAUP affects results when point-based measures are aggregated into districts.\nThe resulting summary values (e.g., totals or proportions) are influenced by both the shape and scale of the aggregation unit."
  },
  {
    "objectID": "slides/week-3.html#regular-tiles",
    "href": "slides/week-3.html#regular-tiles",
    "title": "Week 3",
    "section": "Regular Tiles",
    "text": "Regular Tiles\n\nOne way to tile a surface is into regions of equal area\nTiles can be either square (rectilinear) or hexagonal\nst_make_grid generates a square or hexagonal grid covering the geometry of an sf or sfc object\nThe return object of st_make_grid is a new sfc object\nGrids can be specified by cellsize or number of grid cells (n) in the X and Y direction\n\n\n# Create a grid over the south with 70 rows and 50 columns\nsq_grid = st_make_grid(south_counties, n = c(70, 50)) |&gt; \n  st_as_sf() |&gt; \n  mutate(id = 1:n())"
  },
  {
    "objectID": "slides/week-3.html#map-tiles-slippy-maps-pyramids",
    "href": "slides/week-3.html#map-tiles-slippy-maps-pyramids",
    "title": "Week 3",
    "section": "Map Tiles / slippy maps / Pyramids",
    "text": "Map Tiles / slippy maps / Pyramids\n\nUse XYZ where Z is a zoom level ‚Ä¶"
  },
  {
    "objectID": "slides/week-3.html#southern-coverage-county",
    "href": "slides/week-3.html#southern-coverage-county",
    "title": "Week 3",
    "section": "Southern Coverage: County",
    "text": "Southern Coverage: County"
  },
  {
    "objectID": "slides/week-3.html#southern-coverage-square",
    "href": "slides/week-3.html#southern-coverage-square",
    "title": "Week 3",
    "section": "Southern Coverage: Square",
    "text": "Southern Coverage: Square"
  },
  {
    "objectID": "slides/week-3.html#southern-coverage-hexagon",
    "href": "slides/week-3.html#southern-coverage-hexagon",
    "title": "Week 3",
    "section": "Southern Coverage: Hexagon",
    "text": "Southern Coverage: Hexagon"
  },
  {
    "objectID": "slides/week-3.html#southern-coverage-voronoi",
    "href": "slides/week-3.html#southern-coverage-voronoi",
    "title": "Week 3",
    "section": "Southern Coverage: Voronoi",
    "text": "Southern Coverage: Voronoi"
  },
  {
    "objectID": "slides/week-3.html#southern-coverage-triangles",
    "href": "slides/week-3.html#southern-coverage-triangles",
    "title": "Week 3",
    "section": "Southern Coverage: Triangles",
    "text": "Southern Coverage: Triangles"
  },
  {
    "objectID": "slides/week-3.html#wrap-up",
    "href": "slides/week-3.html#wrap-up",
    "title": "Week 3",
    "section": "Wrap up",
    "text": "Wrap up\nToday we covered the following topics: - Spatial predicates and binary predicates - Spatial filtering and joining - Spatial simplification - Spatial tessellations / Coverages - Modifiable areal unit problem (MAUP)\nCombined with your understanding of\n\nGeometry and topology\nCoordinate Reference Systems\nTheir integration with R\n\nWe are well suited to move on to raster data (gridded coverage model!) next week\nCongrats!!"
  },
  {
    "objectID": "slides/week-3.html#southern-coverage-voronoi-1",
    "href": "slides/week-3.html#southern-coverage-voronoi-1",
    "title": "Week 3",
    "section": "Southern Coverage: Voronoi",
    "text": "Southern Coverage: Voronoi"
  },
  {
    "objectID": "slides/week-3.html#southern-coverage-triangles-1",
    "href": "slides/week-3.html#southern-coverage-triangles-1",
    "title": "Week 3",
    "section": "Southern Coverage: Triangles",
    "text": "Southern Coverage: Triangles\n\nt_grid = st_intersection(t_grid, st_union(south_states))"
  },
  {
    "objectID": "slides/week-3.html#st_relates-vs.-predicate-calls",
    "href": "slides/week-3.html#st_relates-vs.-predicate-calls",
    "title": "Week 3",
    "section": "st_relates vs.¬†predicate calls‚Ä¶",
    "text": "st_relates vs.¬†predicate calls‚Ä¶\n\nstates = filter(aoi_get(state = \"all\"), state_abbr %in% c(\"WA\", \"OR\", \"MT\", \"ID\")) |&gt;\n  select(name)\n\nwa = filter(states, name == \"Washington\")\n\n\n\n\nplot(states$geometry)\n\n\n\n\n\n\n\n\n\n\n(mutate(states, \n        deim9 = st_relate(states, wa),\n        touch = st_touches(states, wa, sparse = F)))\n#&gt; Simple feature collection with 4 features and 3 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.8485 ymin: 41.98818 xmax: -104.0397 ymax: 49.00244\n#&gt; Geodetic CRS:  WGS 84\n#&gt;         name                       geometry     deim9 touch\n#&gt; 1      Idaho MULTIPOLYGON (((-111.0455 4... FF2F11212  TRUE\n#&gt; 2    Montana MULTIPOLYGON (((-109.7985 4... FF2FF1212 FALSE\n#&gt; 3     Oregon MULTIPOLYGON (((-117.22 44.... FF2F11212  TRUE\n#&gt; 4 Washington MULTIPOLYGON (((-121.5237 4... 2FFF1FFF2 FALSE"
  },
  {
    "objectID": "labs/Untitled.html",
    "href": "labs/Untitled.html",
    "title": "Lab 2: Distances and Projections",
    "section": "",
    "text": "Question 1:\n\nMaking Spatial Objects & Coordinate Transformation\nSpatial objects (sf) can be built from a vector of X and Y values in addition to a coordinate reference system (CRS). For example:\n\n\n\nQuestion 2:\n\nst_distance review\nThere are two notable things about this result:\n\nIt has units\nIt is returned as a matrix, even though foco only had one row\n\nThis second point highlights a useful feature of st_distance, namley, its ability to return distance matrices between all combinations of features in x and y.\n\n\nunits review\nWhile units are useful, they are not always the preferred units. By default, the units measurement is defined by the projection. For example:\nUnits can be converted using units::set_units. For example, ‚Äòm‚Äô can be converted to ‚Äòkm‚Äô:\nYou might have noticed the data type of the st_distance objects are an S3 class of units. Sometimes, this class can cause problems when trying to using it with other classes or methods:\nIn these cases, the units class can be dropped with units::drop_units\nAs with all functions, these steps can be nested:\n\n\n\nGeometry review\nThere are a few ways to manipulate existing geometries, here we discuss st_union, st_combine and st_cast\n\nst_combine returns a single, combined geometry, with no resolved boundaries.\nst_union() returns a single geometry with resolved boundaries\nst_cast() casts one geometry type to another\n\n\n\n\nQuestion 3:\nIn this section you will extend your growing ggplot skills to handle spatial data using ggrepl to label significant features; gghighlight to emphasize important criteria; and scaled color/fill to create chloropleth represnetations of variables. Below is some example code to provide an example of these tools in action:\n\nGet some data (review)\n\n\nMap"
  },
  {
    "objectID": "slides/week-4.html#terra",
    "href": "slides/week-4.html#terra",
    "title": "Week 4",
    "section": "terra",
    "text": "terra\n\nLike sf, terra is an implementation of standard raster data model\nThe model is used by all GIS platforms\nRepresented continuous data either as continuous or categorical values as a regular set of cells in a grid (matrix)\ncells have: (1) resolution, (2) infered cell coordinate (centroid) (3) the coordinate and value apply to the entire cell area"
  },
  {
    "objectID": "slides/week-4.html#recap-r-data-structures",
    "href": "slides/week-4.html#recap-r-data-structures",
    "title": "Week 4",
    "section": "Recap: R data structures",
    "text": "Recap: R data structures\n\nVector:\n\nA vector can have dimensions\n\nA 1D vector in a collection of values\nA 2D vector is a matrix\nA 3D vector is an array\n\n\nList: a collection of objects\ndata.frame: a list with requirement of equal length column (vectors)\ndata.frames and lists (sfc) defined our vector model\nArrays will define our raster model"
  },
  {
    "objectID": "slides/week-4.html#spatial-extent",
    "href": "slides/week-4.html#spatial-extent",
    "title": "Week 4",
    "section": "Spatial Extent",
    "text": "Spatial Extent\nOne last topic with respect to vector data (that will carry us into raster) is the idea of an extent:\n\n(ny &lt;- AOI::aoi_get(state = \"NY\") |&gt; \n   st_transform(5070) |&gt; \n   dplyr::select(name))\n#&gt; Simple feature collection with 1 feature and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1319502 ymin: 2149150 xmax: 1997508 ymax: 2658543\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt;       name                       geometry\n#&gt; 1 New York MULTIPOLYGON (((1661335 263...\n\nIn geometry, the minimum bounding box for a point set (stored as POINT, POLYLINE, POLYGON) in N dimensions is ‚Äú‚Ä¶the box with the smallest measure within which all the points lie.‚Äù"
  },
  {
    "objectID": "slides/week-4.html#we-can-extract-bounding-box-coordinates-with-st_bbox",
    "href": "slides/week-4.html#we-can-extract-bounding-box-coordinates-with-st_bbox",
    "title": "Week 4",
    "section": "We can extract bounding box coordinates with st_bbox",
    "text": "We can extract bounding box coordinates with st_bbox\n\nreturns: an object of class bbox of length 4.\n\n\n(bb = st_bbox(ny))\n#&gt;    xmin    ymin    xmax    ymax \n#&gt; 1319502 2149150 1997508 2658543\n\nclass(bb)\n#&gt; [1] \"bbox\"\n\ntypeof(bb)\n#&gt; [1] \"double\""
  },
  {
    "objectID": "slides/week-4.html#there-is-a-method-for-creating-an-sfc-form-a-bbox-object",
    "href": "slides/week-4.html#there-is-a-method-for-creating-an-sfc-form-a-bbox-object",
    "title": "Week 4",
    "section": "There is a method for creating an sfc form a bbox object",
    "text": "There is a method for creating an sfc form a bbox object\n\n(bb = st_as_sfc(bb))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1319502 ymin: 2149150 xmax: 1997508 ymax: 2658543\n#&gt; Projected CRS: NAD83 / Conus Albers\n\nclass(bb)\n#&gt; [1] \"sfc_POLYGON\" \"sfc\"\n\ntypeof(bb)\n#&gt; [1] \"list\""
  },
  {
    "objectID": "slides/week-4.html#result",
    "href": "slides/week-4.html#result",
    "title": "Week 4",
    "section": "Result:",
    "text": "Result:\n\nplot(bb, border = rgb(0,0,1))\nplot(ny, add = TRUE, col = rgb(1,0,0, .5))"
  },
  {
    "objectID": "slides/week-4.html#extents-can-be-discritized-in-a-number-of-ways",
    "href": "slides/week-4.html#extents-can-be-discritized-in-a-number-of-ways",
    "title": "Week 4",
    "section": "Extents can be discritized in a number of ways:",
    "text": "Extents can be discritized in a number of ways:\n\n\n\ngrid = st_make_grid(bb)\nplot(ny$geometry)\nplot(grid, add = TRUE)\n\n\n\n\n\n\n\n\n\n\ngrid1km = st_make_grid(bb, cellsize = 10000)\nplot(ny$geometry)\nplot(grid1km, add = TRUE)"
  },
  {
    "objectID": "slides/week-4.html#section",
    "href": "slides/week-4.html#section",
    "title": "Week 4",
    "section": "",
    "text": "The secondary colors in an RGB color wheel are cyan, magenta, and yellow because these are the three subtractive colors\nThink of your printer and the CMYK ink cartridges! Where black, is the absence of color (0,0,0)\n\npar(mfrow = c(1,4), mar = c(0,0,0,0))\nplot(ny$geometry, col = rgb(0,1,1))  # cyan\nplot(ny$geometry, col = rgb(1,0,1)) # Magenta\nplot(ny$geometry, col = rgb(1,1,0)) # Yellow\nplot(ny$geometry, col = rgb(0,0,0)) # Key (black)"
  },
  {
    "objectID": "slides/week-4.html#section-1",
    "href": "slides/week-4.html#section-1",
    "title": "Week 4",
    "section": "",
    "text": "So, any image (.png, .tif, .gif) can be read as a raster‚Ä¶\nThe raster is defined by the extent and resolution of the cells\nTo be spatial, the extent (thus coordinates) must be grounded in an CRS\n\n\n\n\n(img = terra::rast('images/17-raster-extent.png'))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 788, 1067, 4  (nrow, ncol, nlyr)\n#&gt; resolution  : 1, 1  (x, y)\n#&gt; extent      : 0, 1067, 0, 788  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. :  \n#&gt; source      : 17-raster-extent.png \n#&gt; names       : 17-rast~xtent_1, 17-rast~xtent_2, 17-rast~xtent_3, 17-rast~xtent_4\n\n\n\nterra::plotRGB(img, r = 1, g = 2, b = 3)"
  },
  {
    "objectID": "slides/week-4.html#section-2",
    "href": "slides/week-4.html#section-2",
    "title": "Week 4",
    "section": "",
    "text": "img[1,1,1:3]\n#&gt; [1] 1.0000000 1.0000000 0.9333333\nrgb(1.0000000, 0.9607843, 0.8980392)\n#&gt; [1] \"#FFF5E5\"\n\n\nGoogle Color Picker"
  },
  {
    "objectID": "slides/week-4.html#section-3",
    "href": "slides/week-4.html#section-3",
    "title": "Week 4",
    "section": "",
    "text": "img[1,1,1:3]\n#&gt; [1] 1.0000000 1.0000000 0.9333333\nrgb(1.0000000, 0.9607843, 0.8980392)\n#&gt; [1] \"#FFF5E5\"\n\n\nGoogle Color Picker"
  },
  {
    "objectID": "slides/week-4.html#photos-and-computers",
    "href": "slides/week-4.html#photos-and-computers",
    "title": "Week 4",
    "section": "Photos and Computers ‚Ä¶",
    "text": "Photos and Computers ‚Ä¶"
  },
  {
    "objectID": "slides/week-4.html#aerial-imagery-really-just-a-photo",
    "href": "slides/week-4.html#aerial-imagery-really-just-a-photo",
    "title": "Week 4",
    "section": "Aerial Imagery (really just a photo üòÑ)",
    "text": "Aerial Imagery (really just a photo üòÑ)"
  },
  {
    "objectID": "slides/week-4.html#what-is-stored-in-these-cells",
    "href": "slides/week-4.html#what-is-stored-in-these-cells",
    "title": "Week 4",
    "section": "What is stored in these cells?",
    "text": "What is stored in these cells?\n\n\nCategorical Values (integer/factor)\n\n\n\n\n\n\n\n\n\n\nContinuous Values (numeric)"
  },
  {
    "objectID": "slides/week-4.html#spectral-values",
    "href": "slides/week-4.html#spectral-values",
    "title": "Week 4",
    "section": "Spectral Values",
    "text": "Spectral Values\n\nEither Color, or sensor"
  },
  {
    "objectID": "slides/week-4.html#section-4",
    "href": "slides/week-4.html#section-4",
    "title": "Week 4",
    "section": "",
    "text": "plot(fc_elev)\nplot(poly, col = \"blue\", add  = TRUE)\n\n\n\n\n\n\n\n\n\n\nma =  mask(fc_elev, poly)\nma2 = mask(fc_elev, poly, inverse = TRUE)\nplot(c(fc_elev, ma, ma2))"
  },
  {
    "objectID": "slides/week-4.html#rgb-and-bytesbits",
    "href": "slides/week-4.html#rgb-and-bytesbits",
    "title": "Week 4",
    "section": "RGB and bytes/bits",
    "text": "RGB and bytes/bits\n\nThe red, green and blue use 8 bits each (1 byte), which each have integer values from 0 to 255.\nThis makes 256^3 = 16,777,216 possible colors.\nSee more here\nHow does this relate to the selection of 256x256 pixel tiles in web maps?"
  },
  {
    "objectID": "slides/week-4.html#section-5",
    "href": "slides/week-4.html#section-5",
    "title": "Week 4",
    "section": "",
    "text": "base_mask = mask_r * fc_elev\nplot(base_mask)"
  },
  {
    "objectID": "slides/week-4.html#resolution-drives-image-clarity-granulairty",
    "href": "slides/week-4.html#resolution-drives-image-clarity-granulairty",
    "title": "Week 4",
    "section": "Resolution drives image clarity (granulairty)",
    "text": "Resolution drives image clarity (granulairty)\n\nHigher resolution (smaller cells) = more detail, but bigger data!"
  },
  {
    "objectID": "slides/week-4.html#raster-images-seek-to-discritize-the-real-world-into-cell-based-values",
    "href": "slides/week-4.html#raster-images-seek-to-discritize-the-real-world-into-cell-based-values",
    "title": "Week 4",
    "section": "Raster images seek to discritize the real world into cell-based values",
    "text": "Raster images seek to discritize the real world into cell-based values\n\nAgain either integer (categorical), continuous, or signal"
  },
  {
    "objectID": "slides/week-4.html#all-rasters-have-an-extent",
    "href": "slides/week-4.html#all-rasters-have-an-extent",
    "title": "Week 4",
    "section": "All rasters have an extent!",
    "text": "All rasters have an extent!\n\nThis is the same extent as a bounding box\nCan be described as 4 values (xmin,ymin,xmax,ymax)"
  },
  {
    "objectID": "slides/week-4.html#section-6",
    "href": "slides/week-4.html#section-6",
    "title": "Week 4",
    "section": "",
    "text": "(s = c(r, m, elev_cut, rc) |&gt; \n  setNames(c(\"elevation\", \"elev-mask\", \"terrain\", \"topography\")))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 4  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; sources     : foco-elev.tif  \n#&gt;               memory  \n#&gt;               memory  \n#&gt;               memory  \n#&gt; names       : elevation, elev-mask, terrain, topography \n#&gt; min values  :      1382,         1,    1521,          2 \n#&gt; max values  :      4346,         1,    4346,       4346"
  },
  {
    "objectID": "slides/week-4.html#raster-data-in-r",
    "href": "slides/week-4.html#raster-data-in-r",
    "title": "Week 4",
    "section": "Raster Data in R",
    "text": "Raster Data in R\nA SpatRast represents single-layer (variable) raster data.\nA SpatRast always stores the fundamental parameters that describe it. - The number of columns and rows, - The spatial extent - The Coordinate Reference System.\nIn addition, a SpatRast can store information about the file where raster values are stored (if there is such a file).\nHere we construct an empty raster:\n\n(r &lt;- rast(ncol=20, nrow=20, xmax=-80, xmin=-120, ymin=20, ymax=60))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 20, 20, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 2, 2  (x, y)\n#&gt; extent      : -120, -80, 20, 60  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84)\n\nExtract Diminisionality ‚Ä¶\n\nnrow(r)\n#&gt; [1] 20\nncol(r)\n#&gt; [1] 20\nncell(r)\n#&gt; [1] 400\nnlyr(r)\n#&gt; [1] 1"
  },
  {
    "objectID": "slides/week-4.html#raster-values",
    "href": "slides/week-4.html#raster-values",
    "title": "Week 4",
    "section": "Raster Values",
    "text": "Raster Values\nvalues can be extracted with values()\n\nhead(values(r))\n#&gt;      lyr.1\n#&gt; [1,]   NaN\n#&gt; [2,]   NaN\n#&gt; [3,]   NaN\n#&gt; [4,]   NaN\n#&gt; [5,]   NaN\n#&gt; [6,]   NaN"
  },
  {
    "objectID": "slides/week-4.html#assigning-a-values-as-a-vector",
    "href": "slides/week-4.html#assigning-a-values-as-a-vector",
    "title": "Week 4",
    "section": "Assigning a values as a vector",
    "text": "Assigning a values as a vector\n\nvalues(r) &lt;- 1:ncell(r)\n\nr\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 20, 20, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 2, 2  (x, y)\n#&gt; extent      : -120, -80, 20, 60  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n#&gt; source(s)   : memory\n#&gt; name        : lyr.1 \n#&gt; min value   :     1 \n#&gt; max value   :   400\n\nhead(values(r))\n#&gt;      lyr.1\n#&gt; [1,]     1\n#&gt; [2,]     2\n#&gt; [3,]     3\n#&gt; [4,]     4\n#&gt; [5,]     5\n#&gt; [6,]     6\n\n\nplot(r)"
  },
  {
    "objectID": "slides/week-4.html#raster-is-s4",
    "href": "slides/week-4.html#raster-is-s4",
    "title": "Week 4",
    "section": "Raster is S4",
    "text": "Raster is S4\n\nCompared to S3, the S4 object system is much stricter, and much closer to other OO systems.\nWhat does this mean for us?\nData is structured with a ‚Äúrepresentation‚Äù\nwhich is a list of slot (or attributes), giving their names and classes, accessed with @\n\n\nstr(r, max.level = 2)\n#&gt; S4 class 'SpatRaster' [package \"terra\"]\nr@pntr\n#&gt; C++ object &lt;0x11d191f10&gt; of class 'SpatRaster' &lt;0x10d012710&gt;\nr@pntr$ncol\n#&gt; Class method definition for method ncol()\n#&gt; function () \n#&gt; {\n#&gt;     \" unsigned long ncol()  \\n   docstring : ncol\"\n#&gt;     .External(list(name = \"CppMethod__invoke_notvoid\", address = &lt;pointer: 0x1030ca1b0&gt;, \n#&gt;         dll = list(name = \"Rcpp\", path = \"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/libs/Rcpp.so\", \n#&gt;             dynamicLookup = TRUE, handle = &lt;pointer: 0x7aab93b0&gt;, \n#&gt;             info = &lt;pointer: 0x1030ba810&gt;, forceSymbols = FALSE), \n#&gt;         numParameters = -1L), &lt;pointer: 0x10d012710&gt;, &lt;pointer: 0x10310f510&gt;, \n#&gt;         .pointer)\n#&gt; }\n#&gt; &lt;environment: 0x10e2f3db0&gt;"
  },
  {
    "objectID": "slides/week-4.html#access-via-function",
    "href": "slides/week-4.html#access-via-function",
    "title": "Week 4",
    "section": "Access via Function",
    "text": "Access via Function\n\next(r)\n#&gt; SpatExtent : -120, -80, 20, 60 (xmin, xmax, ymin, ymax)\ncrs(r)\n#&gt; [1] \"GEOGCRS[\\\"WGS 84 (CRS84)\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"unknown\\\"],\\n        AREA[\\\"World\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"OGC\\\",\\\"CRS84\\\"]]\"\nnrow(r)\n#&gt; [1] 20\nncol(r)\n#&gt; [1] 20\nhead(values(r))\n#&gt;      lyr.1\n#&gt; [1,]     1\n#&gt; [2,]     2\n#&gt; [3,]     3\n#&gt; [4,]     4\n#&gt; [5,]     5\n#&gt; [6,]     6"
  },
  {
    "objectID": "slides/week-4.html#multi-layers",
    "href": "slides/week-4.html#multi-layers",
    "title": "Week 4",
    "section": "Multi-layers",
    "text": "Multi-layers\n\nIn many cases multi-variable raster data sets are used. Variables can related to time or measurements\nA SpatRaster is a collection ofobjects with the same spatial extent and resolution.\nIn essence it is a list of objects, or a 3D array."
  },
  {
    "objectID": "slides/week-4.html#remember-our-array-data",
    "href": "slides/week-4.html#remember-our-array-data",
    "title": "Week 4",
    "section": "Remember our Array data?",
    "text": "Remember our Array data?\n\n(v = 1:27)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n#&gt; [26] 26 27\n(arr = array(v, dim = c(3,3,3)))\n#&gt; , , 1\n#&gt; \n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    4    7\n#&gt; [2,]    2    5    8\n#&gt; [3,]    3    6    9\n#&gt; \n#&gt; , , 2\n#&gt; \n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   10   13   16\n#&gt; [2,]   11   14   17\n#&gt; [3,]   12   15   18\n#&gt; \n#&gt; , , 3\n#&gt; \n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   19   22   25\n#&gt; [2,]   20   23   26\n#&gt; [3,]   21   24   27"
  },
  {
    "objectID": "slides/week-4.html#an-array-is-a-single-file-so-it-can-be-rasterized",
    "href": "slides/week-4.html#an-array-is-a-single-file-so-it-can-be-rasterized",
    "title": "Week 4",
    "section": "An array is a single file so it can be ‚Äúrasterized‚Äù",
    "text": "An array is a single file so it can be ‚Äúrasterized‚Äù\n\nb = rast(arr)\nplot(b)"
  },
  {
    "objectID": "slides/week-4.html#bounding-box-extents",
    "href": "slides/week-4.html#bounding-box-extents",
    "title": "Week 4",
    "section": "1. Bounding Box / Extents",
    "text": "1. Bounding Box / Extents\nGeometries have extents that define the maximum and minimum coverage of the shape in a coordinate reference system\n\nImage Source: National Ecological Observatory Network (NEON)"
  },
  {
    "objectID": "slides/week-4.html#extent",
    "href": "slides/week-4.html#extent",
    "title": "Week 4",
    "section": "2. Extent",
    "text": "2. Extent\n\nWhen dealing with objects, the extent (or bbox) is derived from the coordinate set\nWhen dealing with raster data, the extent is a fondational component of the raster data structure\n\nThat is, we need to know the area the raster is covering!\n\n\n\nImage Source: National Ecological Observatory Network (NEON)"
  },
  {
    "objectID": "slides/week-4.html#discretization",
    "href": "slides/week-4.html#discretization",
    "title": "Week 4",
    "section": "3. Discretization",
    "text": "3. Discretization\nOnce we know the extent, we need to know how that space is split up\nTwo complimentary bit of information can tell us this:\n\nResolution (res)\nNumber of row and number of columns (nrow/ncol)\n\n\nImage Source: National Ecological Observatory Network (NEON)"
  },
  {
    "objectID": "slides/week-4.html#so",
    "href": "slides/week-4.html#so",
    "title": "Week 4",
    "section": "So,",
    "text": "So,\nA raster is made of an extent, and a resolution / row-column structure\n\nA vector of values fill that structure (same way a vector in R can have diminisons)\n\nThese values are often scaled to integers to reduce file size\n\nValues are referenced in cartisian space, based on cell index\nA CRS along with the extent, can provide spatial reference / coordinates"
  },
  {
    "objectID": "slides/week-4.html#general-process",
    "href": "slides/week-4.html#general-process",
    "title": "Week 4",
    "section": "General Process",
    "text": "General Process\nAlmost all remote sensing / image analysis begins with the same basic steps:\n\nIdentifying an area of interest (AOI)\nIdentifying and downloading the relevant images or products\nAnalyzing the raster products\n\nThe definition of a AOI is critical because raster data in continuous, therefore we need to define the bounds of the study rather then the bounds of the objects\n\nBut, objects often (even typically) define our bounds"
  },
  {
    "objectID": "slides/week-4.html#find-elevation-data-for-fort-collins",
    "href": "slides/week-4.html#find-elevation-data-for-fort-collins",
    "title": "Week 4",
    "section": "Find elevation data for Fort Collins:",
    "text": "Find elevation data for Fort Collins:\n\nDefine the AOI\n\n\nbb = read_csv(\"../labs/data/uscities.csv\") |&gt;\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  filter(city == \"Fort Collins\") |&gt; \n  st_transform(5070) |&gt; \n  st_buffer(50000) |&gt; \n  st_bbox() |&gt; \n  st_as_sfc() |&gt; \n  st_as_sf()\n\n\nRead data from elevation map tiles, for a specific zoom, and crop to the AOI\n\n\nelev = elevatr::get_elev_raster(bb, z = 11) |&gt; crop(bb)\nwriteRaster(elev, filename = \"data/foco-elev.tif\", overwrite = TRUE)\n\n\nThe resulting raster ‚Ä¶\n\n\n(elev = rast(\"data/foco-elev.tif\"))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source      : foco-elev.tif \n#&gt; name        : foco-elev \n#&gt; min value   :      1382 \n#&gt; max value   :      4346"
  },
  {
    "objectID": "slides/week-4.html#raster-values-1",
    "href": "slides/week-4.html#raster-values-1",
    "title": "Week 4",
    "section": "Raster Values",
    "text": "Raster Values\n\n# The length of the vector is equal to the rows * columns\nlength(v) == nrow(elev) * ncol(elev)\n#&gt; [1] TRUE\n# The span of the x extent divided by the resolution equals the raster rows\n((xmax(elev) - xmin(elev)) / res(elev)[1]) == ncol(elev) \n#&gt; [1] TRUE\n# The span of the x extent divided by the number of rows equals the raster resolution\n((xmax(elev) - xmin(elev)) / ncol(elev)) == res(elev)[1] \n#&gt; [1] TRUE"
  },
  {
    "objectID": "slides/week-4.html#raster-values-2",
    "href": "slides/week-4.html#raster-values-2",
    "title": "Week 4",
    "section": "Raster Values",
    "text": "Raster Values\n\n# The length of the vector is equal to the rows * columns\nlength(v) == nrow(elev) * ncol(elev)\n#&gt; [1] TRUE\n# The span of the x extent divided by the resolution equals the raster rows\n((xmax(elev) - xmin(elev)) / res(elev)[1]) == ncol(elev) \n#&gt; [1] TRUE\n# The span of the x extent divided by the number of rows equals the raster resolution\n((xmax(elev) - xmin(elev)) / ncol(elev)) == res(elev)[1] \n#&gt; [1] TRUE"
  },
  {
    "objectID": "slides/week-4.html#all-image-files-are-the-same",
    "href": "slides/week-4.html#all-image-files-are-the-same",
    "title": "Week 4",
    "section": "All image files are the same!",
    "text": "All image files are the same!\n\ndownload.file(url = \"https://a.tile.openstreetmap.org/18/43803/104352.png\",\n              destfile = \"data/104352.png\")\nimg = png::readPNG(\"data/104352.png\")\nclass(img)\n#&gt; [1] \"array\"\ntypeof(img)\n#&gt; [1] \"double\"\ndim(img)\n#&gt; [1] 256 256   3"
  },
  {
    "objectID": "slides/week-4.html#section-7",
    "href": "slides/week-4.html#section-7",
    "title": "Week 4",
    "section": "",
    "text": "plot(s, col = viridis::viridis(256))"
  },
  {
    "objectID": "slides/week-4.html#raster-algebra",
    "href": "slides/week-4.html#raster-algebra",
    "title": "Week 4",
    "section": "Raster Algebra",
    "text": "Raster Algebra\n\nSo our raster data is stored as a large numeric array/vector\nMany generic functions allow for simple algebra on Raster objects,\nThese include:\n\nnormal algebraic operators such as +, -, *, /\nlogical operators such as &gt;, &gt;=, &lt;,==, !\nfunctions like abs, round, ceiling, floor, trunc, sqrt, log, log10, exp, cos, sin, atan, tan, max, min, range, prod, sum, any, all"
  },
  {
    "objectID": "slides/week-4.html#section-9",
    "href": "slides/week-4.html#section-9",
    "title": "Week 4",
    "section": "",
    "text": "plot(r)\nplot(river, add = TRUE, col = \"blue\", lwd = 2)\nplot(line, add = TRUE, col = \"black\", lwd = 2)\nplot(outlet$geom, add = TRUE, pch = 16, col = \"red\")\nplot(inlet$geom,  add = TRUE, pch = 16, col = \"green\")"
  },
  {
    "objectID": "slides/week-4.html#replacement",
    "href": "slides/week-4.html#replacement",
    "title": "Week 4",
    "section": "Replacement",
    "text": "Replacement\n\nRaster values can be replaced on a conditional statements\nDoing this changes the underlying data!\nIf you want to retain the original data, you must make a copy of the base layer\n\n\n\n\nplot(elev)\n\n\n\n\n\n\n\n\n\n\nelev2 = elev #&lt;&lt;\nelev2[elev2 &lt;= 1500] = NA #&lt;&lt;\nplot(elev2)"
  },
  {
    "objectID": "slides/week-4.html#modifying-a-raster",
    "href": "slides/week-4.html#modifying-a-raster",
    "title": "Week 4",
    "section": "Modifying a raster",
    "text": "Modifying a raster\nWhen we want to modify the extent of a raster we can clip it to a new bounds\ncrop: lets you reduce the extent of a raster to the extent of another, overlapping object:\n\n\n\n#remotes::install_github(\"mikejohnson51/AOI\")\nfc = AOI::geocode(\"Fort Collins\", bbox = TRUE) |&gt; \n  st_transform(crs(elev))\n\nplot(elev)\nplot(fc, add = TRUE, col = NA)\n\n\n\n\n\n\n\n\n\n\nfc_elev = crop(elev, fc) #&lt;&lt;\nplot(fc_elev)"
  },
  {
    "objectID": "slides/week-4.html#modifying-the-underlying-data",
    "href": "slides/week-4.html#modifying-the-underlying-data",
    "title": "Week 4",
    "section": "Modifying the underlying data:",
    "text": "Modifying the underlying data:\nmask: mask takes an input object (sf, sp, or raster) and set anything not undelying the input to a new value (default = NA)\n\nlibrary(osmdata)\n\nosm = osmdata::opq(st_bbox(st_transform(fc,4326))) |&gt; \n  add_osm_feature(\"water\") |&gt; \n  osmdata_sf()\n\n(poly = osm$osm_polygons |&gt; \n  st_transform(crs(elev)))\n#&gt; Simple feature collection with 181 features and 17 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -769181 ymin: 1977155 xmax: -753314.8 ymax: 2000444\n#&gt; Projected CRS: PROJCRS[\"unknown\",\n#&gt;     BASEGEOGCRS[\"NAD83\",\n#&gt;         DATUM[\"North American Datum 1983\",\n#&gt;             ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n#&gt;                 LENGTHUNIT[\"metre\",1]]],\n#&gt;         PRIMEM[\"Greenwich\",0,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         ID[\"EPSG\",4269]],\n#&gt;     CONVERSION[\"Albers Equal Area\",\n#&gt;         METHOD[\"Albers Equal Area\",\n#&gt;             ID[\"EPSG\",9822]],\n#&gt;         PARAMETER[\"Latitude of false origin\",23,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8821]],\n#&gt;         PARAMETER[\"Longitude of false origin\",-96,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8822]],\n#&gt;         PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8823]],\n#&gt;         PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;             ID[\"EPSG\",8824]],\n#&gt;         PARAMETER[\"Easting at false origin\",0,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8826]],\n#&gt;         PARAMETER[\"Northing at false origin\",0,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",8827]]],\n#&gt;     CS[Cartesian,2],\n#&gt;         AXIS[\"easting\",east,\n#&gt;             ORDER[1],\n#&gt;             LENGTHUNIT[\"metre\",1,\n#&gt;                 ID[\"EPSG\",9001]]],\n#&gt;         AXIS[\"northing\",north,\n#&gt;             ORDER[2],\n#&gt;             LENGTHUNIT[\"metre\",1,\n#&gt;                 ID[\"EPSG\",9001]]]]\n#&gt; First 10 features:\n#&gt;            osm_id              name         alt_name area basin boat\n#&gt; 6051656   6051656              &lt;NA&gt;             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 23598705 23598705          Lee Lake             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 23598868 23598868              &lt;NA&gt;             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 23598945 23598945      College Lake             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 23599224 23599224       Warren Lake             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 23599227 23599227 Harmony Reservoir             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 23599320 23599320              &lt;NA&gt;             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 23599327 23599327              &lt;NA&gt;             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 44074911 44074911     Lake Sherwood Nelson Reservoir &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt; 47471957 47471957              &lt;NA&gt;             &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt;\n#&gt;          description  ele gnis:feature_id intermittent landuse layer natural\n#&gt; 6051656         &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n#&gt; 23598705        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt; 23598868        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n#&gt; 23598945        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt; 23599224        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt; 23599227        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt; 23599320        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt; 23599327        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt; 44074911        &lt;NA&gt; 1512          201223         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt; 47471957        &lt;NA&gt; &lt;NA&gt;            &lt;NA&gt;         &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;   water\n#&gt;          place salt source     water                       geometry\n#&gt; 6051656   &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt;      &lt;NA&gt; POLYGON ((-762901.7 1988981...\n#&gt; 23598705  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt;      lake POLYGON ((-765209.9 1991246...\n#&gt; 23598868  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt;      &lt;NA&gt; POLYGON ((-767955.5 1985345...\n#&gt; 23598945  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt;      lake POLYGON ((-766871.6 1989267...\n#&gt; 23599224  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt; reservoir POLYGON ((-760465.5 1983486...\n#&gt; 23599227  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt; reservoir POLYGON ((-760046.6 1982348...\n#&gt; 23599320  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt;      pond POLYGON ((-762458.2 1991222...\n#&gt; 23599327  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt;      pond POLYGON ((-763462.8 1992438...\n#&gt; 44074911  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt; reservoir POLYGON ((-758628.3 1984729...\n#&gt; 47471957  &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt; reservoir POLYGON ((-764440.4 1992962..."
  },
  {
    "objectID": "slides/week-4.html#section-10",
    "href": "slides/week-4.html#section-10",
    "title": "Week 4",
    "section": "",
    "text": "(E &lt;- kmeans(vs, 5, iter.max = 100))\n#&gt; K-means clustering with 5 clusters of sizes 1471, 3619, 542, 6521, 4677\n#&gt; \n#&gt; Cluster means:\n#&gt;          ppt       tmax       tmin       srad          q\n#&gt; 1  1.7381744  0.2854842 -0.1759393 -0.3587139 -0.0132010\n#&gt; 2 -0.2805363 -0.1935889  1.4262188  0.5651088  0.5636098\n#&gt; 3  3.2106644  4.8126020 -0.5710265 -1.7692979 -1.2335931\n#&gt; 4 -0.5799378 -0.2419152 -0.1384006  0.6815967  0.6486097\n#&gt; 5  0.1069063 -0.1604128 -0.7891111 -1.0697441 -1.1933422\n#&gt; \n#&gt; Clustering vector:\n#&gt;     [1] 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;    [37] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;    [73] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [109] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [145] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 4 4 4 4\n#&gt;   [181] 4 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5\n#&gt;   [217] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [253] 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [289] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [325] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [361] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [397] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4\n#&gt;   [433] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [469] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [505] 4 4 4 4 4 4 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [541] 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [577] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [613] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [649] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [685] 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5\n#&gt;   [721] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [757] 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [793] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [829] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 4 4 4 4 4 4\n#&gt;   [865] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [901] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [937] 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;   [973] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1009] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1045] 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1081] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4\n#&gt;  [1117] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1153] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1189] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5\n#&gt;  [1225] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1261] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1297] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1333] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4\n#&gt;  [1369] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1405] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1441] 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1477] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1513] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1549] 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1585] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4\n#&gt;  [1621] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1657] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1693] 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5\n#&gt;  [1729] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1765] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1801] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1837] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1873] 4 4 4 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5\n#&gt;  [1909] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1945] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [1981] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2017] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2053] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2089] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2125] 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2161] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2197] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2233] 4 5 5 5 5 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2269] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4\n#&gt;  [2305] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2341] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2377] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5\n#&gt;  [2413] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2449] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2485] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2521] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 4 4\n#&gt;  [2557] 4 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2593] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2629] 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2665] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2701] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4\n#&gt;  [2737] 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2773] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2809] 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2845] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [2881] 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5\n#&gt;  [2917] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2953] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4\n#&gt;  [2989] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3025] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3061] 4 4 4 4 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3097] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3133] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3169] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3205] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3241] 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3277] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3313] 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3349] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3385] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3421] 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3457] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4\n#&gt;  [3493] 4 4 2 4 4 4 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3529] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3565] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5\n#&gt;  [3601] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3637] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 2 2 2 2 2 2 2 2 4\n#&gt;  [3673] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3709] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3745] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3781] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3817] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3853] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3889] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [3925] 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3961] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3997] 5 5 5 4 4 4 4 4 4 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4033] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4069] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5\n#&gt;  [4105] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4141] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 2 2\n#&gt;  [4177] 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4213] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4249] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4285] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4321] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 2 2 2 2 2 2 2 2 2 4 4 4 4\n#&gt;  [4357] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4393] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4429] 4 4 4 5 5 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4465] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4501] 5 5 5 5 5 5 5 5 5 5 4 4 4 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4537] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4573] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 4 4 4 4 4\n#&gt;  [4609] 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4645] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4681] 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4717] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4753] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5\n#&gt;  [4789] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4825] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 2 2 2 2 2 2\n#&gt;  [4861] 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4897] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [4933] 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4969] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5005] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 2 2 2 2 2 2 2 2 2 2 4 4 2 4 4 4 4\n#&gt;  [5041] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5077] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5113] 5 5 4 4 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5149] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5185] 5 5 5 5 5 5 5 4 4 2 2 2 2 2 2 2 2 2 2 2 4 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5221] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5257] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 4 5 5 5 5 5\n#&gt;  [5293] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5329] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4\n#&gt;  [5365] 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5401] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5437] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5473] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5509] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5545] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5581] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5617] 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5653] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5689] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4\n#&gt;  [5725] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5761] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5\n#&gt;  [5797] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5833] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5869] 5 5 5 5 4 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5905] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5941] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5977] 5 5 5 5 4 4 4 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6013] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 2 2 2 2\n#&gt;  [6049] 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6085] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6121] 4 1 4 4 4 4 4 4 4 4 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4\n#&gt;  [6157] 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6193] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6229] 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6265] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6301] 4 5 4 4 4 4 4 5 4 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5\n#&gt;  [6337] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6373] 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4\n#&gt;  [6409] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6445] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4 4 4 5\n#&gt;  [6481] 4 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6517] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6553] 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6589] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6625] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4 4 4 5 4 4 4 4 5 5 5 5\n#&gt;  [6661] 5 5 5 1 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6697] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6733] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6769] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6805] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6841] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6877] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6913] 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6949] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6985] 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7021] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7057] 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 4 4\n#&gt;  [7093] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7129] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7165] 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7201] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 5\n#&gt;  [7237] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7273] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7309] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 5 5 5 5 5\n#&gt;  [7345] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7381] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 5 5 2 2 2 2 2 2 2 2 2\n#&gt;  [7417] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7453] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7489] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7525] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7561] 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7597] 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7633] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7669] 4 4 4 4 4 4 4 4 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7705] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7741] 2 2 2 2 2 2 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4\n#&gt;  [7777] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7813] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 5 5\n#&gt;  [7849] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7885] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 5 5 2 2 5 5 5 2\n#&gt;  [7921] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 4 4 4 4 4 4 4 4 4\n#&gt;  [7957] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7993] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8029] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8065] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8101] 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8137] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8173] 4 4 4 4 4 4 4 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8209] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8245] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8281] 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8317] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5\n#&gt;  [8353] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8389] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8425] 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4\n#&gt;  [8461] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8497] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8533] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8569] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2\n#&gt;  [8605] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8641] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8677] 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 1 5 5 5 1 4 4 1 1 1 5 5 5 5 5 5 1 5 5 5 5\n#&gt;  [8713] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8749] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8785] 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8821] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 4 4 4 4\n#&gt;  [8857] 4 4 4 4 4 1 1 1 1 4 4 4 4 4 4 4 5 5 1 1 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8893] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8929] 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8965] 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9001] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9037] 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9073] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9109] 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4\n#&gt;  [9145] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9181] 4 4 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9217] 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9253] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2\n#&gt;  [9289] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9325] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt;  [9361] 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9397] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9433] 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9469] 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9505] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4\n#&gt;  [9541] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9577] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2\n#&gt;  [9613] 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9649] 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9685] 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9721] 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9757] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 5 5 5 5 5 5 5 2 2\n#&gt;  [9793] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 4 4 4 4 4 4 4\n#&gt;  [9829] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 4\n#&gt;  [9865] 4 4 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 5 5 5 5 5 5 5 5\n#&gt;  [9901] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5\n#&gt;  [9937] 5 5 5 5 5 5 5 5 5 5 2 5 5 2 2 2 2 2 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9973] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10009] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 4 4 1 1 1 1 1 1 1\n#&gt; [10045] 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10081] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10117] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10153] 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10189] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4\n#&gt; [10225] 4 4 4 4 4 4 4 4 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10261] 5 5 5 5 5 5 5 5 5 5 5 5 4 2 2 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2\n#&gt; [10297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10333] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10369] 4 4 1 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 1 5\n#&gt; [10405] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10441] 5 5 4 2 2 2 5 2 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10477] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4\n#&gt; [10513] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 1 4 4 4 4\n#&gt; [10549] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 1 5 5 5 5 5 5 5 5 5 5\n#&gt; [10585] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 5\n#&gt; [10621] 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10657] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10693] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 4 4 4 4 4 4 1 1 1 1 1 1 1 1\n#&gt; [10729] 1 1 1 1 1 1 1 4 4 4 4 4 4 4 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10765] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 5 2 2 2 2 2 2 2 2 2\n#&gt; [10801] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10837] 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10873] 4 4 4 4 4 4 4 4 1 1 4 4 1 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4\n#&gt; [10909] 4 4 4 4 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10945] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10981] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11017] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1\n#&gt; [11053] 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 1 1 5 5 5 5\n#&gt; [11089] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11125] 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11161] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4\n#&gt; [11197] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 4 4 1 4 4 4 4\n#&gt; [11233] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11269] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 2 2 2\n#&gt; [11305] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11341] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11377] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 1 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1\n#&gt; [11413] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5\n#&gt; [11449] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11485] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11521] 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11557] 4 4 4 4 1 4 4 4 4 1 1 1 1 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11593] 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11629] 5 5 5 5 5 5 5 2 2 5 5 3 5 5 5 2 2 2 2 2 2 2 2 2 5 5 2 2 2 2 2 2 2 2 2 2\n#&gt; [11665] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4\n#&gt; [11701] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 1\n#&gt; [11737] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 5 1 5 5\n#&gt; [11773] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2\n#&gt; [11809] 5 5 3 5 5 5 2 2 2 2 2 2 2 2 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11845] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11881] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11917] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 5 5 3 3 3 5 5 5 5 5\n#&gt; [11953] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 5 1 5 5 5 2 2 2 2\n#&gt; [11989] 2 2 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12025] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12061] 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12097] 1 1 1 1 1 1 1 1 1 1 3 3 1 1 3 3 5 5 3 3 3 3 5 5 1 1 3 3 5 5 5 5 5 5 5 5\n#&gt; [12133] 5 5 5 5 5 5 4 4 4 4 2 2 2 2 2 2 2 5 1 1 5 5 5 2 2 2 2 2 5 5 5 5 5 5 2 2\n#&gt; [12169] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12205] 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12241] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12277] 3 3 1 3 3 3 5 5 3 3 3 5 5 5 3 3 3 3 3 1 1 1 1 5 5 5 5 5 5 5 5 5 5 2 2 2\n#&gt; [12313] 2 2 2 2 2 2 2 2 1 3 5 5 5 5 2 2 2 2 2 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12349] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4\n#&gt; [12385] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1\n#&gt; [12421] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 1 1 3 3 1 3 3 3 1 5 3 3 3\n#&gt; [12457] 3 3 3 3 3 3 1 3 3 3 5 5 5 1 5 5 5 5 5 1 5 5 5 4 2 2 2 2 2 2 2 2 2 2 2 5\n#&gt; [12493] 3 5 5 5 2 2 2 2 2 2 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12529] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12565] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12601] 1 1 1 1 1 1 1 1 1 3 3 3 3 3 1 3 1 1 3 3 3 3 1 1 3 3 3 3 3 3 3 1 1 3 3 3\n#&gt; [12637] 3 1 1 3 3 1 1 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 1 5 2 2 2 2 2\n#&gt; [12673] 2 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12709] 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12745] 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12781] 1 3 3 3 3 3 1 1 3 3 3 3 5 3 3 3 3 1 1 3 3 3 3 1 3 3 3 3 3 3 3 3 1 5 5 1\n#&gt; [12817] 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 3 1 1 5 5 2 2 2 2 2 2 5 1 1 5 2 2 2 2\n#&gt; [12853] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12889] 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt; [12925] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 3 3 3 1 3 1 1 3\n#&gt; [12961] 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 5 5 5 5 5 2 2 2 2 2 2 2 2\n#&gt; [12997] 2 2 2 2 2 2 2 5 3 3 3 5 2 2 2 2 2 2 2 1 5 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13033] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4\n#&gt; [13069] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13105] 1 1 1 1 1 1 3 1 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 1 1 1 3 3 3 3 3 1 3 3 3\n#&gt; [13141] 5 5 1 3 3 3 3 3 3 3 3 3 3 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 3\n#&gt; [13177] 3 5 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13213] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13249] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1\n#&gt; [13285] 1 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 3 3 3 3 1 1 5 1 3 3 5 5 5 5 1 3 3 3 3 3\n#&gt; [13321] 1 1 1 1 1 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 2 2 2 2 2 2 2 2\n#&gt; [13357] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13393] 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13429] 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3\n#&gt; [13465] 3 3 1 1 3 3 3 1 3 1 1 1 5 5 1 1 1 3 3 1 1 1 3 3 3 3 1 5 1 1 5 2 2 2 2 2\n#&gt; [13501] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13537] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4\n#&gt; [13573] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1\n#&gt; [13609] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 3 3 3 3 3 3 3 1 3 1 1 3 3 3 3 1 1\n#&gt; [13645] 1 1 3 1 5 5 1 3 3 1 1 3 5 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13681] 2 2 2 2 2 2 1 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13717] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13753] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13789] 1 1 1 1 1 1 1 1 3 1 3 3 1 1 3 3 1 3 1 3 3 3 3 3 3 3 3 3 3 3 1 5 1 3 3 1\n#&gt; [13825] 3 3 1 1 1 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 3 5 5 2\n#&gt; [13861] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13897] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13933] 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13969] 1 1 1 1 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 3 3 3 3 3 3 3 1 1 1 1 1 1 5\n#&gt; [14005] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 1 1 1 5 1 5 2 2 2 2 2 2 2\n#&gt; [14041] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14077] 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 1\n#&gt; [14113] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3\n#&gt; [14149] 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 3 3 3 3 1 1 1 2 2 2 2 2 2 2 2\n#&gt; [14185] 2 2 2 2 2 2 2 2 2 2 2 3 1 1 3 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14221] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4\n#&gt; [14257] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 1 1 1 1 1 1 1 1 1\n#&gt; [14293] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 3 3 1 3 1 1 1 1 3 3\n#&gt; [14329] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14365] 2 3 3 3 1 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14401] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14437] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14473] 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 3 1 1 3 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14509] 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2\n#&gt; [14545] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14581] 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14617] 4 4 4 4 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14653] 1 1 1 3 1 1 1 1 3 1 1 1 1 1 1 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1\n#&gt; [14689] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14725] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14761] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2\n#&gt; [14797] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14833] 1 1 1 1 1 1 3 3 1 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 2 2 2\n#&gt; [14869] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14905] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4\n#&gt; [14941] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 1 1 1 2 2 1 1 1 1\n#&gt; [14977] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15013] 1 1 1 3 3 3 3 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15049] 2 2 2 1 1 1 1 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15085] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15121] 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15157] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3\n#&gt; [15193] 3 3 3 3 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 3 1 2 1 1\n#&gt; [15229] 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15265] 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15301] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15337] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 2 2 2\n#&gt; [15373] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 3 3 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt; [15409] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4\n#&gt; [15445] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 1 1 2 2\n#&gt; [15481] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15517] 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15553] 2 2 2 2 2 2 2 2 1 3 3 3 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15589] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15625] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 1 1 2 2 2 1 1 1 1 1 1 1 1 1\n#&gt; [15661] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15697] 3 3 3 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n#&gt; [15733] 3 3 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15769] 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15805] 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15841] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15877] 3 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 3 3 1 2 2 2 2 2 2\n#&gt; [15913] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15949] 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2\n#&gt; [15985] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1\n#&gt; [16021] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 3 3 3 3 3 3 3 3 3 1 1 1 2 2 2 2 2 2\n#&gt; [16057] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 3 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16093] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4\n#&gt; [16129] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16165] 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 1 1 2 1 1 1 1 1 1 1 2 2\n#&gt; [16201] 1 1 1 1 1 3 3 1 3 3 3 3 3 3 3 3 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16237] 2 2 2 2 2 1 3 1 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16273] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16309] 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16345] 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 3 3\n#&gt; [16381] 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 3 3 1 1\n#&gt; [16417] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16453] 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16489] 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16525] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 3 3 1 3 3 3 3 3 3 3 1 2\n#&gt; [16561] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 3 3 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt; [16597] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4\n#&gt; [16633] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2\n#&gt; [16669] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2\n#&gt; [16705] 2 2 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt; [16741] 2 2 2 2 2 2 2 2 1 3 3 3 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16777] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16813] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 3516.041 4545.222 3785.104 5524.327 7478.029\n#&gt;  (between_SS / total_SS =  70.5 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/week-4.html#what-is-mask-doing",
    "href": "slides/week-4.html#what-is-mask-doing",
    "title": "Week 4",
    "section": "What is mask doing?",
    "text": "What is mask doing?\n\nNA * 7\n#&gt; [1] NA\n\n\n\nmask_r = rasterize(poly, fc_elev, background = NA)\nplot(mask_r)"
  },
  {
    "objectID": "slides/week-4.html#section-11",
    "href": "slides/week-4.html#section-11",
    "title": "Week 4",
    "section": "",
    "text": "par(mfrow = c(2,1))\nplot(elev)\nplot(co$ppt)\n\n\n\n\n\n\n\n\n\n\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))"
  },
  {
    "objectID": "slides/week-4.html#crop-orand-mask",
    "href": "slides/week-4.html#crop-orand-mask",
    "title": "Week 4",
    "section": "Crop or/and mask",
    "text": "Crop or/and mask\n\nCrop is more efficient then mask\nOften you will want to mask and crop a raster\nThe correct way to do this is crop then mask\n\n\ncm = crop(fc_elev, poly) |&gt;  \n  mask(poly)\n\nplot(cm)"
  },
  {
    "objectID": "slides/week-4.html#aggregate-and-disaggregate",
    "href": "slides/week-4.html#aggregate-and-disaggregate",
    "title": "Week 4",
    "section": "Aggregate and disaggregate",
    "text": "Aggregate and disaggregate\n\naggregate and disaggregate allow for changing the resolution of a Raster object.\nThis is similar to the zoom scaling on a web map except the scale factor is not set to 2\nFor aggregate, you need to specify a function determining what to do with the grouped cell values (default = mean).\n\n\n\n\nplot(fc_elev)\nplot(rpoly, add = T)\n\n\n\n\n\n\n\n\n\n\nagg = aggregate(fc_elev, 10, fun = max) #&lt;&lt;\nplot(agg)\nplot(rpoly, add = T)"
  },
  {
    "objectID": "slides/week-4.html#summary-values",
    "href": "slides/week-4.html#summary-values",
    "title": "Week 4",
    "section": "Summary Values",
    "text": "Summary Values\nglobal: computes statistics for the values of each layer in a Raster* object.\n\nelev &lt;- rast('data/foco-elev.tif')\nglobal(elev, mean)\n#&gt;               mean\n#&gt; foco-elev 1887.874\nmean(values(elev), na.rm = TRUE)\n#&gt; [1] 1887.874"
  },
  {
    "objectID": "slides/week-4.html#reclassify",
    "href": "slides/week-4.html#reclassify",
    "title": "Week 4",
    "section": "Reclassify",
    "text": "Reclassify\n\nReclassify is a function that allows you to change the values of a raster based on a set of rules\nThe rules are defined in a data frame with three columns:\n\nmin = the minimum value of the range\nmax = the maximum value of the range\nlab = the new value to assign to that range\n\n\n\n(rcl = data.frame(min = seq(1500,1590,10), max =  seq(1510,1600,10), lab = c(0:9)))\n#&gt;     min  max lab\n#&gt; 1  1500 1510   0\n#&gt; 2  1510 1520   1\n#&gt; 3  1520 1530   2\n#&gt; 4  1530 1540   3\n#&gt; 5  1540 1550   4\n#&gt; 6  1550 1560   5\n#&gt; 7  1560 1570   6\n#&gt; 8  1570 1580   7\n#&gt; 9  1580 1590   8\n#&gt; 10 1590 1600   9\n\n\n(rc = classify(elev_cut, rcl, include.lowest = TRUE))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; name        : lyr.1 \n#&gt; min value   :     2 \n#&gt; max value   :  4346"
  },
  {
    "objectID": "slides/week-4.html#real-example-rainfall-regions-of-colorado",
    "href": "slides/week-4.html#real-example-rainfall-regions-of-colorado",
    "title": "Week 4",
    "section": "Real Example: Rainfall Regions of Colorado",
    "text": "Real Example: Rainfall Regions of Colorado\n\n#remotes::install_github(\"mikejohnson51/climateR\")\nlibrary(climateR)\n\nAOI = AOI::aoi_get(state = 'CO') \n\nsystem.time({ prcp = climateR::getTerraClim(AOI, \"ppt\", \n                                            startDate = \"2000-01-01\", endDate = '2005-12-31') })\n#&gt;    user  system elapsed \n#&gt;   0.235   0.020   2.385\n\n\n# More on global below ...\nquarts = global(prcp$ppt, fivenum)\n\n(quarts = colMeans(quarts))\n#&gt;         X1         X2         X3         X4         X5 \n#&gt;   6.271233  18.434247  27.995890  41.608219 105.642466\n\n(rcl = data.frame(quarts[1:4], quarts[2:5], 1:4))\n#&gt;    quarts.1.4. quarts.2.5. X1.4\n#&gt; X1    6.271233    18.43425    1\n#&gt; X2   18.434247    27.99589    2\n#&gt; X3   27.995890    41.60822    3\n#&gt; X4   41.608219   105.64247    4"
  },
  {
    "objectID": "slides/week-4.html#section-12",
    "href": "slides/week-4.html#section-12",
    "title": "Week 4",
    "section": "",
    "text": "plot(r)\nplot(river, add = TRUE, col = \"blue\", lwd = 2)\nplot(line, add = TRUE, col = \"black\", lwd = 2)\nplot(outlet$geometry, add = TRUE, pch = 16, col = \"red\")\nplot(inlet$geometry, add = TRUE, pch = 16, col = \"green\")"
  },
  {
    "objectID": "slides/week-4.html#section-13",
    "href": "slides/week-4.html#section-13",
    "title": "Week 4",
    "section": "",
    "text": "(E = kmeans(vs, 5, iter.max = 100))\n#&gt; K-means clustering with 5 clusters of sizes 542, 6521, 3619, 1471, 4677\n#&gt; \n#&gt; Cluster means:\n#&gt;          ppt       tmax       tmin       srad          q\n#&gt; 1  3.2106644  4.8126020 -0.5710265 -1.7692979 -1.2335931\n#&gt; 2 -0.5799378 -0.2419152 -0.1384006  0.6815967  0.6486097\n#&gt; 3 -0.2805363 -0.1935889  1.4262188  0.5651088  0.5636098\n#&gt; 4  1.7381744  0.2854842 -0.1759393 -0.3587139 -0.0132010\n#&gt; 5  0.1069063 -0.1604128 -0.7891111 -1.0697441 -1.1933422\n#&gt; \n#&gt; Clustering vector:\n#&gt;     [1] 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;    [37] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;    [73] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [109] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [145] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 2 2 2 2\n#&gt;   [181] 2 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5\n#&gt;   [217] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [253] 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [289] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [325] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [361] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [397] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2\n#&gt;   [433] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [469] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [505] 2 2 2 2 2 2 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [541] 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [577] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [613] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [649] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [685] 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5\n#&gt;   [721] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [757] 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [793] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [829] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 2 2 2 2 2 2\n#&gt;   [865] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [901] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [937] 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [973] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1009] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1045] 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1081] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2\n#&gt;  [1117] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1153] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1189] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5\n#&gt;  [1225] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1261] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1333] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 2 2 2 2 2 2 2\n#&gt;  [1369] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1405] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1441] 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1477] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1513] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1549] 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1585] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2\n#&gt;  [1621] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1657] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1693] 2 2 2 2 2 2 2 2 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5\n#&gt;  [1729] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1765] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1801] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1837] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1873] 2 2 2 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5\n#&gt;  [1909] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1945] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1981] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2017] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2053] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2089] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2125] 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2161] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2197] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2233] 2 5 5 5 5 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2269] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2\n#&gt;  [2305] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2341] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2377] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5\n#&gt;  [2413] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2449] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2485] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2521] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 2 2\n#&gt;  [2557] 2 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2593] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2629] 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2665] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2701] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2\n#&gt;  [2737] 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2773] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2809] 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2845] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2881] 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5\n#&gt;  [2917] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2953] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2\n#&gt;  [2989] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3025] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3061] 2 2 2 2 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3097] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3133] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3169] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3205] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3241] 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3277] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3313] 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3349] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3385] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3421] 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3457] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2\n#&gt;  [3493] 2 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3529] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3565] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5\n#&gt;  [3601] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3637] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 3 3 3 3 3 3 3 3 2\n#&gt;  [3673] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3709] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3745] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3781] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3817] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3853] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3889] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3925] 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3961] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3997] 5 5 5 2 2 2 2 2 2 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4033] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4069] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5\n#&gt;  [4105] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4141] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 3 3\n#&gt;  [4177] 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4213] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4249] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4285] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4321] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 3 3 3 3 3 3 3 3 3 2 2 2 2\n#&gt;  [4357] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4393] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4429] 2 2 2 5 5 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4465] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4501] 5 5 5 5 5 5 5 5 5 5 2 2 2 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4537] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4573] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 2 2 2 2 2\n#&gt;  [4609] 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4645] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4681] 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4717] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4753] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5\n#&gt;  [4789] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4825] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 3 3 3 3 3 3\n#&gt;  [4861] 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4897] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4933] 2 2 2 2 2 2 2 2 2 2 5 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4969] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5005] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 3 3 3 3 3 3 3 3 3 3 2 2 3 2 2 2 2\n#&gt;  [5041] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5077] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5113] 5 5 2 2 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5149] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5185] 5 5 5 5 5 5 5 2 2 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5221] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5257] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 2 5 5 5 5 5\n#&gt;  [5293] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5329] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2\n#&gt;  [5365] 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5401] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5437] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5473] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5509] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5545] 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5581] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5617] 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5653] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5689] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2\n#&gt;  [5725] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5761] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5\n#&gt;  [5797] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5833] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5869] 5 5 5 5 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5905] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5941] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5977] 5 5 5 5 2 2 2 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6013] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 3 3 3 3\n#&gt;  [6049] 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6085] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6121] 2 4 2 2 2 2 2 2 2 2 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2\n#&gt;  [6157] 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6193] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6229] 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6265] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6301] 2 5 2 2 2 2 2 5 2 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5\n#&gt;  [6337] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6373] 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n#&gt;  [6409] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6445] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 5\n#&gt;  [6481] 2 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6517] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6553] 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6589] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6625] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 5 2 2 2 2 5 5 5 5\n#&gt;  [6661] 5 5 5 4 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6697] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6733] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6769] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6805] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6841] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6877] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6913] 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6949] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6985] 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7021] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7057] 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 2 2\n#&gt;  [7093] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7129] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7165] 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7201] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 5\n#&gt;  [7237] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7273] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7309] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 5 5 5 5 5\n#&gt;  [7345] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7381] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 5 5 3 3 3 3 3 3 3 3 3\n#&gt;  [7417] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7453] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7489] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7525] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7561] 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7597] 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7633] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7669] 2 2 2 2 2 2 2 2 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7705] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7741] 3 3 3 3 3 3 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n#&gt;  [7777] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7813] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5\n#&gt;  [7849] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7885] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 5 5 3 3 5 5 5 3\n#&gt;  [7921] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 2\n#&gt;  [7957] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7993] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8029] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8065] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5 5 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8101] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8137] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8173] 2 2 2 2 2 2 2 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8209] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8245] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8281] 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8317] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5\n#&gt;  [8353] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8389] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8425] 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2\n#&gt;  [8461] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8497] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8533] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8569] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3\n#&gt;  [8605] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8641] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8677] 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 4 5 5 5 4 2 2 4 4 4 5 5 5 5 5 5 4 5 5 5 5\n#&gt;  [8713] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8749] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8785] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8821] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 2 2 2 2\n#&gt;  [8857] 2 2 2 2 2 4 4 4 4 2 2 2 2 2 2 2 5 5 4 4 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8893] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8929] 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8965] 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9001] 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9037] 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9073] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9109] 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2\n#&gt;  [9145] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9181] 2 2 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9217] 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9253] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3\n#&gt;  [9289] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9325] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4\n#&gt;  [9361] 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9397] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9433] 5 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9469] 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9505] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2\n#&gt;  [9541] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9577] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3\n#&gt;  [9613] 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9649] 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9685] 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9721] 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9757] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 5 5 5 5 5 5 5 3 3\n#&gt;  [9793] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 2 2 2 2 2\n#&gt;  [9829] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2\n#&gt;  [9865] 2 2 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5 5 5 5 5 5 5\n#&gt;  [9901] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5\n#&gt;  [9937] 5 5 5 5 5 5 5 5 5 5 3 5 5 3 3 3 3 3 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9973] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10009] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 4 4 4 4 4 4 4\n#&gt; [10045] 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10081] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10117] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10153] 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10189] 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2\n#&gt; [10225] 2 2 2 2 2 2 2 2 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10261] 5 5 5 5 5 5 5 5 5 5 5 5 2 3 3 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3\n#&gt; [10297] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10333] 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10369] 2 2 4 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 4 5\n#&gt; [10405] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10441] 5 5 2 3 3 3 5 3 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10477] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2\n#&gt; [10513] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 4 2 2 2 2\n#&gt; [10549] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 4 5 5 5 5 5 5 5 5 5 5\n#&gt; [10585] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 5\n#&gt; [10621] 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10657] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10693] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 2 2 2 2 2 2 4 4 4 4 4 4 4 4\n#&gt; [10729] 4 4 4 4 4 4 4 2 2 2 2 2 2 2 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10765] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 5 3 3 3 3 3 3 3 3 3\n#&gt; [10801] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10837] 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10873] 2 2 2 2 2 2 2 2 4 4 2 2 4 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2\n#&gt; [10909] 2 2 2 2 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10945] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10981] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11017] 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4\n#&gt; [11053] 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 4 4 5 5 5 5\n#&gt; [11089] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11125] 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11161] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2\n#&gt; [11197] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 4 2 2 2 2\n#&gt; [11233] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11269] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 3 3 3\n#&gt; [11305] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11341] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11377] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 4 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4\n#&gt; [11413] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5\n#&gt; [11449] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11485] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11521] 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11557] 2 2 2 2 4 2 2 2 2 4 4 4 4 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11593] 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11629] 5 5 5 5 5 5 5 3 3 5 5 1 5 5 5 3 3 3 3 3 3 3 3 3 5 5 3 3 3 3 3 3 3 3 3 3\n#&gt; [11665] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n#&gt; [11701] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 4\n#&gt; [11737] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 4 5 5\n#&gt; [11773] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3\n#&gt; [11809] 5 5 1 5 5 5 3 3 3 3 3 3 3 3 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11845] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11881] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11917] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 5 5 1 1 1 5 5 5 5 5\n#&gt; [11953] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 5 4 5 5 5 3 3 3 3\n#&gt; [11989] 3 3 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12025] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12061] 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12097] 4 4 4 4 4 4 4 4 4 4 1 1 4 4 1 1 5 5 1 1 1 1 5 5 4 4 1 1 5 5 5 5 5 5 5 5\n#&gt; [12133] 5 5 5 5 5 5 2 2 2 2 3 3 3 3 3 3 3 5 4 4 5 5 5 3 3 3 3 3 5 5 5 5 5 5 3 3\n#&gt; [12169] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12205] 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12241] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12277] 1 1 4 1 1 1 5 5 1 1 1 5 5 5 1 1 1 1 1 4 4 4 4 5 5 5 5 5 5 5 5 5 5 3 3 3\n#&gt; [12313] 3 3 3 3 3 3 3 3 4 1 5 5 5 5 3 3 3 3 3 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12349] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2\n#&gt; [12385] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4\n#&gt; [12421] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 4 4 1 1 4 1 1 1 4 5 1 1 1\n#&gt; [12457] 1 1 1 1 1 1 4 1 1 1 5 5 5 4 5 5 5 5 5 4 5 5 5 2 3 3 3 3 3 3 3 3 3 3 3 5\n#&gt; [12493] 1 5 5 5 3 3 3 3 3 3 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12529] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12565] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12601] 4 4 4 4 4 4 4 4 4 1 1 1 1 1 4 1 4 4 1 1 1 1 4 4 1 1 1 1 1 1 1 4 4 1 1 1\n#&gt; [12637] 1 4 4 1 1 4 4 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 4 5 3 3 3 3 3\n#&gt; [12673] 3 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12709] 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12745] 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12781] 4 1 1 1 1 1 4 4 1 1 1 1 5 1 1 1 1 4 4 1 1 1 1 4 1 1 1 1 1 1 1 1 4 5 5 4\n#&gt; [12817] 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 1 4 4 5 5 3 3 3 3 3 3 5 4 4 5 3 3 3 3\n#&gt; [12853] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12889] 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4\n#&gt; [12925] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 1 1 1 4 1 4 4 1\n#&gt; [12961] 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 4 5 5 5 5 5 3 3 3 3 3 3 3 3\n#&gt; [12997] 3 3 3 3 3 3 3 5 1 1 1 5 3 3 3 3 3 3 3 4 5 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13033] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2\n#&gt; [13069] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13105] 4 4 4 4 4 4 1 4 4 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 4 4 4 1 1 1 1 1 4 1 1 1\n#&gt; [13141] 5 5 4 1 1 1 1 1 1 1 1 1 1 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 1\n#&gt; [13177] 1 5 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13213] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13249] 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4\n#&gt; [13285] 4 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 1 1 1 1 4 4 5 4 1 1 5 5 5 5 4 1 1 1 1 1\n#&gt; [13321] 4 4 4 4 4 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 3 3 3 3 3 3 3 3\n#&gt; [13357] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13393] 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13429] 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1\n#&gt; [13465] 1 1 4 4 1 1 1 4 1 4 4 4 5 5 4 4 4 1 1 4 4 4 1 1 1 1 4 5 4 4 5 3 3 3 3 3\n#&gt; [13501] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13537] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2\n#&gt; [13573] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4\n#&gt; [13609] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 1 1 1 1 1 1 1 1 4 1 4 4 1 1 1 1 4 4\n#&gt; [13645] 4 4 1 4 5 5 4 1 1 4 4 1 5 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13681] 3 3 3 3 3 3 4 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13717] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13753] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13789] 4 4 4 4 4 4 4 4 1 4 1 1 4 4 1 1 4 1 4 1 1 1 1 1 1 1 1 1 1 1 4 5 4 1 1 4\n#&gt; [13825] 1 1 4 4 4 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 1 5 5 3\n#&gt; [13861] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13897] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13933] 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13969] 4 4 4 4 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 1 1 1 1 1 1 1 4 4 4 4 4 4 5\n#&gt; [14005] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 1 4 4 4 5 4 5 3 3 3 3 3 3 3\n#&gt; [14041] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14077] 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 4\n#&gt; [14113] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1\n#&gt; [14149] 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 1 1 1 1 1 4 4 4 3 3 3 3 3 3 3 3\n#&gt; [14185] 3 3 3 3 3 3 3 3 3 3 3 1 4 4 1 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14221] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2\n#&gt; [14257] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 4 4 4 4 4 4 4 4 4\n#&gt; [14293] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 4 1 1 4 1 4 4 4 4 1 1\n#&gt; [14329] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14365] 3 1 1 1 4 4 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14401] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14437] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14473] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 1 4 4 1 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14509] 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 3 3 3 3 3 3\n#&gt; [14545] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14581] 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14617] 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14653] 4 4 4 1 4 4 4 4 1 4 4 4 4 4 4 4 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4\n#&gt; [14689] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14725] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14761] 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3\n#&gt; [14797] 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14833] 4 4 4 4 4 4 1 1 4 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3\n#&gt; [14869] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14905] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2\n#&gt; [14941] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 4 4 4 3 3 4 4 4 4\n#&gt; [14977] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15013] 4 4 4 1 1 1 1 1 1 1 1 1 1 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15049] 3 3 3 4 4 4 4 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15085] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15121] 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15157] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1\n#&gt; [15193] 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 1 4 3 4 4\n#&gt; [15229] 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15265] 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15301] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15337] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3\n#&gt; [15373] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 1 1 4 4 4 3 3 3 3 3 3 3 3 3 3\n#&gt; [15409] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2\n#&gt; [15445] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 4 4 3 3\n#&gt; [15481] 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15517] 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15553] 3 3 3 3 3 3 3 3 4 1 1 1 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15589] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15625] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 4 4 3 3 3 4 4 4 4 4 4 4 4 4\n#&gt; [15661] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15697] 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4\n#&gt; [15733] 1 1 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15769] 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15805] 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15841] 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15877] 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 1 1 4 3 3 3 3 3 3\n#&gt; [15913] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15949] 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n#&gt; [15985] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 3 3 3 3 3 3 3 3 3 4 4 4\n#&gt; [16021] 3 3 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 1 1 1 1 1 1 1 1 1 4 4 4 3 3 3 3 3 3\n#&gt; [16057] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16093] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2\n#&gt; [16129] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16165] 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 4 4 4 3 4 4 4 4 4 4 4 3 3\n#&gt; [16201] 4 4 4 4 4 1 1 4 1 1 1 1 1 1 1 1 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16237] 3 3 3 3 3 4 1 4 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16273] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16309] 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16345] 3 3 3 3 3 3 3 4 4 3 3 3 3 3 3 3 3 4 3 4 3 3 3 3 3 4 4 4 4 4 4 4 4 4 1 1\n#&gt; [16381] 1 1 1 1 1 1 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 1 1 1 4 4\n#&gt; [16417] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16453] 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16489] 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16525] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 1 1 4 1 1 1 1 1 1 1 4 3\n#&gt; [16561] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 1 1 1 4 4 4 3 3 3 3 3 3 3 3 3 3\n#&gt; [16597] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n#&gt; [16633] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n#&gt; [16669] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3\n#&gt; [16705] 3 3 4 3 4 4 4 3 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 4 4 4 3 3 3 3 3 3 3 3 3 3\n#&gt; [16741] 3 3 3 3 3 3 3 3 4 1 1 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16777] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16813] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 3785.104 5524.327 4545.222 3516.041 7478.029\n#&gt;  (between_SS / total_SS =  70.5 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/week-4.html#section-14",
    "href": "slides/week-4.html#section-14",
    "title": "Week 4",
    "section": "",
    "text": "par(mfrow = c(2,1))\nplot(elev)\nplot(co$ppt)\n\n\n\n\n\n\n\n\n\n\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))"
  },
  {
    "objectID": "slides/week-4.html#extract",
    "href": "slides/week-4.html#extract",
    "title": "Week 4",
    "section": "extract",
    "text": "extract"
  },
  {
    "objectID": "slides/week-4.html#read-in-the-saved-raster-file",
    "href": "slides/week-4.html#read-in-the-saved-raster-file",
    "title": "Week 4",
    "section": "Read in the saved raster file",
    "text": "Read in the saved raster file\n\n(r = rast(\"data/foco-elev.tif\"))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source      : foco-elev.tif \n#&gt; name        : foco-elev \n#&gt; min value   :      1382 \n#&gt; max value   :      4346"
  },
  {
    "objectID": "slides/week-4.html#create-a-conditional-threshold-mask",
    "href": "slides/week-4.html#create-a-conditional-threshold-mask",
    "title": "Week 4",
    "section": "Create a conditional (threshold) mask",
    "text": "Create a conditional (threshold) mask\n\nthreshold = function(x) {ifelse(x &lt;= 1520 , NA, 1)}\n\n\nthreshold(1600)\n#&gt; [1] 1\nthreshold(-100)\n#&gt; [1] NA\n\n\n(m = app(r, threshold))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; name        : lyr.1 \n#&gt; min value   :     1 \n#&gt; max value   :     1"
  },
  {
    "objectID": "slides/week-4.html#section-15",
    "href": "slides/week-4.html#section-15",
    "title": "Week 4",
    "section": "",
    "text": "plot(r)\nplot(river, add = TRUE, col = \"blue\", lwd = 2)\nplot(line, add = TRUE, col = \"black\", lwd = 2)\nplot(outlet$geometry, add = TRUE, pch = 16, col = \"red\")\nplot(inlet$geometry, add = TRUE, pch = 16, col = \"green\")"
  },
  {
    "objectID": "slides/week-4.html#multiply-cell-wise",
    "href": "slides/week-4.html#multiply-cell-wise",
    "title": "Week 4",
    "section": "Multiply cell-wise",
    "text": "Multiply cell-wise\n\nalgebraic, logical, and functional operations act on a raster cell-wise\n\n\nelev_cut = m * r\nplot(elev_cut, col = viridis::viridis(256))"
  },
  {
    "objectID": "slides/week-4.html#reclassify-1",
    "href": "slides/week-4.html#reclassify-1",
    "title": "Week 4",
    "section": "Reclassify",
    "text": "Reclassify\n\n(rcl = data.frame(min = seq(1500,1590,10), max =  seq(1510,1600,10), lab = c(0:9)))\n#&gt;     min  max lab\n#&gt; 1  1500 1510   0\n#&gt; 2  1510 1520   1\n#&gt; 3  1520 1530   2\n#&gt; 4  1530 1540   3\n#&gt; 5  1540 1550   4\n#&gt; 6  1550 1560   5\n#&gt; 7  1560 1570   6\n#&gt; 8  1570 1580   7\n#&gt; 9  1580 1590   8\n#&gt; 10 1590 1600   9\n\n\n(rc = classify(elev_cut, rcl, include.lowest = TRUE))\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; name        : lyr.1 \n#&gt; min value   :     2 \n#&gt; max value   :  4346"
  },
  {
    "objectID": "slides/week-4.html#section-16",
    "href": "slides/week-4.html#section-16",
    "title": "Week 4",
    "section": "",
    "text": "(E = kmeans(vs, 5, iter.max = 100))\n#&gt; K-means clustering with 5 clusters of sizes 4308, 2774, 2766, 582, 6400\n#&gt; \n#&gt; Cluster means:\n#&gt;           ppt        tmax       tmin       srad          q\n#&gt; 1  0.50370218 -0.07799912 -1.0490021 -1.0622712 -1.1084656\n#&gt; 2 -0.05979203 -0.13294388  0.6074999 -0.4043326 -0.6171530\n#&gt; 3 -0.13698246 -0.17036199  1.4333914  0.8283695  0.9769278\n#&gt; 4  3.17949220  4.61554662 -0.5704863 -1.7480514 -1.2250697\n#&gt; 5 -0.54307139 -0.23597193 -0.1248189  0.6912467  0.7028219\n#&gt; \n#&gt; Clustering vector:\n#&gt;     [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;    [37] 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;    [73] 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5\n#&gt;   [109] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [145] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2\n#&gt;   [181] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1\n#&gt;   [217] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n#&gt;   [253] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [289] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [325] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [361] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [397] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [433] 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [469] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [505] 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [541] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [577] 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5\n#&gt;   [613] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [649] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2\n#&gt;   [685] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n#&gt;   [721] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt;   [757] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [793] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [829] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2\n#&gt;   [865] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [901] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n#&gt;   [937] 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;   [973] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1009] 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1045] 2 5 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1081] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 5 5 5 5 5 5\n#&gt;  [1117] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1153] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1189] 5 5 2 2 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 2 2 2 2 2 2 2 2 2\n#&gt;  [1225] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1261] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1297] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1333] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 5 5 5 5\n#&gt;  [1369] 5 5 5 2 2 2 5 5 5 2 2 2 2 2 2 5 5 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt;  [1405] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1441] 1 1 1 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1477] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1513] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 5 5 5 5 5 5 5 5 2 2 2 5 5 2\n#&gt;  [1549] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1585] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n#&gt;  [1621] 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1657] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1693] 5 5 5 5 5 5 5 5 2 2 2 5 5 2 5 5 5 5 5 5 5 2 5 5 5 5 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1729] 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1765] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5\n#&gt;  [1801] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1837] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2\n#&gt;  [1873] 5 5 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1\n#&gt;  [1909] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1945] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [1981] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2017] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2\n#&gt;  [2053] 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2089] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2125] 1 1 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2161] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2197] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2233] 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2269] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 5 5\n#&gt;  [2305] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2341] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2377] 5 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 1 1 1 1\n#&gt;  [2413] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2449] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2485] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2521] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2\n#&gt;  [2557] 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2629] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2665] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2701] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 1 2 1 2 5 5 5 5 5 5\n#&gt;  [2737] 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2773] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n#&gt;  [2809] 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2845] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [2881] 5 5 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 1 2 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1\n#&gt;  [2917] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2953] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5\n#&gt;  [2989] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3025] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3061] 5 5 5 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3097] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3133] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3169] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3205] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3241] 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3277] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3313] 1 1 1 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3349] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3385] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3421] 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3457] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 5 5\n#&gt;  [3493] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3529] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3565] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1\n#&gt;  [3601] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3637] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3673] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3709] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3745] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3781] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt;  [3817] 2 2 2 1 1 1 1 1 1 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3853] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3889] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [3925] 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3961] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2\n#&gt;  [3997] 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4033] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4069] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1\n#&gt;  [4105] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4141] 1 1 1 1 2 2 2 2 2 2 2 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 2 2 2 5 5 5 5 5 5\n#&gt;  [4177] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4213] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4249] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4285] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n#&gt;  [4321] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 5 5 5 5 3 3 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4357] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4393] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4429] 5 5 5 1 1 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4465] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4501] 1 1 1 1 1 2 2 2 2 2 2 5 5 5 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4537] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4573] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 5 5 5 5\n#&gt;  [4609] 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4645] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n#&gt;  [4681] 2 5 5 5 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4717] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4753] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 5 5 5 5 5 1 1 1 1 1 1 1 1 1\n#&gt;  [4789] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1\n#&gt;  [4825] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 5 5 3 3 3 3 3 3\n#&gt;  [4861] 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4897] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [4933] 5 5 5 5 5 5 5 5 5 1 1 1 1 5 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4969] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5005] 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 5 5 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5\n#&gt;  [5041] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5077] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5113] 1 1 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt;  [5185] 2 2 2 2 2 2 2 2 5 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5221] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5257] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5293] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5329] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 5 5\n#&gt;  [5365] 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5401] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5437] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5473] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5509] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 5 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5545] 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5581] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5617] 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5653] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5689] 1 1 1 1 1 2 2 1 2 2 2 2 2 2 2 5 5 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5\n#&gt;  [5725] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5761] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1\n#&gt;  [5797] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt;  [5833] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n#&gt;  [5869] 2 2 2 2 2 5 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5905] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [5941] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5977] 1 1 1 1 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6013] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3\n#&gt;  [6049] 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6085] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6121] 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5\n#&gt;  [6157] 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6193] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 5 5 5\n#&gt;  [6229] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6265] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6301] 5 1 1 1 5 5 5 1 5 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1\n#&gt;  [6337] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6373] 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6409] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6445] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 5 5 5 5 5 5 1\n#&gt;  [6481] 1 1 1 5 5 5 5 5 5 5 1 1 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6517] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n#&gt;  [6553] 2 2 2 3 3 3 3 3 3 3 3 3 3 3 5 5 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6589] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6625] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 5 5 5 5 5 5 1 1 5 5 5 1 1 1 1\n#&gt;  [6661] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6697] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3\n#&gt;  [6733] 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6769] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6805] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6841] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6877] 1 1 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 5 5 5 5 5\n#&gt;  [6913] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6949] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [6985] 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7021] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1\n#&gt;  [7057] 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 2 3 5 5 5 5 5 3 5 5 5 5 5 5 5 5\n#&gt;  [7093] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7129] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7165] 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7201] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2\n#&gt;  [7237] 2 2 3 2 2 2 2 2 2 2 2 2 2 2 5 5 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7273] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7309] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1\n#&gt;  [7345] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7381] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7417] 2 2 2 2 2 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7453] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7489] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7525] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n#&gt;  [7561] 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 5\n#&gt;  [7597] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7633] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7669] 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7705] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n#&gt;  [7741] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7777] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7813] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1\n#&gt;  [7849] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7885] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7921] 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7957] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [7993] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8029] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8065] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8101] 2 2 2 3 3 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8137] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8173] 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8209] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n#&gt;  [8245] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 5 5 5 5\n#&gt;  [8281] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8317] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1\n#&gt;  [8353] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8389] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8425] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8461] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8497] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8533] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8569] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8605] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8641] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8677] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8713] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 2 2\n#&gt;  [8749] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 3 3 3 3 3\n#&gt;  [8785] 3 2 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [8821] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 5 5 5 5\n#&gt;  [8857] 5 5 5 5 5 1 1 1 5 5 5 5 5 5 5 5 1 1 1 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8893] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8929] 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5\n#&gt;  [8965] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9001] 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9037] 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9073] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1\n#&gt;  [9109] 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9145] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9181] 5 5 5 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9217] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n#&gt;  [9253] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 3 3 3 3 3 3 3\n#&gt;  [9289] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9325] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1\n#&gt;  [9361] 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9397] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9433] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9469] 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9505] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 5 5 5 5 5 5 5\n#&gt;  [9541] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9577] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9613] 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5\n#&gt;  [9649] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9685] 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9721] 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9757] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n#&gt;  [9793] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5\n#&gt;  [9829] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt;  [9865] 5 5 5 5 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1\n#&gt;  [9901] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n#&gt;  [9937] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9973] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10009] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1\n#&gt; [10045] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10081] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2\n#&gt; [10117] 2 2 2 2 2 2 3 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10153] 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10189] 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5\n#&gt; [10225] 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10261] 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2\n#&gt; [10297] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10333] 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10369] 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1\n#&gt; [10405] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt; [10441] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10477] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5\n#&gt; [10513] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5\n#&gt; [10549] 5 5 5 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10585] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt; [10621] 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10657] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10693] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1\n#&gt; [10729] 1 1 1 1 1 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10765] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3\n#&gt; [10801] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10837] 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [10873] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 5 5 5\n#&gt; [10909] 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10945] 1 1 1 1 1 1 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10981] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11017] 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11053] 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 5 5 5 5 5 5 1 1 1 1 1\n#&gt; [11089] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n#&gt; [11125] 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11161] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5\n#&gt; [11197] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11233] 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11269] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 2 2 2 1 1 2 2 2 2 2 2\n#&gt; [11305] 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11341] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11377] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1\n#&gt; [11413] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11449] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11485] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11521] 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11557] 5 5 5 5 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11593] 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11629] 1 1 1 1 2 2 2 2 2 2 1 4 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3\n#&gt; [11665] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11701] 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 5\n#&gt; [11737] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11773] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2\n#&gt; [11809] 2 1 4 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11845] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5\n#&gt; [11881] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 5 3 3 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [11917] 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 1 1 1 4 4 4 1 1 1 1 1\n#&gt; [11953] 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2\n#&gt; [11989] 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12025] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [12061] 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1\n#&gt; [12097] 1 1 1 1 1 1 1 1 1 1 4 4 1 1 4 4 1 1 4 4 4 4 1 1 1 1 4 4 1 1 1 1 1 1 1 1\n#&gt; [12133] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n#&gt; [12169] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12205] 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [12241] 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12277] 4 4 1 4 4 4 1 1 4 4 4 1 1 1 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n#&gt; [12313] 2 2 2 2 2 2 2 2 2 4 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12349] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5\n#&gt; [12385] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 3 3 3 3 3 5 5 5\n#&gt; [12421] 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 1 1 4 4 4 4 4 4 1 1 4 4 4\n#&gt; [12457] 4 4 4 4 4 4 1 4 4 4 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12493] 4 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12529] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5\n#&gt; [12565] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 2 3 2 3 3 3 3 5 5 5 5 5 2 2 1 1 1 1 1\n#&gt; [12601] 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 1 1 4 4 4 4 4 1 4 4 4 4 4 4 4 1 1 4 4 4\n#&gt; [12637] 4 1 1 4 4 1 4 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 2 2 2 2 2\n#&gt; [12673] 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12709] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [12745] 5 5 5 5 5 5 3 2 2 2 2 2 3 3 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12781] 1 4 4 4 4 4 1 1 4 4 4 4 1 4 4 4 4 1 1 4 4 4 4 1 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt; [12817] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3\n#&gt; [12853] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12889] 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 2 2\n#&gt; [12925] 2 2 2 2 2 2 5 5 5 2 1 1 1 1 1 4 4 1 1 1 1 1 1 1 1 4 4 1 4 4 4 1 4 1 4 4\n#&gt; [12961] 4 4 4 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt; [12997] 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13033] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5\n#&gt; [13069] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 2 2 2 2 2 2 2 2 2 5 5\n#&gt; [13105] 2 1 1 1 1 1 4 4 4 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4 4 4 1 4 4 4\n#&gt; [13141] 1 1 1 4 4 4 4 4 4 4 4 4 4 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4\n#&gt; [13177] 4 2 2 2 2 2 2 3 3 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13213] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [13249] 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1 1 1 4 1 1\n#&gt; [13285] 1 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 4 4 4 1 1 1 4 4 1 1 1 1 1 4 4 4 4 4\n#&gt; [13321] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3\n#&gt; [13357] 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13393] 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [13429] 5 5 3 3 3 3 3 3 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4\n#&gt; [13465] 4 4 4 4 4 4 4 1 4 4 1 1 1 1 1 1 1 4 4 1 1 1 4 4 4 4 2 2 2 2 2 2 2 2 2 2\n#&gt; [13501] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13537] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13573] 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 2\n#&gt; [13609] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 4 4 4 4 4 4 4 4 1 4 1 1 4 4 4 4 1 1\n#&gt; [13645] 1 1 4 1 1 1 1 4 4 1 1 4 1 1 1 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13681] 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13717] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5\n#&gt; [13753] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 2 2 1 1 1 2 2 1 1\n#&gt; [13789] 1 1 1 1 1 4 4 1 4 1 4 4 4 1 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4\n#&gt; [13825] 4 4 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2\n#&gt; [13861] 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13897] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [13933] 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13969] 1 1 1 1 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 1 4 2 2 2\n#&gt; [14005] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 3 3 3 3\n#&gt; [14041] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14077] 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3\n#&gt; [14113] 3 3 3 3 3 3 3 3 3 3 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4\n#&gt; [14149] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14185] 2 2 2 2 2 2 2 2 2 2 2 4 1 1 4 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14221] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3\n#&gt; [14257] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14293] 3 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 1 4 4 1 4 1 1 1 1 4 4\n#&gt; [14329] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14365] 2 4 4 4 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14401] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [14437] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 1 1 1 1\n#&gt; [14473] 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 4 1 1 4 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14509] 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14545] 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14581] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [14617] 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14653] 1 1 1 4 1 1 1 1 4 1 1 1 1 1 1 1 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2\n#&gt; [14689] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n#&gt; [14725] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14761] 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3\n#&gt; [14797] 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 1 1 4 4 4 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14833] 1 1 1 1 1 1 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2\n#&gt; [14869] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14905] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5\n#&gt; [14941] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14977] 3 3 2 2 2 1 1 1 1 4 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1\n#&gt; [15013] 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15049] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15085] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [15121] 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 1 1 1 1\n#&gt; [15157] 1 2 2 2 2 1 1 2 1 2 1 1 1 1 1 1 2 1 1 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4\n#&gt; [15193] 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2\n#&gt; [15229] 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15265] 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [15301] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15337] 2 2 2 1 1 2 1 1 1 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2\n#&gt; [15373] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 3 3 3 3 3 3 3\n#&gt; [15409] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5\n#&gt; [15445] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3\n#&gt; [15481] 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n#&gt; [15517] 2 2 2 2 2 2 2 1 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15553] 2 2 2 2 2 2 2 2 2 4 4 4 4 4 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15589] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [15625] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15661] 3 3 2 2 2 2 2 2 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2\n#&gt; [15697] 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 3 3 2 3 2 2 2 2 2 2 2 2 2\n#&gt; [15733] 4 4 1 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15769] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [15805] 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3\n#&gt; [15841] 3 3 3 3 3 3 3 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15877] 4 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 3 3 3 3\n#&gt; [15913] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15949] 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3\n#&gt; [15985] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n#&gt; [16021] 3 3 3 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2\n#&gt; [16057] 2 2 2 3 3 3 3 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16093] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5\n#&gt; [16129] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16165] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 2 2 2 3 2 2 3 3\n#&gt; [16201] 2 2 2 2 2 4 4 2 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3\n#&gt; [16237] 2 2 2 2 2 2 4 1 4 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16273] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [16309] 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16345] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 4 4\n#&gt; [16381] 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 4 4 4 2 2\n#&gt; [16417] 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16453] 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [16489] 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16525] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 2 2\n#&gt; [16561] 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 4 4 4 2 2 2 2 2 2 2 3 3 3 3 3 3\n#&gt; [16597] 3 3 3 3 3 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16633] 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3\n#&gt; [16669] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [16705] 3 3 3 3 3 3 2 3 3 2 2 2 2 2 2 2 4 2 4 4 4 4 4 2 2 2 2 2 2 3 3 3 3 3 3 3\n#&gt; [16741] 3 3 3 3 3 3 2 2 2 4 4 4 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3\n#&gt; [16777] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5\n#&gt; [16813] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 8707.478 4246.217 2973.004 4165.963 5405.927\n#&gt;  (between_SS / total_SS =  69.7 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/week-4.html#section-17",
    "href": "slides/week-4.html#section-17",
    "title": "Week 4",
    "section": "",
    "text": "par(mfrow = c(2,1))\nplot(elev)\nplot(co$ppt)\n\n\n\n\n\n\n\n\n\n\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))"
  },
  {
    "objectID": "slides/week-4.html#objectraster-interaction",
    "href": "slides/week-4.html#objectraster-interaction",
    "title": "Week 4",
    "section": "Object/Raster Interaction",
    "text": "Object/Raster Interaction\n\nObjects can be used to extract values from a raster!\nLets get some objects from OpenStreetMap\nOSM provides the largest, most up to date digital representation of the earths features\nLearning to identify and extract OSM data will provide you with more data to anwser questions then you weill ever need!\nLets say we want the river channels in the Fort Collins Area‚Ä¶"
  },
  {
    "objectID": "slides/week-4.html#openstreetmap-osm-tags",
    "href": "slides/week-4.html#openstreetmap-osm-tags",
    "title": "Week 4",
    "section": "OpenStreetMap (OSM) tags",
    "text": "OpenStreetMap (OSM) tags\n\nA OSM tag consists of two items:\n\na ‚Äúkey‚Äù and a ‚Äúvalue.‚Äù\nTags describe specific features of map elements (nodes, ways, or relations).\nTags are presented ‚Äúfor humans‚Äù as key=value: separated by an equals sign.\n\nThe key is used to describe a topic, category, or type of feature (e.g., highway or name).\nKeys can be qualified with prefixes, infixes, or suffixes (usually, separated with a colon, :), forming super- or sub-categories\nThe value provides detail about a key-specified feature. Commonly, values are:\n\nfree form text: name=‚ÄúHollistor Road‚Äù\none of a set of distinct values: highway=motorway,\nmultiple values (separated by a semicolon) (e.g.¬†motorcycle:rental=yes)\n\nSo for a river, if we google OSM river ‚Ä¶"
  },
  {
    "objectID": "slides/week-4.html#lets-get-the-waterway-data-in-r",
    "href": "slides/week-4.html#lets-get-the-waterway-data-in-r",
    "title": "Week 4",
    "section": "Lets get the waterway data in R!",
    "text": "Lets get the waterway data in R!\n\nOSM has global coverage, we need to limit the results to Fort Collins\nGoltea is our ‚ÄòAOI‚Äô, and can be defined by its bbox or extent.\nSince OSM is global, all data is in the WGS84 Geographic CRS (EPSG:4326)\n\n\n(bb  = st_bbox(s) |&gt; st_as_sfc() |&gt; st_transform(4326))\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -105.7188 ymin: 40.0597 xmax: -104.4198 ymax: 41.03357\n#&gt; Geodetic CRS:  WGS 84"
  },
  {
    "objectID": "slides/week-4.html#section-19",
    "href": "slides/week-4.html#section-19",
    "title": "Week 4",
    "section": "",
    "text": "par(mfrow = c(2,1))\nplot(elev)\nplot(co$ppt)\n\n\n\n\n\n\n\n\n\n\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))"
  },
  {
    "objectID": "slides/week-4.html#but-we-want-features",
    "href": "slides/week-4.html#but-we-want-features",
    "title": "Week 4",
    "section": "But‚Ä¶ we want features!",
    "text": "But‚Ä¶ we want features!\nSpecifically features that match the tag waterway=stream\n\n(osm = osmdata::opq(bb) |&gt; \n  add_osm_feature(key = 'waterway', value = \"stream\") )\n#&gt; $bbox\n#&gt; [1] \"40.0596958764483,-105.71879494006,41.0335747063874,-104.419776125118\"\n#&gt; \n#&gt; $prefix\n#&gt; [1] \"[out:xml][timeout:25];\\n(\\n\"\n#&gt; \n#&gt; $suffix\n#&gt; [1] \");\\n(._;&gt;;);\\nout body;\"\n#&gt; \n#&gt; $features\n#&gt; [1] \"[\\\"waterway\\\"=\\\"stream\\\"]\"\n#&gt; \n#&gt; $osm_types\n#&gt; [1] \"node\"     \"way\"      \"relation\"\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"list\"           \"overpass_query\"\n#&gt; attr(,\"nodes_only\")\n#&gt; [1] FALSE"
  },
  {
    "objectID": "slides/week-4.html#section-20",
    "href": "slides/week-4.html#section-20",
    "title": "Week 4",
    "section": "",
    "text": "With a bounding box, we can use the osmdata library to format OSM (overpass API) queries:\n\n(osm = osmdata::opq(bb))\n#&gt; $bbox\n#&gt; [1] \"40.4989011988385,-105.130466261988,40.5963424259927,-105.000806842117\"\n#&gt; \n#&gt; $prefix\n#&gt; [1] \"[out:xml][timeout:25];\\n(\\n\"\n#&gt; \n#&gt; $suffix\n#&gt; [1] \");\\n(._;&gt;;);\\nout body;\"\n#&gt; \n#&gt; $features\n#&gt; NULL\n#&gt; \n#&gt; $osm_types\n#&gt; [1] \"node\"     \"way\"      \"relation\"\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"list\"           \"overpass_query\"\n#&gt; attr(,\"nodes_only\")\n#&gt; [1] FALSE"
  },
  {
    "objectID": "slides/week-4.html#so-what-did-we-get",
    "href": "slides/week-4.html#so-what-did-we-get",
    "title": "Week 4",
    "section": "So what did we get?",
    "text": "So what did we get?\n\n(river = osm$osm_lines |&gt; \n   dplyr::select(osm_id, name, waterway))\n#&gt; Simple feature collection with 3071 features and 3 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -105.7771 ymin: 40.02204 xmax: -104.2319 ymax: 41.18559\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;            osm_id               name waterway                       geometry\n#&gt; 6143291   6143291               &lt;NA&gt;   stream LINESTRING (-105.4111 40.28...\n#&gt; 6143573   6143573          Ruby Wash   stream LINESTRING (-105.1765 40.99...\n#&gt; 6152827   6152827               &lt;NA&gt;   stream LINESTRING (-105.4115 40.28...\n#&gt; 6156166   6156166               &lt;NA&gt;   stream LINESTRING (-105.405 40.276...\n#&gt; 6159403   6159403               &lt;NA&gt;   stream LINESTRING (-105.4027 40.28...\n#&gt; 6159910   6159910               &lt;NA&gt;   stream LINESTRING (-105.4056 40.28...\n#&gt; 6164104   6164104               &lt;NA&gt;   stream LINESTRING (-105.6442 40.58...\n#&gt; 6168629   6168629               &lt;NA&gt;   stream LINESTRING (-105.6365 40.58...\n#&gt; 35017161 35017161 Black Canyon Creek   stream LINESTRING (-105.6114 40.46...\n#&gt; 37452417 37452417         Fern Creek   stream LINESTRING (-105.6756 40.33..."
  },
  {
    "objectID": "slides/week-4.html#lets-find-the-longest-river-segment-in-our-extent",
    "href": "slides/week-4.html#lets-find-the-longest-river-segment-in-our-extent",
    "title": "Week 4",
    "section": "Lets find the longest river segment IN our extent",
    "text": "Lets find the longest river segment IN our extent\n\nriver = foco_rivers |&gt; \n  st_transform(crs(r)) |&gt; \n  st_intersection(st_as_sfc(st_bbox(r))) %&gt;% \n  mutate(length = st_length(.)) |&gt; \n  slice_max(length, n = 1)\n\n\nplot(r)\nplot(river, add = TRUE, col = \"blue\", lwd = 2)"
  },
  {
    "objectID": "slides/week-4.html#value-extraction",
    "href": "slides/week-4.html#value-extraction",
    "title": "Week 4",
    "section": "Value Extraction",
    "text": "Value Extraction\n\nOften, we want to know the profile and sinousity of a river\nTo do this, we need to know the inlet and outlet as well as the straight line connector\n\n\ninlet  = head(st_cast(river, \"POINT\"), 1)\noutlet = tail(st_cast(river, \"POINT\"), 1) \npts    = bind_rows(inlet, outlet) \n\nline = st_cast(st_union(pts), \"LINESTRING\")"
  },
  {
    "objectID": "slides/week-4.html#section-21",
    "href": "slides/week-4.html#section-21",
    "title": "Week 4",
    "section": "",
    "text": "With a formatted query, we can extract the web-based data as sf objects\n\n(osm = osmdata::opq(bb) |&gt; \n  add_osm_feature(key = 'waterway', value = \"stream\") |&gt; \n  osmdata_sf())\n#&gt; Object of class 'osmdata' with:\n#&gt;                  $bbox : 40.4989011988385,-105.130466261988,40.5963424259927,-105.000806842117\n#&gt;         $overpass_call : The call submitted to the overpass API\n#&gt;                  $meta : metadata including timestamp and version numbers\n#&gt;            $osm_points : 'sf' Simple Features Collection with 3948 points\n#&gt;             $osm_lines : 'sf' Simple Features Collection with 206 linestrings\n#&gt;          $osm_polygons : 'sf' Simple Features Collection with 0 polygons\n#&gt;        $osm_multilines : NULL\n#&gt;     $osm_multipolygons : NULL\n\nprint(osm)\n#&gt; Object of class 'osmdata' with:\n#&gt;                  $bbox : 40.4989011988385,-105.130466261988,40.5963424259927,-105.000806842117\n#&gt;         $overpass_call : The call submitted to the overpass API\n#&gt;                  $meta : metadata including timestamp and version numbers\n#&gt;            $osm_points : 'sf' Simple Features Collection with 3948 points\n#&gt;             $osm_lines : 'sf' Simple Features Collection with 206 linestrings\n#&gt;          $osm_polygons : 'sf' Simple Features Collection with 0 polygons\n#&gt;        $osm_multilines : NULL\n#&gt;     $osm_multipolygons : NULL"
  },
  {
    "objectID": "slides/week-4.html#sinuosity",
    "href": "slides/week-4.html#sinuosity",
    "title": "Week 4",
    "section": "Sinuosity",
    "text": "Sinuosity\nChannel sinuosity is calculated by dividing the length of the stream channel by the straight line distance between the end points of the selected channel reach.\n\n(sin = st_length(river) / st_length(line))\n#&gt; 1.400515 [1]"
  },
  {
    "objectID": "slides/week-4.html#river-slope",
    "href": "slides/week-4.html#river-slope",
    "title": "Week 4",
    "section": "River Slope:",
    "text": "River Slope:\nThe change in elevation between the inlet/outlet divided by the length (rise/run) give us the slope of the river:\nTo calculate this, we must extract elevation values at the inlet and outlet:\n\n(elev = extract(r, pts))\n#&gt;   ID foco-elev\n#&gt; 1  1      1813\n#&gt; 2  2      1675\n\n\n100 * (elev$`foco-elev`[1] - elev$`foco-elev`[2]) / units::drop_units(st_length(river))\n#&gt; [1] 0.600446"
  },
  {
    "objectID": "slides/week-4.html#river-profile",
    "href": "slides/week-4.html#river-profile",
    "title": "Week 4",
    "section": "River profile",
    "text": "River profile\nWhat does the elevation profile of the river look like?\n\nprofile = extract(r, river)$`foco-elev`\n\n\nplot(profile, type = \"l\")\nlines(zoo::rollmean(profile,k = 10), \n      col = \"darkred\", lwd = 3)"
  },
  {
    "objectID": "slides/week-4.html#map-algebra",
    "href": "slides/week-4.html#map-algebra",
    "title": "Week 4",
    "section": "Map Algebra",
    "text": "Map Algebra\n\nDana Tomlin (Tomlin 1990) defined a framework for the analyizing field data stored as grided values.\nHe called this framework map algebra.\nMap algebra operations and functions are broken down into four types:\n\nlocal\nfocal\nzonal\nglobal"
  },
  {
    "objectID": "slides/week-4.html#local",
    "href": "slides/week-4.html#local",
    "title": "Week 4",
    "section": "Local",
    "text": "Local\n\nLocal operations and functions are applied to each individual cell and only involve those cells sharing the same location.\nMore than one raster can be involved in a local operation.\nFor example, rasters can be summed ( each overlapping pixels is added)\nLocal operations also include reclassification of values.\n\n\ns = c(mean(prcp$ppt), app(prcp$ppt, sd), min(prcp$ppt), max(prcp$ppt)) |&gt; \n  setNames(c(\"Mean\", \"StDev\", \"Min\", \"Max\"))\n\nrasterVis::levelplot(s)"
  },
  {
    "objectID": "slides/week-4.html#focal",
    "href": "slides/week-4.html#focal",
    "title": "Week 4",
    "section": "Focal",
    "text": "Focal\n\nAlso referred to as ‚Äúneighborhood‚Äù operations.\nAssigns summary values to the output cells based on the neighboring cells in the input raster.\nFor example, a cell output value can be the average of 9 neighboring input cells (including the center cell) - this acts as a smoothing function."
  },
  {
    "objectID": "slides/week-4.html#focal-1",
    "href": "slides/week-4.html#focal-1",
    "title": "Week 4",
    "section": "Focal",
    "text": "Focal\n\nFocal operations require a window (also known as a kernel) to work over\nAdditionally a kernel also defines the weight each neighboring cell contributes to the summary statistic.\nFor example, all cells in a 3x3 neighbor could each contribute 1/9th of their value to the summarized value (i.e.¬†equal weight).\nThe weight can take on a more complex form defined by a function; such weights are defined by a kernel function.\nOne popular function is a Gaussian weighted function which assigns greater weight to nearby cells than those further away (Toblers first law)"
  },
  {
    "objectID": "slides/week-4.html#example-focal",
    "href": "slides/week-4.html#example-focal",
    "title": "Week 4",
    "section": "Example: Focal",
    "text": "Example: Focal\nLets apply a smoothing kernel to our Fort Collins elevation data over an 25x25 window, using the mean operator\n\nfoco = AOI::geocode(\"Fort Collins\", bbox = TRUE) |&gt; st_transform(crs(r))\n\nfoco_elev = crop(r, foco)\nf1 &lt;- focal(foco_elev, w= matrix(1,nrow=25,ncol=25), fun=mean)"
  },
  {
    "objectID": "slides/week-4.html#results",
    "href": "slides/week-4.html#results",
    "title": "Week 4",
    "section": "Results",
    "text": "Results\n\n\n\nplot(r)\n\n\n\n\n\n\n\n\n\n\nplot(m)"
  },
  {
    "objectID": "slides/week-4.html#what-did-we-do",
    "href": "slides/week-4.html#what-did-we-do",
    "title": "Week 4",
    "section": "What did we do?",
    "text": "What did we do?\n\n\n\nmatrix(1,nrow=25,ncol=25)\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n#&gt;  [1,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [2,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [3,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [4,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [5,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [6,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [7,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [8,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;  [9,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [10,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [11,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [12,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [13,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [14,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [16,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [17,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [18,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [19,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [20,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [21,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [22,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [23,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [24,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt; [25,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n#&gt;       [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\n#&gt;  [1,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [2,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [3,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [4,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [5,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [6,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [7,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [8,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  [9,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [10,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [11,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [12,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [13,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [14,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [15,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [16,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [17,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [18,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [19,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [20,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [21,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [22,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [23,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [24,]     1     1     1     1     1     1     1     1     1     1     1     1\n#&gt; [25,]     1     1     1     1     1     1     1     1     1     1     1     1\n\n\n\nmean(foco_elev[1:25, 1:25][,1])\n#&gt; [1] 1610.022\n\nna.omit(values(f1))[1]\n#&gt; [1] 1610.022"
  },
  {
    "objectID": "slides/week-4.html#zonal",
    "href": "slides/week-4.html#zonal",
    "title": "Week 4",
    "section": "Zonal",
    "text": "Zonal\n\nZonal operations compute a summary values (such as the mean) from cells aggregated to some zonal unit.\nLike focal operations, a zone and a mediating function must be defined\nThe most basis example of a zonal function is aggregation!\n\n\naggregate(foco_elev, 10) |&gt; plot()"
  },
  {
    "objectID": "slides/week-4.html#zonal-statisics-more-advanced",
    "href": "slides/week-4.html#zonal-statisics-more-advanced",
    "title": "Week 4",
    "section": "Zonal Statisics (More advanced)",
    "text": "Zonal Statisics (More advanced)\n\nFor more complicated object zones, exactextractr is a fast and effiecient R utility that binds the C++ exactextract tool.\nWhat is the county level mean January rainfall in California?\n\n\nAOI = AOI::aoi_get(state = \"CO\", county = \"all\")\nAOI$janPTT = exactextractr::exact_extract(prcp$ppt$`ppt_2000-01-01_total`, AOI, \"mean\", progress = FALSE)\nplot(AOI['janPTT'])"
  },
  {
    "objectID": "slides/week-4.html#what-about-the-us",
    "href": "slides/week-4.html#what-about-the-us",
    "title": "Week 4",
    "section": "What about the US?",
    "text": "What about the US?\n\ncounties &lt;-  AOI::aoi_get(state = \"conus\", county = \"all\")\n  \njan &lt;- climateR::getTerraClim(counties, \"ppt\", startDate = \"2000-01-01\") \n  \ncounties$janPTT &lt;-  exactextractr::exact_extract(jan$ppt, counties, \"mean\", progress = FALSE)\n\n\nplot(counties['janPTT'], border = NA, key.pos = 4)"
  },
  {
    "objectID": "slides/week-4.html#global-1",
    "href": "slides/week-4.html#global-1",
    "title": "Week 4",
    "section": "Global",
    "text": "Global\n\nGlobal operations make use of some or all input cells when computing an output cell value.\nThey are a special case of zonal operations with the entire raster represents a single zone.\nExamples include generating descriptive statistics for the entire raster dataset"
  },
  {
    "objectID": "slides/week-4.html#mean-monthly-rainfall-for-california",
    "href": "slides/week-4.html#mean-monthly-rainfall-for-california",
    "title": "Week 4",
    "section": "Mean Monthly Rainfall for California",
    "text": "Mean Monthly Rainfall for California\n\nv = global(prcp$ppt, mean)\nplot(x = 1:73, y = v$mean, type  = \"l\", main = \"Rainfall Monthly Mean\")"
  },
  {
    "objectID": "slides/week-4.html#extra-animating-rasters",
    "href": "slides/week-4.html#extra-animating-rasters",
    "title": "Week 4",
    "section": "Extra! Animating Rasters‚Ä¶",
    "text": "Extra! Animating Rasters‚Ä¶\nInstall and load the gifski package\n\nsave_gif: combines many individual plots\nA for loop build the plots\nThe plot is what we have been doing all along (if you want a ggplot you must print the object!)\ngif_file: the path to save the image\nwidth/height: the image dimensions\ndelay: the pause between frames\nloop: should the gif play over and over?\n\n\nlibrary(gifski)\n\nsave_gif(\n  {for(i in 1:nlyr(prcp$ppt)) {\n      plot(prcp$ppt[[i]], col = blues9, \n           legend = FALSE, \n           main = names(prcp$ppt)[i])}\n  }, \n  gif_file = \"images/ppt.gif\", \n  width = 800, height = 600, delay = .33, loop = TRUE)"
  },
  {
    "objectID": "slides/week-4.html#result-1",
    "href": "slides/week-4.html#result-1",
    "title": "Week 4",
    "section": "Result",
    "text": "Result"
  },
  {
    "objectID": "slides/week-4.html#app",
    "href": "slides/week-4.html#app",
    "title": "Week 4",
    "section": "app",
    "text": "app\nJust like a vector, we can apply functions over a raster with app\nThese types of formulas are very useful for thresholding analysis\nQuestion: separate Fort Collins into the higher and lower elevations\n\nFUN = function(x){ ifelse(x &lt; mean(x), 1, 2) }\n\n\nelev3 = app(elev, FUN) #&lt;&lt;\nplot(elev3, col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "slides/week-4.html#colorado-october-2018-climate",
    "href": "slides/week-4.html#colorado-october-2018-climate",
    "title": "Week 4",
    "section": "Colorado October 2018 climate",
    "text": "Colorado October 2018 climate\n\nplot(co)"
  },
  {
    "objectID": "slides/week-4.html#raster-layers-are-vectors",
    "href": "slides/week-4.html#raster-layers-are-vectors",
    "title": "Week 4",
    "section": "Raster layers are vectors!",
    "text": "Raster layers are vectors!\n\nvalues = values(co)\nhead(values)\n#&gt;       ppt tmax  tmin srad    q\n#&gt; [1,] 37.8  1.9 125.1 12.1 -2.4\n#&gt; [2,] 39.2  2.0 124.2 10.8 -2.8\n#&gt; [3,] 39.2  2.0 124.3 10.6 -2.9\n#&gt; [4,] 37.7  1.9 124.6 11.6 -2.4\n#&gt; [5,] 36.3  1.8 125.0 12.2 -2.2\n#&gt; [6,] 34.8  1.7 125.0 13.0 -2.1"
  },
  {
    "objectID": "slides/week-4.html#data-prep",
    "href": "slides/week-4.html#data-prep",
    "title": "Week 4",
    "section": "Data Prep",
    "text": "Data Prep\n\nIdentify NA indices for latter reference\nRemove NA values\nScale\n\n\nidx &lt;- which(!apply(is.na(values), 1, any))\nv   &lt;- na.omit(values)\nvs  &lt;- scale(v)"
  },
  {
    "objectID": "slides/week-4.html#section-22",
    "href": "slides/week-4.html#section-22",
    "title": "Week 4",
    "section": "",
    "text": "plot(r)\nplot(river, add = TRUE, col = \"blue\", lwd = 2)\nplot(line, add = TRUE, col = \"black\", lwd = 2)\nplot(outlet$geometry, add = TRUE, pch = 16, col = \"red\")\nplot(inlet$geometry, add = TRUE, pch = 16, col = \"green\")"
  },
  {
    "objectID": "slides/week-4.html#copying-a-raster-structure",
    "href": "slides/week-4.html#copying-a-raster-structure",
    "title": "Week 4",
    "section": "Copying a raster structure",
    "text": "Copying a raster structure\n\nclus_raster &lt;- co$tmax\nvalues(clus_raster) &lt;- NA"
  },
  {
    "objectID": "slides/week-4.html#assign-values",
    "href": "slides/week-4.html#assign-values",
    "title": "Week 4",
    "section": "Assign values",
    "text": "Assign values\n\nclus_raster[idx] &lt;- E$cluster\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))"
  },
  {
    "objectID": "slides/week-4.html#merging-across-data-sources",
    "href": "slides/week-4.html#merging-across-data-sources",
    "title": "Week 4",
    "section": "Merging across data sources",
    "text": "Merging across data sources\n\n# Get elevations data\nelev = elevatr::get_elev_raster(AOI, z = 5) %&gt;% \n  crop(AOI) |&gt; \n  rast()\n\n# Align Raster extents and resolutions\nelev = project(elev, co$ppt)\n\n# Extract Values\nvalues = c(co$ppt, elev) %&gt;% values()\n\n# Prep data\nidx = which(!apply(is.na(values), 1, any))\nv = na.omit(values)\nvs = scale(v)\n\n# Cluster\nE = kmeans(vs, 5, iter.max = 100)\n\nclus_raster = elev\nvalues(clus_raster) = NA\nclus_raster[idx] &lt;- E$cluster"
  },
  {
    "objectID": "slides/week-4.html#section-23",
    "href": "slides/week-4.html#section-23",
    "title": "Week 4",
    "section": "",
    "text": "(E = kmeans(vs, 5, iter.max = 100))\n#&gt; K-means clustering with 5 clusters of sizes 6521, 4677, 1471, 3619, 542\n#&gt; \n#&gt; Cluster means:\n#&gt;          ppt       tmax       tmin       srad          q\n#&gt; 1 -0.5799378 -0.2419152 -0.1384006  0.6815967  0.6486097\n#&gt; 2  0.1069063 -0.1604128 -0.7891111 -1.0697441 -1.1933422\n#&gt; 3  1.7381744  0.2854842 -0.1759393 -0.3587139 -0.0132010\n#&gt; 4 -0.2805363 -0.1935889  1.4262188  0.5651088  0.5636098\n#&gt; 5  3.2106644  4.8126020 -0.5710265 -1.7692979 -1.2335931\n#&gt; \n#&gt; Clustering vector:\n#&gt;     [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;    [37] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;    [73] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [109] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [145] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1\n#&gt;   [181] 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt;   [217] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [253] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [289] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [325] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [361] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [397] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1\n#&gt;   [433] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [469] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [505] 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [541] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [577] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [613] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [649] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [685] 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n#&gt;   [721] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [757] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [793] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [829] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1\n#&gt;   [865] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [901] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;   [937] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [973] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1009] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1045] 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1081] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt;  [1117] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1153] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1189] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt;  [1225] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1261] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1\n#&gt;  [1369] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1405] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1441] 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1477] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1513] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1549] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1585] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1\n#&gt;  [1621] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1657] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1693] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt;  [1729] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1765] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1801] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1837] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1873] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n#&gt;  [1909] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [1945] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1981] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2017] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2053] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2089] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2125] 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2161] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2197] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2233] 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2269] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1\n#&gt;  [2305] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2341] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2377] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n#&gt;  [2413] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2449] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2485] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2521] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1\n#&gt;  [2557] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2593] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2629] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2665] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2701] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1\n#&gt;  [2737] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2773] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2809] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2845] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2881] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n#&gt;  [2917] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [2953] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt;  [2989] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3025] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3061] 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3097] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3133] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3169] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3205] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3241] 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3277] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3313] 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3349] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3385] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3421] 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3457] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1\n#&gt;  [3493] 1 1 4 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3529] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3565] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n#&gt;  [3601] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3637] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 4 4 4 4 4 4 4 4 1\n#&gt;  [3673] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3709] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3745] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3781] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3817] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3853] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3925] 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3961] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [3997] 2 2 2 1 1 1 1 1 1 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4033] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4069] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n#&gt;  [4105] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4141] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 4 4\n#&gt;  [4177] 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4213] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4249] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4285] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4321] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 4 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt;  [4357] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4393] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4429] 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4465] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4501] 2 2 2 2 2 2 2 2 2 2 1 1 1 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4537] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4573] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1\n#&gt;  [4609] 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4645] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4681] 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4717] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4753] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n#&gt;  [4789] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4825] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 4 4 4 4 4 4\n#&gt;  [4861] 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4897] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4933] 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [4969] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5005] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 4 4 4 4 4 4 4 4 4 4 1 1 4 1 1 1 1\n#&gt;  [5041] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5077] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5113] 2 2 1 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5185] 2 2 2 2 2 2 2 1 1 4 4 4 4 4 4 4 4 4 4 4 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5221] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5257] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 2 2 2 2 2\n#&gt;  [5293] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5329] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1\n#&gt;  [5365] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5401] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5437] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5473] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5509] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 4 4 4 4 4 4 4 4 4 4\n#&gt;  [5545] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5581] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5617] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5653] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5689] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1\n#&gt;  [5725] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5761] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2\n#&gt;  [5797] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5833] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5869] 2 2 2 2 1 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5905] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5941] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5977] 2 2 2 2 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6013] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 4 4 4 4\n#&gt;  [6049] 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6085] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6121] 1 3 1 1 1 1 1 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1\n#&gt;  [6157] 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6193] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6229] 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6265] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6301] 1 2 1 1 1 1 1 2 1 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n#&gt;  [6337] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6373] 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1\n#&gt;  [6409] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 1 1 1 2\n#&gt;  [6481] 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6517] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6553] 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6589] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6625] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 1 1 1 2 1 1 1 1 2 2 2 2\n#&gt;  [6661] 2 2 2 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6697] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6733] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6769] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6805] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6841] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6877] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [6913] 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6949] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6985] 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7021] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7057] 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 1 1\n#&gt;  [7093] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7129] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7165] 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7201] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2\n#&gt;  [7237] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7273] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7309] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 2 2 2 2 2\n#&gt;  [7345] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7381] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 4 4 4 4 4 4 4 4 4\n#&gt;  [7417] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7453] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7489] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7525] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7561] 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [7597] 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7633] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7669] 1 1 1 1 1 1 1 1 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7705] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7741] 4 4 4 4 4 4 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1\n#&gt;  [7777] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7813] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 2\n#&gt;  [7849] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7885] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 4 4 2 2 2 4\n#&gt;  [7921] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 1 1 1 1 1 1 1 1 1\n#&gt;  [7957] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7993] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8029] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8065] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8101] 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8137] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8173] 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8209] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8245] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8281] 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8317] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n#&gt;  [8353] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8389] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8425] 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1\n#&gt;  [8461] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8497] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8533] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8569] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4\n#&gt;  [8605] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8641] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8677] 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 3 2 2 2 3 1 1 3 3 3 2 2 2 2 2 2 3 2 2 2 2\n#&gt;  [8713] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8749] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8785] 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8821] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n#&gt;  [8857] 1 1 1 1 1 3 3 3 3 1 1 1 1 1 1 1 2 2 3 3 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8893] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8929] 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [8965] 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9001] 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9037] 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9073] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9109] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1\n#&gt;  [9145] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9181] 1 1 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9217] 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9253] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4\n#&gt;  [9289] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9325] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3\n#&gt;  [9361] 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9397] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9433] 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9469] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9505] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n#&gt;  [9541] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9577] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4\n#&gt;  [9613] 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9649] 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9685] 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9721] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9757] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 2 2 2 4 4\n#&gt;  [9793] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 1 1 1 1 1 1 1\n#&gt;  [9829] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1\n#&gt;  [9865] 1 1 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2\n#&gt;  [9901] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2\n#&gt;  [9937] 2 2 2 2 2 2 2 2 2 2 4 2 2 4 4 4 4 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt;  [9973] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10009] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 1 1 1 3 3 3 3 3 3 3\n#&gt; [10045] 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10081] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10117] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10153] 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10189] 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n#&gt; [10225] 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10261] 2 2 2 2 2 2 2 2 2 2 2 2 1 4 4 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4\n#&gt; [10297] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10333] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10369] 1 1 3 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 3 2\n#&gt; [10405] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10441] 2 2 1 4 4 4 2 4 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10477] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1\n#&gt; [10513] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 3 1 1 1 1\n#&gt; [10549] 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2\n#&gt; [10585] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 2\n#&gt; [10621] 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10657] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10693] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 1 1 1 3 3 3 3 3 3 3 3\n#&gt; [10729] 3 3 3 3 3 3 3 1 1 1 1 1 1 1 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10765] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 4 4 4 4 4 4 4 4 4\n#&gt; [10801] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10837] 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10873] 1 1 1 1 1 1 1 1 3 3 1 1 3 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1\n#&gt; [10909] 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10945] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [10981] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11017] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3\n#&gt; [11053] 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 3 3 2 2 2 2\n#&gt; [11089] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11125] 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11161] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1\n#&gt; [11197] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 1 1 1 3 1 1 1 1\n#&gt; [11233] 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11269] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 2 2 2 2 2 2 2 2 2 4 4 4\n#&gt; [11305] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11341] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11377] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 1 3 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3\n#&gt; [11413] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2\n#&gt; [11449] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11485] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11521] 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11557] 1 1 1 1 3 1 1 1 1 3 3 3 3 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11593] 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11629] 2 2 2 2 2 2 2 4 4 2 2 5 2 2 2 4 4 4 4 4 4 4 4 4 2 2 4 4 4 4 4 4 4 4 4 4\n#&gt; [11665] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1\n#&gt; [11701] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 3\n#&gt; [11737] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 2 2\n#&gt; [11773] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4\n#&gt; [11809] 2 2 5 2 2 2 4 4 4 4 4 4 4 4 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11845] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11881] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11917] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 3 2 2 5 5 5 2 2 2 2 2\n#&gt; [11953] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 2 3 2 2 2 4 4 4 4\n#&gt; [11989] 4 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12025] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12061] 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12097] 3 3 3 3 3 3 3 3 3 3 5 5 3 3 5 5 2 2 5 5 5 5 2 2 3 3 5 5 2 2 2 2 2 2 2 2\n#&gt; [12133] 2 2 2 2 2 2 1 1 1 1 4 4 4 4 4 4 4 2 3 3 2 2 2 4 4 4 4 4 2 2 2 2 2 2 4 4\n#&gt; [12169] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12205] 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12241] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12277] 5 5 3 5 5 5 2 2 5 5 5 2 2 2 5 5 5 5 5 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 4 4\n#&gt; [12313] 4 4 4 4 4 4 4 4 3 5 2 2 2 2 4 4 4 4 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12349] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt; [12385] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3\n#&gt; [12421] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 3 3 5 5 3 5 5 5 3 2 5 5 5\n#&gt; [12457] 5 5 5 5 5 5 3 5 5 5 2 2 2 3 2 2 2 2 2 3 2 2 2 1 4 4 4 4 4 4 4 4 4 4 4 2\n#&gt; [12493] 5 2 2 2 4 4 4 4 4 4 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12529] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12565] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12601] 3 3 3 3 3 3 3 3 3 5 5 5 5 5 3 5 3 3 5 5 5 5 3 3 5 5 5 5 5 5 5 3 3 5 5 5\n#&gt; [12637] 5 3 3 5 5 3 3 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 3 2 4 4 4 4 4\n#&gt; [12673] 4 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12709] 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12745] 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [12781] 3 5 5 5 5 5 3 3 5 5 5 5 2 5 5 5 5 3 3 5 5 5 5 3 5 5 5 5 5 5 5 5 3 2 2 3\n#&gt; [12817] 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 5 3 3 2 2 4 4 4 4 4 4 2 3 3 2 4 4 4 4\n#&gt; [12853] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12889] 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3\n#&gt; [12925] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 3 5 5 5 3 5 3 3 5\n#&gt; [12961] 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 3 2 2 2 2 2 4 4 4 4 4 4 4 4\n#&gt; [12997] 4 4 4 4 4 4 4 2 5 5 5 2 4 4 4 4 4 4 4 3 2 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13033] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1\n#&gt; [13069] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13105] 3 3 3 3 3 3 5 3 3 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 3 3 3 5 5 5 5 5 3 5 5 5\n#&gt; [13141] 2 2 3 5 5 5 5 5 5 5 5 5 5 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 5\n#&gt; [13177] 5 2 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13213] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13249] 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 3\n#&gt; [13285] 3 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 5 5 5 5 3 3 2 3 5 5 2 2 2 2 3 5 5 5 5 5\n#&gt; [13321] 3 3 3 3 3 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 4 4 4 4 4 4 4 4\n#&gt; [13357] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13393] 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13429] 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5\n#&gt; [13465] 5 5 3 3 5 5 5 3 5 3 3 3 2 2 3 3 3 5 5 3 3 3 5 5 5 5 3 2 3 3 2 4 4 4 4 4\n#&gt; [13501] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13537] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1\n#&gt; [13573] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3\n#&gt; [13609] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5 5 5 5 5 5 5 5 3 5 3 3 5 5 5 5 3 3\n#&gt; [13645] 3 3 5 3 2 2 3 5 5 3 3 5 2 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13681] 4 4 4 4 4 4 3 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13717] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13753] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13789] 3 3 3 3 3 3 3 3 5 3 5 5 3 3 5 5 3 5 3 5 5 5 5 5 5 5 5 5 5 5 3 2 3 5 5 3\n#&gt; [13825] 5 5 3 3 3 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 5 2 2 4\n#&gt; [13861] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13897] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13933] 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [13969] 3 3 3 3 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 3 5 5 5 5 5 5 5 3 3 3 3 3 3 2\n#&gt; [14005] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 5 3 3 3 2 3 2 4 4 4 4 4 4 4\n#&gt; [14041] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14077] 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3\n#&gt; [14113] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5\n#&gt; [14149] 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 5 5 5 5 5 3 3 3 4 4 4 4 4 4 4 4\n#&gt; [14185] 4 4 4 4 4 4 4 4 4 4 4 5 3 3 5 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14221] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1\n#&gt; [14257] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 3 3 3 3 3 3 3 3 3\n#&gt; [14293] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 3 5 5 3 5 3 3 3 3 5 5\n#&gt; [14329] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14365] 4 5 5 5 3 3 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14401] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14437] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14473] 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 3 5 3 3 5 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [14509] 5 5 5 5 5 5 5 5 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 4 4 4 4 4 4\n#&gt; [14545] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14581] 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14617] 1 1 1 1 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14653] 3 3 3 5 3 3 3 3 5 3 3 3 3 3 3 3 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3\n#&gt; [14689] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14725] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14761] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4\n#&gt; [14797] 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [14833] 3 3 3 3 3 3 5 5 3 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 4 4 4 4 4 4 4 4 4\n#&gt; [14869] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14905] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1\n#&gt; [14941] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 3 3 3 4 4 3 3 3 3\n#&gt; [14977] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15013] 3 3 3 5 5 5 5 5 5 5 5 5 5 5 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15049] 4 4 4 3 3 3 3 4 4 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15085] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15121] 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15157] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5\n#&gt; [15193] 5 5 5 5 5 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 5 3 4 3 3\n#&gt; [15229] 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15265] 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15301] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15337] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 4 4 4 4\n#&gt; [15373] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 5 5 3 3 3 4 4 4 4 4 4 4 4 4 4\n#&gt; [15409] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1\n#&gt; [15445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 3 3 4 4\n#&gt; [15481] 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15517] 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15553] 4 4 4 4 4 4 4 4 3 5 5 5 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15589] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15625] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 3 3 4 4 4 3 3 3 3 3 3 3 3 3\n#&gt; [15661] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15697] 5 5 5 5 5 5 5 5 5 5 5 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3\n#&gt; [15733] 5 5 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15769] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15805] 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [15841] 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [15877] 5 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 5 5 3 4 4 4 4 4 4\n#&gt; [15913] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15949] 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4\n#&gt; [15985] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 4 4 4 4 4 4 4 4 4 3 3 3\n#&gt; [16021] 4 4 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 3 5 5 5 5 5 5 5 5 5 3 3 3 4 4 4 4 4 4\n#&gt; [16057] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 5 5 5 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16093] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1\n#&gt; [16129] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16165] 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 3 3 3 4 3 3 3 3 3 3 3 4 4\n#&gt; [16201] 3 3 3 3 3 5 5 3 5 5 5 5 5 5 5 5 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16237] 4 4 4 4 4 3 5 3 5 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16273] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [16309] 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16345] 4 4 4 4 4 4 4 3 3 4 4 4 4 4 4 4 4 3 4 3 4 4 4 4 4 3 3 3 3 3 3 3 3 3 5 5\n#&gt; [16381] 5 5 5 5 5 5 5 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 5 5 5 3 3\n#&gt; [16417] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16453] 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [16489] 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16525] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 5 5 3 5 5 5 5 5 5 5 3 4\n#&gt; [16561] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 5 5 5 3 3 3 4 4 4 4 4 4 4 4 4 4\n#&gt; [16597] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1\n#&gt; [16633] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4\n#&gt; [16669] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4\n#&gt; [16705] 4 4 3 4 3 3 3 4 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 3 3 3 4 4 4 4 4 4 4 4 4 4\n#&gt; [16741] 4 4 4 4 4 4 4 4 3 5 5 5 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [16777] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [16813] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 5524.327 7478.029 3516.041 4545.222 3785.104\n#&gt;  (between_SS / total_SS =  70.5 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/week-4.html#clustering-at-larger-scales",
    "href": "slides/week-4.html#clustering-at-larger-scales",
    "title": "Week 4",
    "section": "Clustering at larger scales",
    "text": "Clustering at larger scales\n\ncounties =  AOI::aoi_get(state = \"conus\", county = \"all\")\nparams = c(\"tmax\", \"tmin\", \"ppt\", \"srad\")\n\ndat = climateR::getTerraClim(counties, params, startDate = \"2018-06-01\") %&gt;% \n  unlist() |&gt; \n  rast() %&gt;% \n  setNames(params) |&gt; \n  exactextractr::exact_extract(counties, \"mean\", progress = FALSE)\n\n\ndat = scale(dat)\n\ncounties$clust8 = kmeans(dat, 8)$cluster\n\nplot(st_transform(counties[\"clust8\"], 5070), border = NA)"
  },
  {
    "objectID": "slides/week-4.html#section-8",
    "href": "slides/week-4.html#section-8",
    "title": "Week 4",
    "section": "",
    "text": "terra::classify(mean(prcp$ppt), rcl, include.lowest=TRUE) |&gt; \n   plot(col = blues9)"
  },
  {
    "objectID": "slides/week-4.html#section-18",
    "href": "slides/week-4.html#section-18",
    "title": "Week 4",
    "section": "",
    "text": "(E = kmeans(vs, 5, iter.max = 100))\n#&gt; K-means clustering with 5 clusters of sizes 6521, 3619, 4677, 1471, 542\n#&gt; \n#&gt; Cluster means:\n#&gt;          ppt       tmax       tmin       srad          q\n#&gt; 1 -0.5799378 -0.2419152 -0.1384006  0.6815967  0.6486097\n#&gt; 2 -0.2805363 -0.1935889  1.4262188  0.5651088  0.5636098\n#&gt; 3  0.1069063 -0.1604128 -0.7891111 -1.0697441 -1.1933422\n#&gt; 4  1.7381744  0.2854842 -0.1759393 -0.3587139 -0.0132010\n#&gt; 5  3.2106644  4.8126020 -0.5710265 -1.7692979 -1.2335931\n#&gt; \n#&gt; Clustering vector:\n#&gt;     [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;    [37] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;    [73] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [109] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [145] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 1 1 1 1\n#&gt;   [181] 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3\n#&gt;   [217] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;   [253] 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [289] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [325] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [361] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;   [397] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1\n#&gt;   [433] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [469] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [505] 1 1 1 1 1 1 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [541] 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;   [577] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [613] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [649] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [685] 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3\n#&gt;   [721] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;   [757] 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [793] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [829] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 1 1 1 1 1\n#&gt;   [865] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;   [901] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;   [937] 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;   [973] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1009] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1045] 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [1081] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1\n#&gt;  [1117] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1153] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1189] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n#&gt;  [1225] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [1261] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1\n#&gt;  [1369] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [1405] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [1441] 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1477] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1513] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1549] 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [1585] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n#&gt;  [1621] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1657] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1693] 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n#&gt;  [1729] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [1765] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1801] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1837] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1873] 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3\n#&gt;  [1909] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [1945] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [1981] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2017] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2053] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2089] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2125] 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2161] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2197] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2233] 1 3 3 3 3 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2269] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n#&gt;  [2305] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2341] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2377] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3\n#&gt;  [2413] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2449] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2485] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2521] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1\n#&gt;  [2557] 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2593] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2629] 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2665] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2701] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1\n#&gt;  [2737] 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2773] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2809] 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2845] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [2881] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3\n#&gt;  [2917] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [2953] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1\n#&gt;  [2989] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3025] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3061] 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3097] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3133] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3169] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3205] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3241] 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3277] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3313] 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3349] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3385] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3421] 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3457] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n#&gt;  [3493] 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3529] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3565] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3\n#&gt;  [3601] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3637] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 2 2 2 2 2 2 2 1\n#&gt;  [3673] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3709] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3745] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3781] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3817] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3853] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [3925] 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3961] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [3997] 3 3 3 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4033] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4069] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n#&gt;  [4105] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4141] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 2 2\n#&gt;  [4177] 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4213] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4249] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4285] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4321] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 1\n#&gt;  [4357] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4393] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4429] 1 1 1 3 3 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4465] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4501] 3 3 3 3 3 3 3 3 3 3 1 1 1 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4537] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4573] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 1 1\n#&gt;  [4609] 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4645] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4681] 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4717] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4753] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3\n#&gt;  [4789] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4825] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 2 2 2 2 2 2\n#&gt;  [4861] 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4897] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [4933] 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [4969] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5005] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 2 2 2 2 1 1 2 1 1 1 1\n#&gt;  [5041] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5077] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5113] 3 3 1 1 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5185] 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5221] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5257] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 3 3 3 3 3\n#&gt;  [5293] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5329] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1\n#&gt;  [5365] 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5401] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5437] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5473] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5509] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n#&gt;  [5545] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5581] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5617] 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5653] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5689] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1\n#&gt;  [5725] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5761] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3\n#&gt;  [5797] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5833] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5869] 3 3 3 3 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5905] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [5941] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [5977] 3 3 3 3 1 1 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6013] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 2 2 2\n#&gt;  [6049] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6085] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6121] 1 4 1 1 1 1 1 1 1 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1\n#&gt;  [6157] 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6193] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6229] 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6265] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6301] 1 3 1 1 1 1 1 3 1 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3\n#&gt;  [6337] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6373] 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n#&gt;  [6409] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 1 1 1 1 1 1 3\n#&gt;  [6481] 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6517] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6553] 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6589] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6625] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 1 1 1 1 1 1 3 1 1 1 1 3 3 3 3\n#&gt;  [6661] 3 3 3 4 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6697] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6733] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6769] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6805] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6841] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [6877] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [6913] 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6949] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [6985] 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7021] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7057] 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 1\n#&gt;  [7093] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7129] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7165] 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7201] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 3\n#&gt;  [7237] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7273] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7309] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 3 3 3 3 3\n#&gt;  [7345] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7381] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 3 3 2 2 2 2 2 2 2 2 2\n#&gt;  [7417] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7453] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7489] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7525] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7561] 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [7597] 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7633] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7669] 1 1 1 1 1 1 1 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7705] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7741] 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n#&gt;  [7777] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7813] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3\n#&gt;  [7849] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [7885] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 2 2 3 3 3 2\n#&gt;  [7921] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1\n#&gt;  [7957] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [7993] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8029] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8065] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8101] 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8137] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8173] 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8209] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8245] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8281] 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8317] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3\n#&gt;  [8353] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8389] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8425] 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1\n#&gt;  [8461] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8497] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8533] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8569] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2\n#&gt;  [8605] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8641] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8677] 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 4 3 3 3 4 1 1 4 4 4 3 3 3 3 3 3 4 3 3 3 3\n#&gt;  [8713] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8749] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8785] 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [8821] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt;  [8857] 1 1 1 1 1 4 4 4 4 1 1 1 1 1 1 1 3 3 4 4 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8893] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [8929] 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [8965] 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9001] 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9037] 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9073] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9109] 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1\n#&gt;  [9145] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9181] 1 1 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9217] 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9253] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2\n#&gt;  [9289] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9325] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4\n#&gt;  [9361] 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9397] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9433] 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9469] 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9505] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt;  [9541] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9577] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n#&gt;  [9613] 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9649] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9685] 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [9721] 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [9757] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 3 3 3 3 3 3 3 2 2\n#&gt;  [9793] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 1 1\n#&gt;  [9829] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1\n#&gt;  [9865] 1 1 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3\n#&gt;  [9901] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3\n#&gt;  [9937] 3 3 3 3 3 3 3 3 3 3 2 3 3 2 2 2 2 2 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [9973] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10009] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 1 1 1 1 4 4 4 4 4 4 4\n#&gt; [10045] 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10081] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10117] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10153] 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10189] 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1\n#&gt; [10225] 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10261] 3 3 3 3 3 3 3 3 3 3 3 3 1 2 2 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2\n#&gt; [10297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10333] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10369] 1 1 4 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 4 3\n#&gt; [10405] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10441] 3 3 1 2 2 2 3 2 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10477] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt; [10513] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 1 4 1 1 1 1\n#&gt; [10549] 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3\n#&gt; [10585] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 3\n#&gt; [10621] 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10657] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10693] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 1 1 1 4 4 4 4 4 4 4 4\n#&gt; [10729] 4 4 4 4 4 4 4 1 1 1 1 1 1 1 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10765] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 2\n#&gt; [10801] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10837] 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [10873] 1 1 1 1 1 1 1 1 4 4 1 1 4 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1\n#&gt; [10909] 1 1 1 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [10945] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [10981] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11017] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4\n#&gt; [11053] 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 3 3 3 3\n#&gt; [11089] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11125] 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11161] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1\n#&gt; [11197] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 1 1 1 1 4 1 1 1 1\n#&gt; [11233] 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11269] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 3 3 3 3 3 3 3 3 2 2 2\n#&gt; [11305] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11341] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11377] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 1 1 4 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4\n#&gt; [11413] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3\n#&gt; [11449] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11485] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11521] 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11557] 1 1 1 1 4 1 1 1 1 4 4 4 4 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11593] 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt; [11629] 3 3 3 3 3 3 3 2 2 3 3 5 3 3 3 2 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2\n#&gt; [11665] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n#&gt; [11701] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 1 1 1 4\n#&gt; [11737] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 4 3 3\n#&gt; [11773] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2\n#&gt; [11809] 3 3 5 3 3 3 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [11845] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [11881] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [11917] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 4 3 3 5 5 5 3 3 3 3 3\n#&gt; [11953] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 4 3 3 3 2 2 2 2\n#&gt; [11989] 2 2 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12025] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12061] 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12097] 4 4 4 4 4 4 4 4 4 4 5 5 4 4 5 5 3 3 5 5 5 5 3 3 4 4 5 5 3 3 3 3 3 3 3 3\n#&gt; [12133] 3 3 3 3 3 3 1 1 1 1 2 2 2 2 2 2 2 3 4 4 3 3 3 2 2 2 2 2 3 3 3 3 3 3 2 2\n#&gt; [12169] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12205] 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12241] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12277] 5 5 4 5 5 5 3 3 5 5 5 3 3 3 5 5 5 5 5 4 4 4 4 3 3 3 3 3 3 3 3 3 3 2 2 2\n#&gt; [12313] 2 2 2 2 2 2 2 2 4 5 3 3 3 3 2 2 2 2 2 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12349] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1\n#&gt; [12385] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4\n#&gt; [12421] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 4 4 5 5 4 5 5 5 4 3 5 5 5\n#&gt; [12457] 5 5 5 5 5 5 4 5 5 5 3 3 3 4 3 3 3 3 3 4 3 3 3 1 2 2 2 2 2 2 2 2 2 2 2 3\n#&gt; [12493] 5 3 3 3 2 2 2 2 2 2 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12529] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12565] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12601] 4 4 4 4 4 4 4 4 4 5 5 5 5 5 4 5 4 4 5 5 5 5 4 4 5 5 5 5 5 5 5 4 4 5 5 5\n#&gt; [12637] 5 4 4 5 5 4 4 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 4 3 2 2 2 2 2\n#&gt; [12673] 2 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12709] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [12745] 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [12781] 4 5 5 5 5 5 4 4 5 5 5 5 3 5 5 5 5 4 4 5 5 5 5 4 5 5 5 5 5 5 5 5 4 3 3 4\n#&gt; [12817] 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 5 4 4 3 3 2 2 2 2 2 2 3 4 4 3 2 2 2 2\n#&gt; [12853] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [12889] 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4\n#&gt; [12925] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 4 5 5 5 4 5 4 4 5\n#&gt; [12961] 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 4 3 3 3 3 3 2 2 2 2 2 2 2 2\n#&gt; [12997] 2 2 2 2 2 2 2 3 5 5 5 3 2 2 2 2 2 2 2 4 3 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13033] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1\n#&gt; [13069] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13105] 4 4 4 4 4 4 5 4 4 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 4 4 4 5 5 5 5 5 4 5 5 5\n#&gt; [13141] 3 3 4 5 5 5 5 5 5 5 5 5 5 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 5\n#&gt; [13177] 5 3 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13213] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13249] 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4\n#&gt; [13285] 4 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 5 5 5 5 4 4 3 4 5 5 3 3 3 3 4 5 5 5 5 5\n#&gt; [13321] 4 4 4 4 4 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 2 2\n#&gt; [13357] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13393] 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13429] 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5\n#&gt; [13465] 5 5 4 4 5 5 5 4 5 4 4 4 3 3 4 4 4 5 5 4 4 4 5 5 5 5 4 3 4 4 3 2 2 2 2 2\n#&gt; [13501] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13537] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1\n#&gt; [13573] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4\n#&gt; [13609] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 5 5 5 5 5 5 5 5 4 5 4 4 5 5 5 5 4 4\n#&gt; [13645] 4 4 5 4 3 3 4 5 5 4 4 5 3 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13681] 2 2 2 2 2 2 4 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13717] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13753] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13789] 4 4 4 4 4 4 4 4 5 4 5 5 4 4 5 5 4 5 4 5 5 5 5 5 5 5 5 5 5 5 4 3 4 5 5 4\n#&gt; [13825] 5 5 4 4 4 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 5 3 3 2\n#&gt; [13861] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [13897] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [13933] 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [13969] 4 4 4 4 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 4 5 5 5 5 5 5 5 4 4 4 4 4 4 3\n#&gt; [14005] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 4 4 4 3 4 3 2 2 2 2 2 2 2\n#&gt; [14041] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14077] 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 4\n#&gt; [14113] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5\n#&gt; [14149] 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 5 5 5 5 5 4 4 4 2 2 2 2 2 2 2 2\n#&gt; [14185] 2 2 2 2 2 2 2 2 2 2 2 5 4 4 5 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14221] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1\n#&gt; [14257] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 4 4 4 4 4 4 4 4 4\n#&gt; [14293] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 4 5 5 4 5 4 4 4 4 5 5\n#&gt; [14329] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14365] 2 5 5 5 4 4 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14401] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14437] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14473] 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 4 5 4 4 5 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [14509] 5 5 5 5 5 5 5 5 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2\n#&gt; [14545] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14581] 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [14617] 1 1 1 1 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14653] 4 4 4 5 4 4 4 4 5 4 4 4 4 4 4 4 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4\n#&gt; [14689] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14725] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14761] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n#&gt; [14797] 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [14833] 4 4 4 4 4 4 5 5 4 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 2 2 2 2 2 2 2 2 2\n#&gt; [14869] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [14905] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt; [14941] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 4 4 4 2 2 4 4 4 4\n#&gt; [14977] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15013] 4 4 4 5 5 5 5 5 5 5 5 5 5 5 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15049] 2 2 2 4 4 4 4 2 2 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15085] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15121] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15157] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5\n#&gt; [15193] 5 5 5 5 5 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 5 4 2 4 4\n#&gt; [15229] 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15265] 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15301] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15337] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 2 2 2 2\n#&gt; [15373] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 5 5 4 4 4 2 2 2 2 2 2 2 2 2 2\n#&gt; [15409] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1\n#&gt; [15445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 4 4 2 2\n#&gt; [15481] 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15517] 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15553] 2 2 2 2 2 2 2 2 4 5 5 5 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15589] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15625] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 4 4 2 2 2 4 4 4 4 4 4 4 4 4\n#&gt; [15661] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15697] 5 5 5 5 5 5 5 5 5 5 5 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4\n#&gt; [15733] 5 5 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15769] 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [15805] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4\n#&gt; [15841] 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5\n#&gt; [15877] 5 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 5 5 4 2 2 2 2 2 2\n#&gt; [15913] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [15949] 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n#&gt; [15985] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 2 2 2 2 2 2 2 2 2 4 4 4\n#&gt; [16021] 2 2 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 4 5 5 5 5 5 5 5 5 5 4 4 4 2 2 2 2 2 2\n#&gt; [16057] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5 5 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16093] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt; [16129] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16165] 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 4 4 4 2 4 4 4 4 4 4 4 2 2\n#&gt; [16201] 4 4 4 4 4 5 5 4 5 5 5 5 5 5 5 5 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16237] 2 2 2 2 2 4 5 4 5 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16273] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [16309] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16345] 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 2 2 4 2 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 5 5\n#&gt; [16381] 5 5 5 5 5 5 5 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5 5 4 4\n#&gt; [16417] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16453] 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [16489] 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16525] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 5 5 4 5 5 5 5 5 5 5 4 2\n#&gt; [16561] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 5 5 5 4 4 4 2 2 2 2 2 2 2 2 2 2\n#&gt; [16597] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n#&gt; [16633] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n#&gt; [16669] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2\n#&gt; [16705] 2 2 4 2 4 4 4 2 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 4 4 4 2 2 2 2 2 2 2 2 2 2\n#&gt; [16741] 2 2 2 2 2 2 2 2 4 5 5 5 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [16777] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [16813] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 5524.327 4545.222 7478.029 3516.041 3785.104\n#&gt;  (between_SS / total_SS =  70.5 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/week-4.html#section-24",
    "href": "slides/week-4.html#section-24",
    "title": "Week 4",
    "section": "",
    "text": "par(mfrow = c(2,1))\nplot(elev)\nplot(co$ppt)\n\n\n\n\n\n\n\n\n\n\nplot(clus_raster, col = RColorBrewer::brewer.pal(5, \"Spectral\"))"
  },
  {
    "objectID": "slides/week-4.html#recap-sf-and-spatial-concepts",
    "href": "slides/week-4.html#recap-sf-and-spatial-concepts",
    "title": "Week 4",
    "section": "Recap: sf and spatial concepts",
    "text": "Recap: sf and spatial concepts\n\nSpatial phenomena can be thought of discrete objects with clear boundaries or as continuous phenomenon that can be observed everywhere, with no natural boundaries.\nWe have described these as objects and fields (Kuhn, 2012)\nObjects are usually represented by vector data consisting of:\n\na geometry (simple features (sfc, sfg))\nsome attribute information (data.frame)\n\nIn R these are unified as a sf object\nField data is typically represented by raster data\nFor this, we will begin our discussions using the terra package"
  },
  {
    "objectID": "slides/week-4.html#what-makes-of-a-regular-tesselation",
    "href": "slides/week-4.html#what-makes-of-a-regular-tesselation",
    "title": "Week 4",
    "section": "What makes of a regular tesselation",
    "text": "What makes of a regular tesselation\n\nlength(grid1km) # how many grid tiles\n#&gt; [1] 3468\n\n\n\nmapview::npts(grid1km) # how many points?\n#&gt; [1] 17340\n\n\n\n\nmapview::npts(grid1km) * 2 # how many X and Y?\n#&gt; [1] 34680\n\n\n\n\nmapview::npts(grid1km) / length(grid) # how many points per tile?\n#&gt; [1] 173.4\n\n\n\n\nsqrt(st_area(grid1km)[1]) # length of each tile?\n#&gt; 10000 [m]\n\n\nst_bbox(grid1km) # extent of grid\n#&gt;    xmin    ymin    xmax    ymax \n#&gt; 1319502 2149150 1999502 2659150"
  },
  {
    "objectID": "slides/week-4.html#alternative-representation",
    "href": "slides/week-4.html#alternative-representation",
    "title": "Week 4",
    "section": "Alternative representation",
    "text": "Alternative representation\nRegular grids can also be indexed by their centroids\n\ncent &lt;- st_centroid(grid1km)\n\nplot(ny$geometry)\nplot(cent, add = TRUE, pch = 16, cex = .25)\n\n\n\nlength(cent) # how many grid tiles\n#&gt; [1] 3468\n\nmapview::npts(grid1km) # how many points?\n#&gt; [1] 17340\n\nmapview::npts(grid1km) * 2 # how many X and Y?\n#&gt; [1] 34680"
  },
  {
    "objectID": "slides/week-4.html#equal-area-from-centroid",
    "href": "slides/week-4.html#equal-area-from-centroid",
    "title": "Week 4",
    "section": "Equal area from centroid",
    "text": "Equal area from centroid\nWe can use our voroni diagram the show that the area closest to a cell centroid is the cell itself.\n\nvor = st_union(cent) |&gt; \n  st_voronoi() |&gt; \n  st_cast() |&gt; \n  st_intersection(bb)\n\nplot(ny$geometry); plot(vor, add = TRUE)"
  },
  {
    "objectID": "slides/week-4.html#raster-model",
    "href": "slides/week-4.html#raster-model",
    "title": "Week 4",
    "section": "Raster Model",
    "text": "Raster Model\n\nThe raster model is one of the earliest and most widely used data models within geographic information systems (Tomlin, 1990; Goodchild, 1992, Maguire, 1992).\nTypically used to record, analyze and visualize data with a continuous nature such as elevation, temperature (‚ÄúGIS‚Äù), or reflected or emitted electromagnetic radiation (‚ÄúRemote Sensing‚Äù)\nQuotes are used because you‚Äôll find from a data perspective these differences are artificial and a product of the ESRI/ENVI/ERDAS divide\n\n\n\nThe term raster originated from the German word for screen, implying a series of orthogonality oriented parallel lines.\nDigital raster objects most often take the form of a regularly spaced, grid-like pattern of rows and columns\nEach element referred to as a cell, pixel, or grid point."
  },
  {
    "objectID": "slides/week-4.html#many-terms-mean-the-same-thing",
    "href": "slides/week-4.html#many-terms-mean-the-same-thing",
    "title": "Week 4",
    "section": "Many terms mean the same thing ‚Ä¶",
    "text": "Many terms mean the same thing ‚Ä¶\n\nThe entire raster is sometimes referred to as an ‚Äúimage‚Äù, ‚Äúarray‚Äù, ‚Äúsurface‚Äù, ‚Äúmatrix‚Äù, or ‚Äúlattice‚Äù (Wise, 2000).\nThe all mean the same thing‚Ä¶\nCells of the raster are most often square, but may be rectangular (with differing resolutions in x and y directions) or other shapes that can be tessellated such as triangles and hexagons (Figure below from Peuquet, 1984)."
  },
  {
    "objectID": "slides/week-4.html#any-digital-image-contains-an-rbg-channel-for-color",
    "href": "slides/week-4.html#any-digital-image-contains-an-rbg-channel-for-color",
    "title": "Week 4",
    "section": "Any digital image contains an RBG channel for color:",
    "text": "Any digital image contains an RBG channel for color:\n\nRed, green, and blue are the three additive colors (primary colors of light)\nIn R, colors can be defined using the RBG channels\n\n\n(rgb(1,0,0)) # red\n#&gt; [1] \"#FF0000\"\n(rgb(0,.54,.96)) # UCSB navy\n#&gt; [1] \"#008AF5\"\n(rgb(254,188,17, maxColorValue = 255)) # UCSB navy\n#&gt; [1] \"#FEBC11\""
  },
  {
    "objectID": "slides/week-4.html#pure-rbg",
    "href": "slides/week-4.html#pure-rbg",
    "title": "Week 4",
    "section": "Pure RBG",
    "text": "Pure RBG\n\npar(mfrow = c(1,3), mar = c(0,0,0,0))\nplot(ny$geometry,   col = rgb(1,0,0)) # red\nplot(ny$geometry,   col = rgb(0,1,0)) # green\nplot(ny$geometry,   col = rgb(0,0,1)) # blue"
  },
  {
    "objectID": "slides/week-4.html#implicity-coordinates",
    "href": "slides/week-4.html#implicity-coordinates",
    "title": "Week 4",
    "section": "Implicity Coordinates",
    "text": "Implicity Coordinates\n\nUnlike vector data, the raster data model stores the coordinate of the grid cells indirectly\nCoordinates are derived from the reference (Xmin,Ymin) the resolution, and the cell index (e.g.¬†[100,150])\nFor example: If we want the coordinates of a value in the 3rd row and the 40th column of a raster matrix, we have to move from the origin (Xmin, Ymin) (3 x Xres) in x-direction and (40 x Yres) in y-direction"
  },
  {
    "objectID": "slides/week-4.html#raster-values-continuity",
    "href": "slides/week-4.html#raster-values-continuity",
    "title": "Week 4",
    "section": "Raster Values Continuity",
    "text": "Raster Values Continuity\n\nv = values(elev)\nclass(v)\n#&gt; [1] \"matrix\" \"array\"\nlength(v)\n#&gt; [1] 11902500\n\n\nelev\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source      : foco-elev.tif \n#&gt; name        : foco-elev \n#&gt; min value   :      1382 \n#&gt; max value   :      4346"
  },
  {
    "objectID": "slides/week-4.html#raster-algebra-1",
    "href": "slides/week-4.html#raster-algebra-1",
    "title": "Week 4",
    "section": "Raster Algebra",
    "text": "Raster Algebra\n\nIn these functions you can mix SpatRast objects with numbers, as long as the first argument is a raster object.\nThat means you can add 100 to a raster object but not a raster object to 100\n\n\n# GOOD\nraster + 100\n\n# BAD\n100 + raster\n\nFor example:\n\nelev + 100\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; varname     : foco-elev \n#&gt; name        : foco-elev \n#&gt; min value   :      1482 \n#&gt; max value   :      4446\nlog10(elev)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3450, 3450, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 28.98965, 28.98965  (x, y)\n#&gt; extent      : -810156.6, -710142.3, 1934613, 2034627  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; varname     : foco-elev \n#&gt; name        : foco-elev \n#&gt; min value   :  3.140508 \n#&gt; max value   :  3.638090"
  },
  {
    "objectID": "slides/week-4.html#mean-monthly-rainfall-for-colorado",
    "href": "slides/week-4.html#mean-monthly-rainfall-for-colorado",
    "title": "Week 4",
    "section": "Mean Monthly Rainfall for Colorado",
    "text": "Mean Monthly Rainfall for Colorado\n\n\nglobal\n\nplot(global(prcp$ppt, max)$max, type = \"l\", \n     ylab = \"rainfall\", xlab = \"month since 2000-01\")\nlines(global(prcp$ppt, min)$min, type = \"l\", col = \"blue\")\nlines(global(prcp$ppt, mean)$mean, type = \"l\", col = \"darkred\", lwd = 2)\n\n\n\n\n\n\n\n\n\nmean()\n\nplot(mean(prcp$ppt), col = blues9)\nplot(AOI, add =TRUE, col = NA, lwd = 2)"
  },
  {
    "objectID": "slides/week-4.html#results-1",
    "href": "slides/week-4.html#results-1",
    "title": "Week 4",
    "section": "Results",
    "text": "Results\n\n\n\nplot(foco_elev)\n\n\n\n\n\n\n\n\n\n\nplot(f1)"
  },
  {
    "objectID": "slides/week-4.html#global",
    "href": "slides/week-4.html#global",
    "title": "Week 4",
    "section": "Global",
    "text": "Global\n\nGlobal operations make use of some or all input cells when computing an output cell value.\nThey are a special case of zonal operations with the entire raster represents a single zone.\nExamples include generating descriptive statistics for the entire raster dataset"
  },
  {
    "objectID": "slides/week-4.html#real-example-classify-rainfall-regions-of-colorado",
    "href": "slides/week-4.html#real-example-classify-rainfall-regions-of-colorado",
    "title": "Week 4",
    "section": "Real Example: Classify Rainfall Regions of Colorado",
    "text": "Real Example: Classify Rainfall Regions of Colorado\n\n#remotes::install_github(\"mikejohnson51/climateR\")\nlibrary(climateR)\n\nAOI = AOI::aoi_get(state = 'CO') \n\nsystem.time({ prcp = climateR::getTerraClim(AOI, \"ppt\", \n                                            startDate = \"2000-01-01\", endDate = '2005-12-31') })\n#&gt;    user  system elapsed \n#&gt;   0.255   0.029   2.802\n\n\n# More on global below ...\nquarts = global(prcp$ppt, fivenum)\n\n(quarts = colMeans(quarts))\n#&gt;         X1         X2         X3         X4         X5 \n#&gt;   6.271233  18.434247  27.995890  41.608219 105.642466\n\n(rcl = data.frame(quarts[1:4], quarts[2:5], 1:4))\n#&gt;    quarts.1.4. quarts.2.5. X1.4\n#&gt; X1    6.271233    18.43425    1\n#&gt; X2   18.434247    27.99589    2\n#&gt; X3   27.995890    41.60822    3\n#&gt; X4   41.608219   105.64247    4"
  },
  {
    "objectID": "slides/week-4.html#objectfeild-interaction",
    "href": "slides/week-4.html#objectfeild-interaction",
    "title": "Week 4",
    "section": "Object/Feild Interaction",
    "text": "Object/Feild Interaction\nFor this example, we used OSM to extact the river data for the area of interest. We will talk more about OSM next week:\n\nfoco_rivers &lt;- read_sf(\"data/foco-rivers-osm.gpkg\")"
  },
  {
    "objectID": "slides/week-4.html#objectfield-interaction",
    "href": "slides/week-4.html#objectfield-interaction",
    "title": "Week 4",
    "section": "Object/Field Interaction",
    "text": "Object/Field Interaction\nFor this example, we used OSM to extact the river data for the area of interest. We will talk more about OSM next week:\n\nfoco_rivers &lt;- read_sf(\"data/foco-rivers-osm.gpkg\")"
  },
  {
    "objectID": "labs/lab4.html#libraries",
    "href": "labs/lab4.html#libraries",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(rstac) # STAC API\nlibrary(terra) # Raster Data handling\nlibrary(sf) # Vector data processing\nlibrary(mapview) # Rapid Interactive visualization\n\nAlmost all remote sensing / image analysis begins with the same basic steps:\n\nIdentify an area of interest (AOI)\nIdentify the temporal range of interest\nIdentify the relevant images\nDownload the images\nAnalyze the products"
  },
  {
    "objectID": "labs/lab4.html#step-1-aoi-identification",
    "href": "labs/lab4.html#step-1-aoi-identification",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 1: AOI identification",
    "text": "Step 1: AOI identification\nFirst we need to identify an AOI. We want to be able to extract the flood extents for Palo, Iowa and its surroundings. To do this we will use the geocoding capabilities within the AOI package.\n\npalo &lt;- AOI::geocode(\"Palo, Iowa\", bbox = TRUE)\n\nThis region defines the AOI for this analysis."
  },
  {
    "objectID": "labs/lab4.html#step-2-temporal-identification",
    "href": "labs/lab4.html#step-2-temporal-identification",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 2: Temporal identification",
    "text": "Step 2: Temporal identification\nThe flood event occurred on September 26, 2016. A primary challenge with remote sensing is the fact that all satellite imagery is not available at all times. In this case Landsat 8 has an 8 day revisit time. To ensure we capture an image within the date of the flood, lets set our time range to the period between September 24th - 29th of 2016. We will define this duration in the form YYYY-MM-DD/YYYY-MM-DD.\n\ntemporal_range &lt;- \"2016-09-24/2016-09-29\""
  },
  {
    "objectID": "labs/lab4.html#step-3-identifying-the-relevant-images",
    "href": "labs/lab4.html#step-3-identifying-the-relevant-images",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 3: Identifying the relevant images",
    "text": "Step 3: Identifying the relevant images\nThe next step is to identify the images that are available for our AOI and time range. This is where the rstac package comes in. The rstac package provides a simple interface to the SpatioTemporal Asset Catalog (STAC) API, which is a standard for discovering and accessing geospatial data.\nSTAC is a specification for describing geospatial data in a consistent way, making it easier to discover and access datasets. It provides a standardized way to describe the metadata of geospatial assets, including their spatial and temporal extents, data formats, and other relevant information.\n\nCatalog: A catalog is a collection of STAC items and collections. It serves as a top-level container for organizing and managing geospatial data. A catalog can contain multiple collections, each representing a specific dataset or group of related datasets.\nItems: The basic unit of data in STAC. Each item represents a single asset, such as a satellite image or a vector dataset. Items contain metadata that describes the asset, including its spatial and temporal extents, data format, and other relevant information.\nAsset: An asset is a specific file or data product associated with an item. For example, a single satellite image may have multiple assets, such as different bands or processing levels. Assets are typically stored in a cloud storage system and can be accessed via URLs.\n\nFor this project we are going to use a STAC catalog to identify the data available for our analysis. We want data from the Landsat 8 collection which is served by the USGS (via AWS), Google, and Microsoft Planetary Computer (MPC). MPC is the one that provides free access so we will use that data store.\nIf you go to this link you see the JSON representation of the full data holdings. If you CMD/CTL+F on that page for Landsat you‚Äôll find the references for the available data stores.\nWithin R, we can open a connection to this endpoint with the stac function:\n\n# Open a connection to the MPC STAC API\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\"))\n\n###rstac_query\n- url: https://planetarycomputer.microsoft.com/api/stac/v1/\n- params:\n- field(s): version, base_url, endpoint, params, verb, encode\n\n\nThat connection will provide an open entry to ALL data hosted by MPC. The stac_search function allows us to reduce the catalog to assets that match certain criteria (just like dplyr::filter reduces a data.frame). The get_request() function sends your search to the STAC API returning the metadata about the objects that match a criteria. The service implementation at MPC sets a return limit of 250 items (but it could be overridden with the limit parameter).\nHere, we are interested in the ‚ÄúLandsat Collection 2 Level-2‚Äù data. From the JSON file (seen in the browser). To start, lets search for that collection using the stac -&gt; stac_search ‚Äì&gt; get_request workflow:\n\n(stac_query &lt;-stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\") |&gt; \n  get_request())\n\n###Items\n- features (250 item(s)):\n  - LC09_L2SP_015248_20250414_02_T1\n  - LC09_L2SP_015247_20250414_02_T1\n  - LC09_L2SP_015054_20250414_02_T1\n  - LC09_L2SP_015053_20250414_02_T1\n  - LC09_L2SP_015052_20250414_02_T2\n  - LC09_L2SP_015051_20250414_02_T2\n  - LC09_L2SP_015050_20250414_02_T1\n  - LC09_L2SP_015049_20250414_02_T1\n  - LC09_L2SP_015048_20250414_02_T2\n  - LC09_L2SP_015047_20250414_02_T1\n  - ... with 240 more feature(s).\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nAwesome! So the first 250 items from the Level-2 Landsat collection were returned. Within each item, there are a number of assets (e.g.¬†the red, green, blue bands) and all items have some associated fields like the sub item assets, the bounding box, etc. We can now refine our search to limit the returned results to those that cover our AOI and time range of interest:\n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo)) |&gt; \n  get_request())\n\n###Items\n- features (2 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n  - LE07_L2SP_026031_20160925_02_T1\n- assets: \nang, atmos_opacity, atran, blue, cdist, cloud_qa, coastal, drad, emis, emsd, green, lwir, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nBy adding these constraints, we now see just two items. One from the Landsat 7 Level 2 dataset, and one from the Landsat 8 Level 2 dataset. For this lab, lets focus on the Landsat 8 item. We can use either the item or the id search criteria to elect this:\n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo),\n    limit = 1) |&gt; \n  get_request())\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n## OR ## \n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    id = 'LC08_L2SP_025031_20160926_02_T1',\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo)) |&gt; \n  get_request())\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nThe last thing we need to do, is sign this request. In rstac, items_sign(sign_planetary_computer()) signs STAC item asset URLs retrieved from Microsoft‚Äôs Planetary Computer, ensuring they include authentication tokens for access. sign_planetary_computer() generates the necessary signing function, and items_sign() applies it to STAC items. This is essential for accessing datasets hosted on the Planetary Computer, and other catalog were data access might be requester-paid or limited.\n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo),\n    limit = 1) |&gt; \n  get_request() |&gt; \n  items_sign(sign_planetary_computer()))\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type"
  },
  {
    "objectID": "labs/lab4.html#step-4-downloading-needed-images",
    "href": "labs/lab4.html#step-4-downloading-needed-images",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 4: Downloading needed images",
    "text": "Step 4: Downloading needed images\nOK! Now that we have identified the item we want, we are ready to download the data using assets_download(). In total, a Landsat 8 item has the following 11 bands:\n\nknitr::include_graphics(\"images/lsat8-bands.jpg\")\n\n\n\n\n\n\n\n\nFor this lab, lets just get the first 6 bands. Assets are extracted from a STAC item by the asset name (look at the print statements of the stac_query). Let‚Äôs define a vector of the assets we want:\n\n# Bands 1-6\nbands &lt;- c('coastal', 'blue', 'green', 'red', 'nir08', 'swir16')\n\nNow we can use the assets_download() function to download the data. The output_dir argument specifies where to save the files, and the overwrite argument specifies whether to overwrite existing files with the same name.\n\nassets_download(items = stac_query,\n                asset_names = bands, \n                output_dir = '../data', \n                overwrite = TRUE)\n\nAnd that does it! You now have the process needed to get you data.\nWith a set of local files, you can create a raster object! Remember your files need to be in the order of the bands (double check step 2).\n\nlist.files() can search a directory for a pattern and return a list of files. The recursive argument will search all sub-directories. The full.names argument will return the full path to the files.\nThe rast() function will read the files into a raster object.\nThe setNames() function will set the names of the bands to the names we defined above."
  },
  {
    "objectID": "labs/lab4.html#step-5-analyize-the-images",
    "href": "labs/lab4.html#step-5-analyize-the-images",
    "title": "Lab 4: Rasters & Remote Sensing",
    "section": "Step 5: Analyize the images",
    "text": "Step 5: Analyize the images\nWe only want to analyze our image for the regions surrounding Palo (our AOI). Transform your AOI to the CRS of the landsat stack and use it to crop your raster stack.\nAwesome! We have now (1) identified, (2) downloaded, and (3) saved our images.\nWe have loaded them as a multiband SpatRast object and cropped the domain to our AOI. Lets make a few RGB plots to see what these images reveal.\n\n\n\n\n\n\nNote\n\n\n\nThere are many online examples that discuss the applications of the available bands in Landsat. What we want to do here is simply show how loading different combinations into the RGB channels make different features stand out. A useful reference of popular band combinations is here."
  },
  {
    "objectID": "slides/week-5.html#unit-3-modeling-machine-learning",
    "href": "slides/week-5.html#unit-3-modeling-machine-learning",
    "title": "Week 5",
    "section": "Unit 3: Modeling (Machine Learning)",
    "text": "Unit 3: Modeling (Machine Learning)\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "slides/week-5.html#what-is-machine-learning",
    "href": "slides/week-5.html#what-is-machine-learning",
    "title": "Week 5",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/week-5.html#what-is-machine-learning-2025-edition",
    "href": "slides/week-5.html#what-is-machine-learning-2025-edition",
    "title": "Week 5",
    "section": "What is machine learning? (2025 edition)",
    "text": "What is machine learning? (2025 edition)\n\n\n\n\n\n\nNote\n\n\nIn the early 2010s, ‚ÄúArtificial intelligence‚Äù (AI) was largely synonymous with what we‚Äôll refer to as ‚Äúmachine learning‚Äù in this workshop. In the late 2010s and early 2020s, AI usually referred to deep learning methods. Since the release of ChatGPT in late 2022, ‚ÄúAI‚Äù has come to also encompass large language models (LLMs) / generative models.\n\n\n\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/week-5.html#classic-conceptual-model",
    "href": "slides/week-5.html#classic-conceptual-model",
    "title": "Week 5",
    "section": "Classic Conceptual Model",
    "text": "Classic Conceptual Model\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/week-5.html#what-is-tidymodels",
    "href": "slides/week-5.html#what-is-tidymodels",
    "title": "Week 5",
    "section": "What is tidymodels?",
    "text": "What is tidymodels?\n\nAn R package ecosystem for modeling and machine learning\nBuilt on the tidyverse principles (consistent syntax, modular design)\nProvides a structured workflow for preprocessing, model fitting, and evaluation\nIncludes packages for model specification, preprocessing, resampling, tuning, and evaluation\nPromotes best practices for reproducibility and efficiency\nOffers a unified interface for different models and tasks"
  },
  {
    "objectID": "slides/week-5.html#the-big-picture-road-map",
    "href": "slides/week-5.html#the-big-picture-road-map",
    "title": "Week 5",
    "section": "The big picture: Road map",
    "text": "The big picture: Road map"
  },
  {
    "objectID": "slides/week-5.html#tidymodels",
    "href": "slides/week-5.html#tidymodels",
    "title": "Week 5",
    "section": "Tidymodels",
    "text": "Tidymodels"
  },
  {
    "objectID": "slides/week-5.html#what-is-tidymodels-1",
    "href": "slides/week-5.html#what-is-tidymodels-1",
    "title": "Week 5",
    "section": "What is tidymodels?",
    "text": "What is tidymodels?\n\nAn R package ecosystem for modeling and machine learning\nBuilt on the tidyverse principles (consistent syntax, modular design)\nProvides a structured workflow for preprocessing, model fitting, and evaluation\nIncludes packages for model specification, preprocessing, resampling, tuning, and evaluation\nPromotes best practices for reproducibility and efficiency\nOffers a unified interface for different models and tasks"
  },
  {
    "objectID": "slides/week-5.html#key-tidymodels-packages",
    "href": "slides/week-5.html#key-tidymodels-packages",
    "title": "Week 5",
    "section": "Key tidymodels Packages",
    "text": "Key tidymodels Packages\nüì¶ recipes: Feature engineering and preprocessing\nüì¶ parsnip: Unified interface for model specification\nüì¶ workflows: Streamlined modeling pipelines\nüì¶ tune: Hyperparameter tuning\nüì¶ rsample: Resampling and validation\nüì¶ yardstick: Model evaluation metrics\n\nknitr::include_graphics('images/tidymodels-packages.png')\n\ntidymodels::tidymodels_packages()\n#&gt;  [1] \"broom\"        \"cli\"          \"conflicted\"   \"dials\"        \"dplyr\"       \n#&gt;  [6] \"ggplot2\"      \"hardhat\"      \"infer\"        \"modeldata\"    \"parsnip\"     \n#&gt; [11] \"purrr\"        \"recipes\"      \"rlang\"        \"rsample\"      \"rstudioapi\"  \n#&gt; [16] \"tibble\"       \"tidyr\"        \"tune\"         \"workflows\"    \"workflowsets\"\n#&gt; [21] \"yardstick\"    \"tidymodels\""
  },
  {
    "objectID": "slides/week-5.html#typcical-workflow-in-tidymodels",
    "href": "slides/week-5.html#typcical-workflow-in-tidymodels",
    "title": "Week 5",
    "section": "Typcical Workflow in tidymodels",
    "text": "Typcical Workflow in tidymodels\n1Ô∏è‚É£ Load Package & Data\n2Ô∏è‚É£ Preprocess Data (recipes)\n3Ô∏è‚É£ Define Model (parsnip)\n4Ô∏è‚É£ Create a Workflow (workflows)\n5Ô∏è‚É£ Train & Tune (tune)\n6Ô∏è‚É£ Evaluate Performance (yardstick)"
  },
  {
    "objectID": "slides/week-5.html#data-normalization-tidymodels",
    "href": "slides/week-5.html#data-normalization-tidymodels",
    "title": "Week 5",
    "section": "Data Normalization: tidymodels",
    "text": "Data Normalization: tidymodels\n\nData normalization is a crucial step in data preprocessing for machine learning.\nIt ensures that features contribute equally to model performance by transforming them into a common scale.\nThis is particularly important for algorithms that rely on distance metrics (e.g., k-nearest neighbors, support vector machines) or gradient-based optimization (e.g., neural networks, logistic regression).\nIn the tidymodels framework, data normalization is handled within preprocessing workflows using the recipes package, which provides a structured way to apply transformations consistently."
  },
  {
    "objectID": "slides/week-5.html#why-normalize-data-for-ml",
    "href": "slides/week-5.html#why-normalize-data-for-ml",
    "title": "Week 5",
    "section": "Why Normalize Data for ML?",
    "text": "Why Normalize Data for ML?\n‚úÖ Improves Model Convergence: Many machine learning models rely on gradient descent, which can be inefficient if features are on different scales.\n‚úÖ Prevents Feature Domination: Features with large magnitudes can overshadow smaller ones, leading to biased models.\n‚úÖ Enhances Interpretability: Standardized data improves comparisons across variables and aids in better model understanding.\n‚úÖ Facilitates Distance-Based Methods: Algorithms like k-nearest neighbors and principal component analysis (PCA) perform best when data is normalized."
  },
  {
    "objectID": "slides/week-5.html#common-normalization-techniques",
    "href": "slides/week-5.html#common-normalization-techniques",
    "title": "Week 5",
    "section": "Common Normalization Techniques",
    "text": "Common Normalization Techniques\n1. Min-Max Scaling\n\nTransforms data into a fixed range (typically [0,1]).\nPreserves relationships but sensitive to outliers.\nFormula: \\[ x' = \\frac{x - x_{min}}{x_{max} - x_{min}} \\]\nImplemented using step_range() in recipes.\n\n2. Z-Score Standardization (Standard Scaling)\n\nCenters data to have zero mean and unit variance.\nMore robust to outliers compared to min-max scaling.\nFormula: \\[ x' = \\frac{x - \\mu}{\\sigma} \\]\nImplemented using step_normalize() in recipes."
  },
  {
    "objectID": "slides/week-5.html#common-normalization-techniques-1",
    "href": "slides/week-5.html#common-normalization-techniques-1",
    "title": "Week 5",
    "section": "Common Normalization Techniques",
    "text": "Common Normalization Techniques\n3. Robust Scaling\n\nUses median and interquartile range (IQR) for scaling.\nLess sensitive to outliers.\nFormula: \\[ x' = \\frac{x - median(x)}{IQR(x)} \\]\nImplemented using step_YeoJohnson() or step_BoxCox() in recipes.\n\n4. Log Transformation\n\nUseful for skewed distributions to reduce the impact of extreme values.\nImplemented using step_log() in recipes."
  },
  {
    "objectID": "slides/week-5.html#common-normalization-techniques-2",
    "href": "slides/week-5.html#common-normalization-techniques-2",
    "title": "Week 5",
    "section": "Common Normalization Techniques",
    "text": "Common Normalization Techniques\n5. Principal Component Analysis (PCA)\n\nProjects data into a lower-dimensional space while maintaining variance.\nUsed in high-dimensional datasets.\nImplemented using step_pca() in recipes."
  },
  {
    "objectID": "slides/week-5.html#other-feature-engineering-tasks",
    "href": "slides/week-5.html#other-feature-engineering-tasks",
    "title": "Week 5",
    "section": "Other Feature Engineering Tasks",
    "text": "Other Feature Engineering Tasks\nHandling Missing Data\n\nstep_impute_mean()\nstep_impute_median()\nstep_impute_knn()\n\ncan be used to fill missing values.\nEncoding Categorical Variables\n\nstep_dummy() creates dummy variables for categorical features.\nstep_other() groups infrequent categories into an ‚Äúother‚Äù category.\n\nCreating Interaction Terms\n\nstep_interact() generates interaction terms between predictors.\n\nFeature Extraction\n\nstep_pca() or step_ica() can be used to extract important components.\n\nText Feature Engineering\nstep_tokenize(), step_stopwords(), and step_tf() for text processing.\nBinning Numerical Data\nstep_bin2factor() converts continuous variables into categorical bins.\nPolynomial and Spline Features\nstep_poly() and step_bs() for generating polynomial and spline transformations."
  },
  {
    "objectID": "slides/week-5.html#implementing-normalization-in-tidymodels",
    "href": "slides/week-5.html#implementing-normalization-in-tidymodels",
    "title": "Week 5",
    "section": "Implementing Normalization in Tidymodels",
    "text": "Implementing Normalization in Tidymodels"
  },
  {
    "objectID": "slides/week-5.html#what-will-we-do",
    "href": "slides/week-5.html#what-will-we-do",
    "title": "Week 5",
    "section": "What will we do?",
    "text": "What will we do?\n\nWe will create a recipe to normalize the numerical predictors in the penguins dataset using the recipes package.\nWe will then prepare (prep) the recipe and apply it to the dataset to obtain normalized data.\nWe will then bake the recipe to apply the transformations to the dataset.\nOnce processed, we can implement a linear regression model using the normalized data.\n\n\nExample Workflow\n\n\n\nlibrary(tidymodels)\n\n(recipe_obj &lt;- recipe(flipper_length_mm ~ \n                       bill_length_mm + body_mass_g + \n                       sex + island + species, \n                      data = penguins) |&gt; \n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_dummy(all_factor_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()))\n\n# Prepare and apply transformations\nprep_recipe     &lt;- prep(recipe_obj, training = penguins) \n\nnormalized_data &lt;- bake(prep_recipe, new_data = NULL) |&gt; \n  mutate(species = penguins$species) |&gt; \n  drop_na()\n\nhead(normalized_data)\n#&gt; # A tibble: 6 √ó 9\n#&gt;   bill_length_mm body_mass_g flipper_length_mm sex_male island_Dream\n#&gt;            &lt;dbl&gt;       &lt;dbl&gt;             &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1         -0.886      -0.565               181    0.990       -0.750\n#&gt; 2         -0.812      -0.502               186   -1.01        -0.750\n#&gt; 3         -0.665      -1.19                195   -1.01        -0.750\n#&gt; 4         -1.33       -0.940               193   -1.01        -0.750\n#&gt; 5         -0.849      -0.690               190    0.990       -0.750\n#&gt; 6         -0.923      -0.721               181   -1.01        -0.750\n#&gt; # ‚Ñπ 4 more variables: island_Torgersen &lt;dbl&gt;, species_Chinstrap &lt;dbl&gt;,\n#&gt; #   species_Gentoo &lt;dbl&gt;, species &lt;fct&gt;\n\n\nExplanation:\n\nrecipe() defines preprocessing steps for modeling.\nstep_impute_mean(all_numeric_predictors()) standardizes all numerical predictors.\nstep_dummy(all_factor_predictors()) creates dummy variables for categorical predictors.\nstep_normalize(all_numeric_predictors())) standardizes all numerical predictors.\nprep() estimates parameters for transformations.\nbake() applies the transformations to the dataset."
  },
  {
    "objectID": "slides/week-5.html#build-a-model-with-normalized-data",
    "href": "slides/week-5.html#build-a-model-with-normalized-data",
    "title": "Week 5",
    "section": "Build a model with Normalized Data",
    "text": "Build a model with Normalized Data\n\n\n\n(model = lm(flipper_length_mm ~ . , data = normalized_data) )\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = flipper_length_mm ~ ., data = normalized_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)     bill_length_mm        body_mass_g           sex_male  \n#&gt;          200.9649             2.2754             4.6414             0.7208  \n#&gt;      island_Dream   island_Torgersen  species_Chinstrap     species_Gentoo  \n#&gt;            0.6525             0.9800             0.5640             8.0796  \n#&gt;  speciesChinstrap      speciesGentoo  \n#&gt;                NA                 NA"
  },
  {
    "objectID": "slides/week-5.html#build-a-model-with-normalized-data-1",
    "href": "slides/week-5.html#build-a-model-with-normalized-data-1",
    "title": "Week 5",
    "section": "Build a model with Normalized Data",
    "text": "Build a model with Normalized Data\n\n\n\n(model = lm(flipper_length_mm ~ . , data = normalized_data) )\n\nglance(model)\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = flipper_length_mm ~ ., data = normalized_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)     bill_length_mm        body_mass_g           sex_male  \n#&gt;          200.9649             2.2754             4.6414             0.7208  \n#&gt;      island_Dream   island_Torgersen  species_Chinstrap     species_Gentoo  \n#&gt;            0.6525             0.9800             0.5640             8.0796  \n#&gt;  speciesChinstrap      speciesGentoo  \n#&gt;                NA                 NA\n#&gt; # A tibble: 1 √ó 12\n#&gt;   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.863         0.861  5.23      294. 2.15e-136     7 -1020. 2057. 2092.\n#&gt; # ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "slides/week-5.html#build-a-model-with-normalized-data-2",
    "href": "slides/week-5.html#build-a-model-with-normalized-data-2",
    "title": "Week 5",
    "section": "Build a model with Normalized Data",
    "text": "Build a model with Normalized Data\n\n\n\n(model = lm(flipper_length_mm ~ . , data = normalized_data) )\n\nglance(model)\n\n(pred &lt;- augment(model, normalized_data))\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = flipper_length_mm ~ ., data = normalized_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)     bill_length_mm        body_mass_g           sex_male  \n#&gt;          200.9649             2.2754             4.6414             0.7208  \n#&gt;      island_Dream   island_Torgersen  species_Chinstrap     species_Gentoo  \n#&gt;            0.6525             0.9800             0.5640             8.0796  \n#&gt;  speciesChinstrap      speciesGentoo  \n#&gt;                NA                 NA\n#&gt; # A tibble: 1 √ó 12\n#&gt;   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.863         0.861  5.23      294. 2.15e-136     7 -1020. 2057. 2092.\n#&gt; # ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n#&gt; # A tibble: 333 √ó 15\n#&gt;    bill_length_mm body_mass_g flipper_length_mm sex_male island_Dream\n#&gt;             &lt;dbl&gt;       &lt;dbl&gt;             &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1         -0.886      -0.565               181    0.990       -0.750\n#&gt;  2         -0.812      -0.502               186   -1.01        -0.750\n#&gt;  3         -0.665      -1.19                195   -1.01        -0.750\n#&gt;  4         -1.33       -0.940               193   -1.01        -0.750\n#&gt;  5         -0.849      -0.690               190    0.990       -0.750\n#&gt;  6         -0.923      -0.721               181   -1.01        -0.750\n#&gt;  7         -0.867       0.592               195    0.990       -0.750\n#&gt;  8         -0.518      -1.25                182   -1.01        -0.750\n#&gt;  9         -0.978      -0.502               191    0.990       -0.750\n#&gt; 10         -1.71        0.248               198    0.990       -0.750\n#&gt; # ‚Ñπ 323 more rows\n#&gt; # ‚Ñπ 10 more variables: island_Torgersen &lt;dbl&gt;, species_Chinstrap &lt;dbl&gt;,\n#&gt; #   species_Gentoo &lt;dbl&gt;, species &lt;fct&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#&gt; #   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "slides/week-5.html#build-a-model-with-normalized-data-3",
    "href": "slides/week-5.html#build-a-model-with-normalized-data-3",
    "title": "Week 5",
    "section": "Build a model with Normalized Data",
    "text": "Build a model with Normalized Data\n\n\n\n(model = lm(flipper_length_mm ~ . , data = normalized_data) )\n\nglance(model)\n\n(pred &lt;- augment(model, normalized_data))\n\nggscatter(pred,\n          x = 'flipper_length_mm', y = '.fitted',\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"pearson\",\n          color = \"species\", palette = \"jco\")\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = flipper_length_mm ~ ., data = normalized_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)     bill_length_mm        body_mass_g           sex_male  \n#&gt;          200.9649             2.2754             4.6414             0.7208  \n#&gt;      island_Dream   island_Torgersen  species_Chinstrap     species_Gentoo  \n#&gt;            0.6525             0.9800             0.5640             8.0796  \n#&gt;  speciesChinstrap      speciesGentoo  \n#&gt;                NA                 NA\n#&gt; # A tibble: 1 √ó 12\n#&gt;   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.863         0.861  5.23      294. 2.15e-136     7 -1020. 2057. 2092.\n#&gt; # ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n#&gt; # A tibble: 333 √ó 15\n#&gt;    bill_length_mm body_mass_g flipper_length_mm sex_male island_Dream\n#&gt;             &lt;dbl&gt;       &lt;dbl&gt;             &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1         -0.886      -0.565               181    0.990       -0.750\n#&gt;  2         -0.812      -0.502               186   -1.01        -0.750\n#&gt;  3         -0.665      -1.19                195   -1.01        -0.750\n#&gt;  4         -1.33       -0.940               193   -1.01        -0.750\n#&gt;  5         -0.849      -0.690               190    0.990       -0.750\n#&gt;  6         -0.923      -0.721               181   -1.01        -0.750\n#&gt;  7         -0.867       0.592               195    0.990       -0.750\n#&gt;  8         -0.518      -1.25                182   -1.01        -0.750\n#&gt;  9         -0.978      -0.502               191    0.990       -0.750\n#&gt; 10         -1.71        0.248               198    0.990       -0.750\n#&gt; # ‚Ñπ 323 more rows\n#&gt; # ‚Ñπ 10 more variables: island_Torgersen &lt;dbl&gt;, species_Chinstrap &lt;dbl&gt;,\n#&gt; #   species_Gentoo &lt;dbl&gt;, species &lt;fct&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#&gt; #   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "slides/week-5.html#data-budget",
    "href": "slides/week-5.html#data-budget",
    "title": "Week 5",
    "section": "Data Budget",
    "text": "Data Budget\n\nIn any modeling effort, it‚Äôs crucial to evaluate the performance of a model using different validation techniques to ensure a model can generalize to unseen data.\nBut data is limited even in the age of ‚Äúbig data‚Äù."
  },
  {
    "objectID": "slides/week-5.html#our-typical-process-will-like-this",
    "href": "slides/week-5.html#our-typical-process-will-like-this",
    "title": "Week 5",
    "section": "Our typical process will like this:",
    "text": "Our typical process will like this:\n\nRead in raw data (readr, dplyr::*_join) (single or multi table)\n\n\n\nPrepare the data (EDA/mutate/summarize/clean, etc)\n\n\nThis is ‚ÄúFeature Engineering‚Äù\n\n\n\n\nOnce we‚Äôve established our features (rows), we decide how to ‚Äúspend‚Äù them ‚Ä¶\n\n\n\nFor machine learning, we typically split data into training and test sets:\n\n\n\nThe training set is used to estimate model parameters.\nSpending too much data in training prevents us from computing a good assessment of model performance.\n\n\n\n\nThe test set is used as an independent assessment of performance.\nSpending too much data in testing prevents us from computing good model parameter estimates."
  },
  {
    "objectID": "slides/week-5.html#how-we-split-data",
    "href": "slides/week-5.html#how-we-split-data",
    "title": "Week 5",
    "section": "How we split data",
    "text": "How we split data\n\n\n\nSplitting can be handled in many of ways. Typically, we base it off of a ‚Äúhold out‚Äù percentage (e.g.¬†20%)\nThese hold out cases are extracted randomly from the data set. (remember seeds?)\n\n\n\n\nThe training set is usually the majority of the data and provides a sandbox for testing different modeling appraoches.\n\n\n\n\nThe test set is held in reserve until one or two models are chosen.\nThe test set is then used as the final arbiter to determine the efficacy of the model."
  },
  {
    "objectID": "slides/week-5.html#startification",
    "href": "slides/week-5.html#startification",
    "title": "Week 5",
    "section": "Startification",
    "text": "Startification\n\nIn many cases, there is a structure to the data that would inhibit inpartial spliiting (e.g.¬†a class, a region, a species, a sex, etc)\nImagine you have a jar of M&Ms in different colors ‚Äî red, blue, and green. You want to take some candies out to taste, but you want to make sure you get a fair mix of each color, not just grabbing a bunch of red ones by accident.\nStratified resampling is like making sure that if your jar is 50% red, 30% blue, and 20% green, then the handful of candies you take keeps the same balance.\nIn data science, we do the same thing when we pick samples from a dataset: we make sure that different groups (like categories of people, animals, or weather types) are still fairly represented!"
  },
  {
    "objectID": "slides/week-5.html#initial-splits",
    "href": "slides/week-5.html#initial-splits",
    "title": "Week 5",
    "section": "Initial splits ",
    "text": "Initial splits \n\nIn tidymodels, the rsample package provides functions for creating initial splits of data.\nThe initial_split() function is used to create a single split of the data.\nThe prop argument defines the proportion of the data to be used for training.\nThe default is 0.75, which means 75% of the data will be used for training and 25% for testing.\n\n\nset.seed(101991)\n(resample_split &lt;- initial_split(penguins, prop = 0.8))\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;275/69/344&gt;\n\n#Sanity check\n69/344\n#&gt; [1] 0.2005814"
  },
  {
    "objectID": "slides/week-5.html#accessing-the-data",
    "href": "slides/week-5.html#accessing-the-data",
    "title": "Week 5",
    "section": "Accessing the data: ",
    "text": "Accessing the data: \n\nOnce the data is split, we can access the training and testing data using the training() and testing() functions to extract the partitioned data from the full set:\n\n\npenguins_train &lt;- training(resample_split)\nglimpse(penguins_train)\n#&gt; Rows: 275\n#&gt; Columns: 7\n#&gt; $ species           &lt;fct&gt; Gentoo, Adelie, Gentoo, Chinstrap, Gentoo, Chinstrap‚Ä¶\n#&gt; $ island            &lt;fct&gt; Biscoe, Torgersen, Biscoe, Dream, Biscoe, Dream, Dre‚Ä¶\n#&gt; $ bill_length_mm    &lt;dbl&gt; 46.2, 43.1, 46.2, 45.9, 46.5, 48.1, 45.2, 55.9, 38.1‚Ä¶\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 14.9, 19.2, 14.4, 17.1, 13.5, 16.4, 17.8, 17.0, 17.0‚Ä¶\n#&gt; $ flipper_length_mm &lt;int&gt; 221, 197, 214, 190, 210, 199, 198, 228, 181, 195, 20‚Ä¶\n#&gt; $ body_mass_g       &lt;int&gt; 5300, 3500, 4650, 3575, 4550, 3325, 3950, 5600, 3175‚Ä¶\n#&gt; $ sex               &lt;fct&gt; male, male, NA, female, female, female, female, male‚Ä¶\n\n\n\npenguins_test &lt;- testing(resample_split)\nglimpse(penguins_test)\n#&gt; Rows: 69\n#&gt; Columns: 7\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Biscoe, Biscoe, Bis‚Ä¶\n#&gt; $ bill_length_mm    &lt;dbl&gt; NA, 39.3, 36.6, 35.9, 38.8, 37.9, 39.2, 39.6, 36.7, ‚Ä¶\n#&gt; $ bill_depth_mm     &lt;dbl&gt; NA, 20.6, 17.8, 19.2, 17.2, 18.6, 21.1, 17.2, 18.8, ‚Ä¶\n#&gt; $ flipper_length_mm &lt;int&gt; NA, 190, 185, 189, 180, 172, 196, 196, 187, 205, 187‚Ä¶\n#&gt; $ body_mass_g       &lt;int&gt; NA, 3650, 3700, 3800, 3800, 3150, 4150, 3550, 3800, ‚Ä¶\n#&gt; $ sex               &lt;fct&gt; NA, male, female, female, male, female, male, female‚Ä¶"
  },
  {
    "objectID": "slides/week-5.html#proportional-gaps",
    "href": "slides/week-5.html#proportional-gaps",
    "title": "Week 5",
    "section": "Proportional Gaps",
    "text": "Proportional Gaps\n\n# Dataset\n(table(penguins$species) / nrow(penguins))\n#&gt; \n#&gt;    Adelie Chinstrap    Gentoo \n#&gt; 0.4418605 0.1976744 0.3604651\n# Training Data\n(table(penguins_train$species) / nrow(penguins_train))\n#&gt; \n#&gt;    Adelie Chinstrap    Gentoo \n#&gt; 0.4581818 0.1818182 0.3600000\n# Testing Data\n(table(penguins_test$species) / nrow(penguins_test))\n#&gt; \n#&gt;    Adelie Chinstrap    Gentoo \n#&gt; 0.3768116 0.2608696 0.3623188"
  },
  {
    "objectID": "slides/week-5.html#stratification-example",
    "href": "slides/week-5.html#stratification-example",
    "title": "Week 5",
    "section": "Stratification Example",
    "text": "Stratification Example\n\nA stratified random sample conducts a specified split within defined subsets of subsets, and then pools the results.\nOnly one column can be used to define a strata but grouping/mutate opperations can be used to create a new column that can be used as a strata (e.g.¬†species/island)\nIn the case of the penguins data, we can stratify by species (think back to our nested/linear model appraoch)\nThis ensures that the training and testing sets have the same proportion of each species as the original data.\n\n\n# Set the seed\nset.seed(123)\n\n# Drop missing values and split the data\npenguins &lt;- drop_na(penguins)\npenguins_strata &lt;- initial_split(penguins, strata = species, prop = .8)\n\n# Extract the training and testing sets\n\ntrain_strata &lt;- training(penguins_strata)\ntest_strata  &lt;- testing(penguins_strata)"
  },
  {
    "objectID": "slides/week-5.html#proportional-alignment",
    "href": "slides/week-5.html#proportional-alignment",
    "title": "Week 5",
    "section": "Proportional Alignment",
    "text": "Proportional Alignment\n\n# Check the proportions\n# Dataset\ntable(penguins$species) / nrow(penguins)\n#&gt; \n#&gt;    Adelie Chinstrap    Gentoo \n#&gt; 0.4384384 0.2042042 0.3573574\n# Training Data\ntable(train_strata$species) / nrow(train_strata)\n#&gt; \n#&gt;    Adelie Chinstrap    Gentoo \n#&gt; 0.4377358 0.2037736 0.3584906\n# Testing Data\ntable(test_strata$species) / nrow(test_strata)\n#&gt; \n#&gt;    Adelie Chinstrap    Gentoo \n#&gt; 0.4411765 0.2058824 0.3529412"
  },
  {
    "objectID": "slides/week-5.html#modeling",
    "href": "slides/week-5.html#modeling",
    "title": "Week 5",
    "section": "Modeling",
    "text": "Modeling\n\n\n\nOnce the data is split, we can decide what type of model to invoke.\nOften, users simply pick a well known model type or class for a type of problem (Classic Conceptual Model)\nWe will learn more about model types and uses next week!\nFor now, just know that the training data is used to fit a model.\nIf we are certain about the model, we can use the test data to evaluate the model."
  },
  {
    "objectID": "slides/week-5.html#modeling-options",
    "href": "slides/week-5.html#modeling-options",
    "title": "Week 5",
    "section": "Modeling Options",
    "text": "Modeling Options\n\nBut, there are many types of models, with different assumptions, behaviors, and qualities, that make some more applicable to a given dataset!\nMost models have some type of parmeterization, that can often be tuned.\nTesting combinations of model and tuning parmaters also requires some combintation of training/testing splits."
  },
  {
    "objectID": "slides/week-5.html#modeling-options-1",
    "href": "slides/week-5.html#modeling-options-1",
    "title": "Week 5",
    "section": "Modeling Options",
    "text": "Modeling Options\nWhat if we want to compare more models?\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?\n\n\nHow can we use the training data to compare and evaluate different models? ü§î"
  },
  {
    "objectID": "slides/week-5.html#resampling",
    "href": "slides/week-5.html#resampling",
    "title": "Week 5",
    "section": "Resampling",
    "text": "Resampling\n\n\n\nTesting combinations of model and tuning parmaters also requires some combintation of training/testing splits.\nResampling methods, such as cross-validation and bootstraping, are empirical simulation systems that help facilitate this.\nThey create a series of data sets similar to the initial training/testing split.\nIn the first level of the diagram to the right, we first split the original data into training/testing sets. Then, the training set is chosen for resampling."
  },
  {
    "objectID": "slides/week-5.html#key-resampling-methods",
    "href": "slides/week-5.html#key-resampling-methods",
    "title": "Week 5",
    "section": "Key Resampling Methods",
    "text": "Key Resampling Methods\n\n\n\n\n\n\nWarning\n\n\nResampling is always used with the training set.\n\n\n\nResampling is a key step in model validation and assessment. The rsample package provides tools to create resampling strategies such as cross-validation, bootstrapping, and validation splits.\n\n\nK-Fold Cross-Validation: The dataset is split into multiple ‚Äúfolds‚Äù (e.g., 5-fold or 10-fold), where each fold is used as a validation set while the remaining data is used for training.\n\n\n\n\nBootstrap Resampling: Samples are drawn with replacement from the training dataset to generate multiple datasets, which used to train and test the model.\n\n\n\n\nMonte Carlo Resampling (Repeated Train-Test Splits): Randomly splits data multiple times.\n\n\n\n\nValidation Split: Creates a single training and testing partition."
  },
  {
    "objectID": "slides/week-5.html#cross-validation",
    "href": "slides/week-5.html#cross-validation",
    "title": "Week 5",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/week-5.html#cross-validation-1",
    "href": "slides/week-5.html#cross-validation-1",
    "title": "Week 5",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/week-5.html#cross-validation-2",
    "href": "slides/week-5.html#cross-validation-2",
    "title": "Week 5",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\n\n\npenguins_train |&gt; glimpse() \n#&gt; Rows: 275\n#&gt; Columns: 7\n#&gt; $ species           &lt;fct&gt; Gentoo, Adelie, Gentoo, Chinstrap, Gentoo, Chinstrap‚Ä¶\n#&gt; $ island            &lt;fct&gt; Biscoe, Torgersen, Biscoe, Dream, Biscoe, Dream, Dre‚Ä¶\n#&gt; $ bill_length_mm    &lt;dbl&gt; 46.2, 43.1, 46.2, 45.9, 46.5, 48.1, 45.2, 55.9, 38.1‚Ä¶\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 14.9, 19.2, 14.4, 17.1, 13.5, 16.4, 17.8, 17.0, 17.0‚Ä¶\n#&gt; $ flipper_length_mm &lt;int&gt; 221, 197, 214, 190, 210, 199, 198, 228, 181, 195, 20‚Ä¶\n#&gt; $ body_mass_g       &lt;int&gt; 5300, 3500, 4650, 3575, 4550, 3325, 3950, 5600, 3175‚Ä¶\n#&gt; $ sex               &lt;fct&gt; male, male, NA, female, female, female, female, male‚Ä¶\n\n\n\nnrow(penguins_train) * 1/10\n#&gt; [1] 27.5\n\nvfold_cv(penguins_train, v = 10) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 √ó 2\n#&gt;    splits           id    \n#&gt;    &lt;list&gt;           &lt;chr&gt; \n#&gt;  1 &lt;split [247/28]&gt; Fold01\n#&gt;  2 &lt;split [247/28]&gt; Fold02\n#&gt;  3 &lt;split [247/28]&gt; Fold03\n#&gt;  4 &lt;split [247/28]&gt; Fold04\n#&gt;  5 &lt;split [247/28]&gt; Fold05\n#&gt;  6 &lt;split [248/27]&gt; Fold06\n#&gt;  7 &lt;split [248/27]&gt; Fold07\n#&gt;  8 &lt;split [248/27]&gt; Fold08\n#&gt;  9 &lt;split [248/27]&gt; Fold09\n#&gt; 10 &lt;split [248/27]&gt; Fold10"
  },
  {
    "objectID": "slides/week-5.html#cross-validation-3",
    "href": "slides/week-5.html#cross-validation-3",
    "title": "Week 5",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\npenguins_folds &lt;- vfold_cv(penguins_train)\npenguins_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;247/28/275&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;247/28/275&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;247/28/275&gt;\n\n\n\n\n\n\n\nNote\n\n\nHere is another example of a list column enabling the storage of non-atomic types in tibble\n\n\n\n\n\n\n\n\n\nImportant\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "slides/week-5.html#bootstrapping",
    "href": "slides/week-5.html#bootstrapping",
    "title": "Week 5",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "slides/week-5.html#bootstrapping-1",
    "href": "slides/week-5.html#bootstrapping-1",
    "title": "Week 5",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(penguins_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 √ó 2\n#&gt;    splits            id         \n#&gt;    &lt;list&gt;            &lt;chr&gt;      \n#&gt;  1 &lt;split [275/96]&gt;  Bootstrap01\n#&gt;  2 &lt;split [275/105]&gt; Bootstrap02\n#&gt;  3 &lt;split [275/108]&gt; Bootstrap03\n#&gt;  4 &lt;split [275/111]&gt; Bootstrap04\n#&gt;  5 &lt;split [275/102]&gt; Bootstrap05\n#&gt;  6 &lt;split [275/98]&gt;  Bootstrap06\n#&gt;  7 &lt;split [275/92]&gt;  Bootstrap07\n#&gt;  8 &lt;split [275/97]&gt;  Bootstrap08\n#&gt;  9 &lt;split [275/100]&gt; Bootstrap09\n#&gt; 10 &lt;split [275/99]&gt;  Bootstrap10\n#&gt; # ‚Ñπ 15 more rows"
  },
  {
    "objectID": "slides/week-5.html#monte-carlo-cross-validation",
    "href": "slides/week-5.html#monte-carlo-cross-validation",
    "title": "Week 5",
    "section": "Monte Carlo Cross-Validation ",
    "text": "Monte Carlo Cross-Validation \n\nset.seed(322)\nmc_cv(penguins_train, times = 10)\n#&gt; # Monte Carlo cross-validation (0.75/0.25) with 10 resamples  \n#&gt; # A tibble: 10 √ó 2\n#&gt;    splits           id        \n#&gt;    &lt;list&gt;           &lt;chr&gt;     \n#&gt;  1 &lt;split [206/69]&gt; Resample01\n#&gt;  2 &lt;split [206/69]&gt; Resample02\n#&gt;  3 &lt;split [206/69]&gt; Resample03\n#&gt;  4 &lt;split [206/69]&gt; Resample04\n#&gt;  5 &lt;split [206/69]&gt; Resample05\n#&gt;  6 &lt;split [206/69]&gt; Resample06\n#&gt;  7 &lt;split [206/69]&gt; Resample07\n#&gt;  8 &lt;split [206/69]&gt; Resample08\n#&gt;  9 &lt;split [206/69]&gt; Resample09\n#&gt; 10 &lt;split [206/69]&gt; Resample10"
  },
  {
    "objectID": "slides/week-5.html#validation-set",
    "href": "slides/week-5.html#validation-set",
    "title": "Week 5",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\npenguins_val_split &lt;- initial_validation_split(penguins)\npenguins_val_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;199/67/67/333&gt;\nvalidation_set(penguins_val_split)\n#&gt; # A tibble: 1 √ó 2\n#&gt;   splits           id        \n#&gt;   &lt;list&gt;           &lt;chr&gt;     \n#&gt; 1 &lt;split [199/67]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "slides/week-5.html#the-whole-game---status-update",
    "href": "slides/week-5.html#the-whole-game---status-update",
    "title": "Week 5",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "slides/week-5.html#data-on-forests-in-washington",
    "href": "slides/week-5.html#data-on-forests-in-washington",
    "title": "Week 5",
    "section": "Data on forests in Washington",
    "text": "Data on forests in Washington\n\n\n\nThe U.S. Forest Service maintains ML models to predict whether a plot of land is ‚Äúforested.‚Äù\nThis classification is important for all sorts of research, legislation, and land management purposes.\nPlots are typically remeasured every 10 years and this dataset contains the most recent measurement per plot.\nType ?forested to learn more about this dataset, including references.\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/251793/forest-mountain"
  },
  {
    "objectID": "slides/week-5.html#data-on-forests-in-washington-1",
    "href": "slides/week-5.html#data-on-forests-in-washington-1",
    "title": "Week 5",
    "section": "Data on forests in Washington",
    "text": "Data on forests in Washington\n\n\n\nlibrary(tidymodels)\nlibrary(forested)\ndim(forested)\n#&gt; [1] 7107   19\n\n\nOne observation from each of 7,107 6000-acre hexagons in Washington state.\nA nominal outcome, forested, with levels \"Yes\" and \"No\", measured ‚Äúon-the-ground.‚Äù\n\n\ntable(forested$forested)\n#&gt; \n#&gt;  Yes   No \n#&gt; 3894 3213\n\n\n18 remotely-sensed and easily-accessible predictors:\n\n\nnames(forested)\n#&gt;  [1] \"forested\"         \"year\"             \"elevation\"        \"eastness\"        \n#&gt;  [5] \"northness\"        \"roughness\"        \"tree_no_tree\"     \"dew_temp\"        \n#&gt;  [9] \"precip_annual\"    \"temp_annual_mean\" \"temp_annual_min\"  \"temp_annual_max\" \n#&gt; [13] \"temp_january_min\" \"vapor_min\"        \"vapor_max\"        \"canopy_cover\"    \n#&gt; [17] \"lon\"              \"lat\"              \"land_type\"\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/67614/forest"
  },
  {
    "objectID": "slides/week-5.html#data-splitting-and-spending",
    "href": "slides/week-5.html#data-splitting-and-spending",
    "title": "Week 5",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nThe initial split \n\nset.seed(123)\nforested_split &lt;- initial_split(forested, prop = .8, strata = tree_no_tree)\nforested_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5685/1422/7107&gt;"
  },
  {
    "objectID": "slides/week-5.html#accessing-the-data-1",
    "href": "slides/week-5.html#accessing-the-data-1",
    "title": "Week 5",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nforested_train &lt;- training(forested_split)\nforested_test  &lt;- testing(forested_split)\n\nnrow(forested_train)\n#&gt; [1] 5685\nnrow(forested_test)\n#&gt; [1] 1422"
  },
  {
    "objectID": "slides/week-5.html#k-fold-cross-validation",
    "href": "slides/week-5.html#k-fold-cross-validation",
    "title": "Week 5",
    "section": "K-Fold Cross-Validation ",
    "text": "K-Fold Cross-Validation \n\n# Load the dataset\nset.seed(123)\n\n# Create a 10-fold cross-validation object\n(forested_folds &lt;- vfold_cv(forested_train, v = 10))\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 √ó 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [5116/569]&gt; Fold01\n#&gt;  2 &lt;split [5116/569]&gt; Fold02\n#&gt;  3 &lt;split [5116/569]&gt; Fold03\n#&gt;  4 &lt;split [5116/569]&gt; Fold04\n#&gt;  5 &lt;split [5116/569]&gt; Fold05\n#&gt;  6 &lt;split [5117/568]&gt; Fold06\n#&gt;  7 &lt;split [5117/568]&gt; Fold07\n#&gt;  8 &lt;split [5117/568]&gt; Fold08\n#&gt;  9 &lt;split [5117/568]&gt; Fold09\n#&gt; 10 &lt;split [5117/568]&gt; Fold10"
  },
  {
    "objectID": "slides/week-5.html#how-do-you-fit-a-linear-model-in-r",
    "href": "slides/week-5.html#how-do-you-fit-a-linear-model-in-r",
    "title": "Week 5",
    "section": "How do you fit a linear model in R?",
    "text": "How do you fit a linear model in R?\n\nlm for linear model &lt;‚Äì The one we have looked at!\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression using Stan\nspark for large data sets using spark\nbrulee for regression using torch (PyTourch)"
  },
  {
    "objectID": "slides/week-5.html#challenge",
    "href": "slides/week-5.html#challenge",
    "title": "Week 5",
    "section": "Challenge",
    "text": "Challenge\n\nAll of these models have different syntax and functions\nHow do you keep track of all of them?\nHow do you know which one to use?\nHow would you compare them?\n\nComparing 3-5 of these models is a lot of work using functions for diverse packages."
  },
  {
    "objectID": "slides/week-5.html#the-tidymodels-advantage",
    "href": "slides/week-5.html#the-tidymodels-advantage",
    "title": "Week 5",
    "section": "The tidymodels advantage",
    "text": "The tidymodels advantage\n\n\n\nIn the tidymodels framework, all models are created using the same syntax:\n\nThis makes it easy to compare models\nThis makes it easy to switch between models\nThis makes it easy to use the same model with different engines (packages)\n\nThe parsnip package provides a consistent interface to many models\nFor example, to fit a linear model you would be able to access the linear_reg() function\n\n\n\n?linear_reg"
  },
  {
    "objectID": "slides/week-5.html#a-tidymodels-prediction-will",
    "href": "slides/week-5.html#a-tidymodels-prediction-will",
    "title": "Week 5",
    "section": "A tidymodels prediction will ‚Ä¶ ",
    "text": "A tidymodels prediction will ‚Ä¶ \n\nalways be inside a tibble\nhave column names and types are unsurprising and predictable\nensure the number of rows in new_data and the output are the same"
  },
  {
    "objectID": "slides/week-5.html#to-specify-a-model",
    "href": "slides/week-5.html#to-specify-a-model",
    "title": "Week 5",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/week-5.html#specify-a-model-type",
    "href": "slides/week-5.html#specify-a-model-type",
    "title": "Week 5",
    "section": "Specify a model: Type ",
    "text": "Specify a model: Type \nNext week we will discuss model types more thoroughly. For now, we will focus on two types:\n\nLogistic Regression\n\n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\nWe chose these two because they are robust, simple model that fit our goal of predicting a binary condition/class (forested/not forested)"
  },
  {
    "objectID": "slides/week-5.html#specify-a-model-engine",
    "href": "slides/week-5.html#specify-a-model-engine",
    "title": "Week 5",
    "section": "Specify a model: engine ",
    "text": "Specify a model: engine \n\n\n\nChoose a model\nSpecify an engine\nSet the mode\n\n\n\n?logistic_reg()"
  },
  {
    "objectID": "slides/week-5.html#specify-a-model-engine-1",
    "href": "slides/week-5.html#specify-a-model-engine-1",
    "title": "Week 5",
    "section": "Specify a model: engine ",
    "text": "Specify a model: engine \n\nlogistic_reg() %&gt;%\n  set_engine(\"glmnet\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glmnet\n\n\n\n\nlogistic_reg() %&gt;%\n  set_engine(\"stan\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "slides/week-5.html#specify-a-model-mode",
    "href": "slides/week-5.html#specify-a-model-mode",
    "title": "Week 5",
    "section": "Specify a model: mode ",
    "text": "Specify a model: mode \n\n\n\nChoose a model\nSpecify an engine\nSet the mode\n\n\n\n?logistic_reg()"
  },
  {
    "objectID": "slides/week-5.html#specify-a-model-mode-1",
    "href": "slides/week-5.html#specify-a-model-mode-1",
    "title": "Week 5",
    "section": "Specify a model: mode ",
    "text": "Specify a model: mode \nSome models have limit classes ‚Ä¶\n\nlogistic_reg() \n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n. . .\nOthers requires specification ‚Ä¶\n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\ndecision_tree() %&gt;% \n  set_mode(\"classification\")\n#&gt; Decision Tree Model Specification (classification)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "slides/week-5.html#to-specify-a-model-1",
    "href": "slides/week-5.html#to-specify-a-model-1",
    "title": "Week 5",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/week-5.html#logistic-regression",
    "href": "slides/week-5.html#logistic-regression",
    "title": "Week 5",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\nLogistic regression predicts probability‚Äîinstead of a straight line, it gives an S-shaped curve that estimates how likely an outcome (e.g., is forested) is based on a predictor (e.g., rainfall and temperature).\nThe dots in the plot show the actual proportion of ‚Äúsuccesses‚Äù (e.g., is forested) within different bins of the predictor variable(s) (A).\nThe vertical error bars represent uncertainty‚Äîshowing a range where the true probability might fall.\nLogistic regression helps answer ‚Äúhow likely‚Äù questions‚Äîe.g., ‚ÄúHow likely is someone to pass based on their study hours?‚Äù rather than just predicting a yes/no outcome."
  },
  {
    "objectID": "slides/week-5.html#decision-trees",
    "href": "slides/week-5.html#decision-trees",
    "title": "Week 5",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "slides/week-5.html#decision-trees-1",
    "href": "slides/week-5.html#decision-trees-1",
    "title": "Week 5",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "slides/week-5.html#decision-trees-2",
    "href": "slides/week-5.html#decision-trees-2",
    "title": "Week 5",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "slides/week-5.html#what-algorithm-is-best-for-estimate-forested-plots",
    "href": "slides/week-5.html#what-algorithm-is-best-for-estimate-forested-plots",
    "title": "Week 5",
    "section": "What algorithm is best for estimate forested plots?",
    "text": "What algorithm is best for estimate forested plots?\nWe can only really know by testing them ‚Ä¶\n\n\nLogistic regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "slides/week-5.html#fitting-your-model",
    "href": "slides/week-5.html#fitting-your-model",
    "title": "Week 5",
    "section": "Fitting your model:",
    "text": "Fitting your model:\n\nOK! So you have split data, you have a model, and you have an engine and mode\nNow you need to fit the model!\nThis is done using the fit() function\n\n\ndt_model &lt;- decision_tree() %&gt;% \n  set_engine('rpart') %&gt;%\n  set_mode(\"classification\")\n\n# Model, formula, data inputs\n(output &lt;- fit(dt_model, forested ~ ., data = forested_train) )\n#&gt; parsnip model object\n#&gt; \n#&gt; n= 5685 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 5685 2571 Yes (0.54775726 0.45224274)  \n#&gt;    2) land_type=Tree 3025  289 Yes (0.90446281 0.09553719) *\n#&gt;    3) land_type=Barren,Non-tree vegetation 2660  378 No (0.14210526 0.85789474)  \n#&gt;      6) temp_annual_max&lt; 13.395 351  155 Yes (0.55840456 0.44159544)  \n#&gt;       12) tree_no_tree=Tree 91    5 Yes (0.94505495 0.05494505) *\n#&gt;       13) tree_no_tree=No tree 260  110 No (0.42307692 0.57692308) *\n#&gt;      7) temp_annual_max&gt;=13.395 2309  182 No (0.07882200 0.92117800) *"
  },
  {
    "objectID": "slides/week-5.html#component-extracts",
    "href": "slides/week-5.html#component-extracts",
    "title": "Week 5",
    "section": "Component extracts:",
    "text": "Component extracts:\n\n\n\nWhile tidymodels provides a standard interface to all models all base models have specific methods (remember summary.lm? )\nTo use these, you must extract a the underlying object.\nextract_fit_engine() extracts the underlying model object\nextract_fit_parsnip() extracts the parsnip object\nextract_recipe() extracts the recipe object\nextract_preprocessor() extracts the preprocessor object\n‚Ä¶ many more\n\n\n\nex &lt;- extract_fit_engine(output)\nclass(ex) # the class of the engine used!\n#&gt; [1] \"rpart\"\n\n# Use rpart plot and text methods ...\nplot(ex)\ntext(ex, cex = 0.9, use.n = TRUE, xpd = TRUE,)"
  },
  {
    "objectID": "slides/week-5.html#workflows",
    "href": "slides/week-5.html#workflows",
    "title": "Week 5",
    "section": "Workflows? ",
    "text": "Workflows? \nThe workflows package in tidymodels helps:\n\nManage preprocessing and modeling in a structured pipeline\nAvoid repetitive code\nReduce errors when integrating steps\n\n\nThink of them as containers for:\n\nPreprocessing steps (e.g., feature engineering, transformations)\nModel specification\nFitting approaches\n\n\n\nWhy Use Workflows?\n‚úÖ Keeps preprocessing and modeling together\n\n‚úÖ Ensures consistency in data handling ¬†\n‚úÖ Makes code easier to read and maintain"
  },
  {
    "objectID": "slides/week-5.html#basic-workflow-structure",
    "href": "slides/week-5.html#basic-workflow-structure",
    "title": "Week 5",
    "section": "Basic Workflow Structure: ",
    "text": "Basic Workflow Structure: \n\nCreate a workflow object (similar to how ggplot() instantiates a canvas)\nAdd a formula or recipe (preprocessor)\nAdd the model\nFit the workflow"
  },
  {
    "objectID": "slides/week-5.html#a-model-workflow-1",
    "href": "slides/week-5.html#a-model-workflow-1",
    "title": "Week 5",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ndt_wf &lt;- workflow() %&gt;%\n  add_formula(forested ~ .) %&gt;%\n  add_model(dt_model) %&gt;%\n  fit(data = forested_train)\n\n\n\n#&gt; ‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; forested ~ .\n#&gt; \n#&gt; ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; n= 5685 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 5685 2571 Yes (0.54775726 0.45224274)  \n#&gt;    2) land_type=Tree 3025  289 Yes (0.90446281 0.09553719) *\n#&gt;    3) land_type=Barren,Non-tree vegetation 2660  378 No (0.14210526 0.85789474)  \n#&gt;      6) temp_annual_max&lt; 13.395 351  155 Yes (0.55840456 0.44159544)  \n#&gt;       12) tree_no_tree=Tree 91    5 Yes (0.94505495 0.05494505) *\n#&gt;       13) tree_no_tree=No tree 260  110 No (0.42307692 0.57692308) *\n#&gt;      7) temp_annual_max&gt;=13.395 2309  182 No (0.07882200 0.92117800) *"
  },
  {
    "objectID": "slides/week-5.html#predict-with-your-model",
    "href": "slides/week-5.html#predict-with-your-model",
    "title": "Week 5",
    "section": "Predict with your model ",
    "text": "Predict with your model \nHow do you use your new model?\n\naugment() will return the dataset with predictions and residuals added.\n\n\ndt_preds &lt;- augment(dt_wf, new_data = forested_test)\n\n\n\n#&gt; # A tibble: 1,422 √ó 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Yes            0.904    0.0955 No        2005       164      -84        53\n#&gt;  2 Yes            0.904    0.0955 Yes       2005       806       47       -88\n#&gt;  3 No             0.423    0.577  Yes       2005      2240      -67       -74\n#&gt;  4 Yes            0.904    0.0955 Yes       2005       787       66       -74\n#&gt;  5 Yes            0.904    0.0955 Yes       2005      1330       99         7\n#&gt;  6 Yes            0.904    0.0955 Yes       2005      1423       46        88\n#&gt;  7 Yes            0.904    0.0955 Yes       2014       546      -92       -38\n#&gt;  8 Yes            0.904    0.0955 Yes       2014      1612       30       -95\n#&gt;  9 No             0.0788   0.921  No        2014       235        0      -100\n#&gt; 10 No             0.0788   0.921  No        2014       308      -70       -70\n#&gt; # ‚Ñπ 1,412 more rows\n#&gt; # ‚Ñπ 14 more variables: roughness &lt;dbl&gt;, tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;,\n#&gt; #   precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;,\n#&gt; #   temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;,\n#&gt; #   vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;"
  },
  {
    "objectID": "slides/week-5.html#evaluate-your-model",
    "href": "slides/week-5.html#evaluate-your-model",
    "title": "Week 5",
    "section": "Evaluate your model ",
    "text": "Evaluate your model \nHow do you evaluate the skill of your models?\nWe will learn more about model evaluation and tuning latter in this unit, but for now we can blindly use the metrics() function defaults to get a sense of how our model is doing.\n\nThe default metrics for classification models are accuracy and kap (Cohen‚Äôs Kappa)\nThe default metric for regression models are RMSE, R^2, and MAE\n\n\n# We have a classification model\nmetrics(dt_preds, truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 √ó 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.885\n#&gt; 2 kap      binary         0.768"
  },
  {
    "objectID": "slides/week-5.html#so-who-wins",
    "href": "slides/week-5.html#so-who-wins",
    "title": "Week 5",
    "section": "So who wins?",
    "text": "So who wins?\n\nmetrics(dt_preds, truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 √ó 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.885\n#&gt; 2 kap      binary         0.768\nmetrics(log_preds, truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 √ó 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.890\n#&gt; 2 kap      binary         0.777\n\nRight out of the gate, there doesn‚Äôt seem to be much difference, but ‚Ä¶"
  },
  {
    "objectID": "slides/week-5.html#dont-get-to-confident",
    "href": "slides/week-5.html#dont-get-to-confident",
    "title": "Week 5",
    "section": "Don‚Äôt get to confident!",
    "text": "Don‚Äôt get to confident!\n\nWe have evaluated the model on the same data we trained it on\nThis is not a good practice and can give us a false sense of confidence\nInstead, we can use our resamples to better understand the skill of a model based on the iterative leave out policy:\n\n\ndt_wf_rs &lt;- workflow() %&gt;%\n  add_formula(forested ~ .) %&gt;%\n  add_model(dt_model) %&gt;%\n  fit_resamples(resamples = forested_folds, \n                # Here we just ask tidymodels to save all predictions...\n                control   = control_resamples(save_pred = TRUE))\n\n\n\n#&gt; # A tibble: 1 √ó 5\n#&gt;   splits             id     .metrics         .notes           .predictions      \n#&gt;   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n#&gt; 1 &lt;split [5116/569]&gt; Fold01 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [569 √ó 6]&gt;\n\n\n\n\n\n\n\nNote\n\n\ntidyr::unnest could be used to expand any of those list columns!"
  },
  {
    "objectID": "slides/week-5.html#dont-get-to-confident-1",
    "href": "slides/week-5.html#dont-get-to-confident-1",
    "title": "Week 5",
    "section": "Don‚Äôt get to confident!",
    "text": "Don‚Äôt get to confident!\n\nWe can execute the same workflow for the logistic regression model, this time evaluating the resamples instead of the test data.\n\n\nlog_wf_rs &lt;- workflow() %&gt;%\n  add_formula(forested ~ .) %&gt;%\n  add_model(log_mod) %&gt;%\n  fit_resamples(resamples = forested_folds,\n                control = control_resamples(save_pred = TRUE))\n\n\n\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 √ó 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [5116/569]&gt; Fold01 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [5116/569]&gt; Fold02 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [5116/569]&gt; Fold03 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [5116/569]&gt; Fold04 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [5116/569]&gt; Fold05 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [5117/568]&gt; Fold06 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [5117/568]&gt; Fold07 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [5117/568]&gt; Fold08 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [5117/568]&gt; Fold09 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [5117/568]&gt; Fold10 &lt;tibble [3 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "slides/week-5.html#dont-get-to-confident-2",
    "href": "slides/week-5.html#dont-get-to-confident-2",
    "title": "Week 5",
    "section": "Don‚Äôt get to confident!",
    "text": "Don‚Äôt get to confident!\n\nSince we now have an ‚Äúensemble‚Äù of models (across the folds), we can collect a summary of the metrics across them.\nThe default metrics for classification ensembles are:\n\naccuracy: accuracy is the proportion of correct predictions\nbrier_class: the Brier score for classification is the mean squared difference between the predicted probability and the actual outcome\nroc_auc: the area under the ROC (Receiver Operating Characteristic) curve\n\n\n\n\ncollect_metrics(log_wf_rs)\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.906     10 0.00360 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0714    10 0.00191 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.961     10 0.00164 Preprocessor1_Model1\ncollect_metrics(dt_wf_rs)\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.896     10 0.00385 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0889    10 0.00224 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.909     10 0.00267 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-5.html#further-simplification-workflowsets",
    "href": "slides/week-5.html#further-simplification-workflowsets",
    "title": "Week 5",
    "section": "Further simplification: workflowsets",
    "text": "Further simplification: workflowsets\nOK, so we have streamlined a lot of things with tidymodels:\n\nWe have a specified preprocesser (e.g.¬†formula or recipe)\nWe have defined a model (complete with engine and mode)\nWe have implemented workflows pairing each model with the preprocesser to either fit the model to the resamples, or, the training data.\n\nWhile a significant improvement, we can do better!\n\nDoes the idea of implementing a process over an list of elements ring a bell?"
  },
  {
    "objectID": "slides/week-5.html#model-mappings",
    "href": "slides/week-5.html#model-mappings",
    "title": "Week 5",
    "section": "Model mappings: ",
    "text": "Model mappings: \n\nworkflowsets is a package that builds off of purrr and allows you to iterate over multiple models and/or multiple resamples\nRemember how map, map_*, map2, and walk2 functions allow lists to map to lists or vectors? workflow_set maps a preprocessor (formula or recipe) to a set of models - each provided as a list object.\nTo start, we will create a workflow_set object (instead of a workflow). The first argument is a list of preprocessor objects (formulas or recipes) and the second argument is a list of model objects.\nBoth must be lists, by nature of the underlying purrr code.\n\n\n(wf_obj &lt;- workflow_set(list(forested ~.), list(log_mod, dt_model)))\n#&gt; # A workflow set/tibble: 2 √ó 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_logistic_reg  &lt;tibble [1 √ó 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_decision_tree &lt;tibble [1 √ó 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "slides/week-5.html#iteritive-extecution",
    "href": "slides/week-5.html#iteritive-extecution",
    "title": "Week 5",
    "section": "Iteritive extecution",
    "text": "Iteritive extecution\n\nOnce the workflow_set object is created, the workflow_map function can be used to map a function across the preprocessor/model combinations.\nHere we are mapping the fit_resamples function across the workflow_set combinations using the resamples argument to specify the resamples we want to use (forested_folds).\n\n\nwf_obj &lt;- \n  workflow_set(list(forested ~.), list(log_mod, dt_model)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = forested_folds) \n\n\n\n#&gt; # A workflow set/tibble: 2 √ó 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_logistic_reg  &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_decision_tree &lt;tibble [1 √ó 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "slides/week-5.html#rapid-comparision",
    "href": "slides/week-5.html#rapid-comparision",
    "title": "Week 5",
    "section": "Rapid comparision",
    "text": "Rapid comparision\nWith that single function, all models have been fit to the resamples and we can quickly compare the results both graphically and statistically:\n\n\n\n# Quick plot function\nautoplot(wf_obj) + \n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n# Long list of results, ranked by accuracy\nrank_results(wf_obj, \n            rank_metric = \"accuracy\", \n            select_best = TRUE)\n#&gt; # A tibble: 6 √ó 9\n#&gt;   wflow_id         .config .metric   mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_logisti‚Ä¶ Prepro‚Ä¶ accura‚Ä¶ 0.906  0.00360    10 formula      logi‚Ä¶     1\n#&gt; 2 formula_logisti‚Ä¶ Prepro‚Ä¶ brier_‚Ä¶ 0.0714 0.00191    10 formula      logi‚Ä¶     1\n#&gt; 3 formula_logisti‚Ä¶ Prepro‚Ä¶ roc_auc 0.961  0.00164    10 formula      logi‚Ä¶     1\n#&gt; 4 formula_decisio‚Ä¶ Prepro‚Ä¶ accura‚Ä¶ 0.896  0.00385    10 formula      deci‚Ä¶     2\n#&gt; 5 formula_decisio‚Ä¶ Prepro‚Ä¶ brier_‚Ä¶ 0.0889 0.00224    10 formula      deci‚Ä¶     2\n#&gt; 6 formula_decisio‚Ä¶ Prepro‚Ä¶ roc_auc 0.909  0.00267    10 formula      deci‚Ä¶     2\n\n\n\nOverall, the logistic regression model appears to the best model for this data set!"
  },
  {
    "objectID": "slides/week-5.html#the-whole-game---status-update-1",
    "href": "slides/week-5.html#the-whole-game---status-update-1",
    "title": "Week 5",
    "section": "The whole game - status update",
    "text": "The whole game - status update\n\nOn Wednesday, we‚Äôll talk more about models we can chose from‚Ä¶\nNext week we will cover"
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "In this lab, we will explore predictive modeling in hydrology using the tidymodels framework and the CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) dataset.\n\n\n\n\n\n\n\n\n\n\n\ntidymodels is an R framework designed for machine learning and statistical modeling. Built on the principles of the tidyverse, tidymodels provides a consistent and modular approach to tasks like data preprocessing, model training, evaluation, and validation. By leveraging the strengths of packages such as recipes, parsnip, and yardstick, tidymodels streamlines the modeling workflow, making it easier to experiment with different models while maintaining reproducibility and interpretability.\n\n\n\nThe CAMELS dataset is a widely used resource in hydrology and environmental science, providing data on over 500 self-draining river basins across the United States. It includes meteorological forcings, streamflow observations, and catchment attributes such as land cover, topography, and soil properties. This dataset is particularly valuable for large-sample hydrology studies, enabling researchers to develop and test models across diverse climatic and physiographic conditions.\nIn this lab, we will focus on predicting mean streamflow for these basins using their associated characteristics. CAMELS has been instrumental in various hydrologic and machine learning applications, including:\n\nCalibrating Hydrologic Models ‚Äì Used for parameter tuning in models like SAC-SMA, VIC, and HBV, improving regional and large-sample studies.\nTraining Machine Learning Models ‚Äì Supports deep learning (e.g., LSTMs) and regression-based streamflow predictions, often outperforming traditional methods.\nUnderstanding Model Behavior ‚Äì Assists in assessing model generalization, uncertainty analysis, and the role of catchment attributes.\nBenchmarking & Regionalization ‚Äì Facilitates large-scale model comparisons and parameter transfer to ungauged basins.\nHybrid Modeling ‚Äì Enhances physics-based models with machine learning for bias correction and improved hydrologic simulations.\n\nA notable study by Kratzert et al.¬†(2019) demonstrated that LSTMs can outperform conceptual models in streamflow prediction. As part of this lab, we will explore how to programmatically download and load the data into R."
  },
  {
    "objectID": "labs/lab5.html#what-is-tidymodels",
    "href": "labs/lab5.html#what-is-tidymodels",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "tidymodels is an R framework designed for machine learning and statistical modeling. Built on the principles of the tidyverse, tidymodels provides a consistent and modular approach to tasks like data preprocessing, model training, evaluation, and validation. By leveraging the strengths of packages such as recipes, parsnip, and yardstick, tidymodels streamlines the modeling workflow, making it easier to experiment with different models while maintaining reproducibility and interpretability."
  },
  {
    "objectID": "labs/lab5.html#what-is-the-camels-dataset",
    "href": "labs/lab5.html#what-is-the-camels-dataset",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "The CAMELS dataset is a widely used resource in hydrology and environmental science, providing data on over 500 self-draining river basins across the United States. It includes meteorological forcings, streamflow observations, and catchment attributes such as land cover, topography, and soil properties. This dataset is particularly valuable for large-sample hydrology studies, enabling researchers to develop and test models across diverse climatic and physiographic conditions.\nIn this lab, we will focus on predicting mean streamflow for these basins using their associated characteristics. CAMELS has been instrumental in various hydrologic and machine learning applications, including:\n\nCalibrating Hydrologic Models ‚Äì Used for parameter tuning in models like SAC-SMA, VIC, and HBV, improving regional and large-sample studies.\nTraining Machine Learning Models ‚Äì Supports deep learning (e.g., LSTMs) and regression-based streamflow predictions, often outperforming traditional methods.\nUnderstanding Model Behavior ‚Äì Assists in assessing model generalization, uncertainty analysis, and the role of catchment attributes.\nBenchmarking & Regionalization ‚Äì Facilitates large-scale model comparisons and parameter transfer to ungauged basins.\nHybrid Modeling ‚Äì Enhances physics-based models with machine learning for bias correction and improved hydrologic simulations.\n\nA notable study by Kratzert et al.¬†(2019) demonstrated that LSTMs can outperform conceptual models in streamflow prediction. As part of this lab, we will explore how to programmatically download and load the data into R."
  },
  {
    "objectID": "labs/lab5.html#lab-goals",
    "href": "labs/lab5.html#lab-goals",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Lab Goals",
    "text": "Lab Goals\nIn this lab, you will:\n\nLearn how to programatically download and access data.\nPractice using tidymodels for predictive modeling.\nTrain and evaluate models to predict mean streamflow across the country.\nInterpret and compare model performance using workflows.\n\nBy the end of this lab, you will have hands-on experience applying machine learning techniques to real-world data, helping to bridge the gap between statistical modeling and environmental science."
  },
  {
    "objectID": "labs/lab5.html#data-download",
    "href": "labs/lab5.html#data-download",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Data Download",
    "text": "Data Download\nThe CAMELS dataset is hosted by NCAR and can be accessed here under the ‚ÄúIndividual Files‚Äù section. The root URL for all data seen on the ‚ÄúIndividual Files‚Äù page is:\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\nNear the bottom of that page, there are many .txt files that contain the data we want. Some hold climate data for each basin, some hold geology data, some hold soil data, etc. There is also a PDF with descriptions of the columns in each file. We are going to download all of the .txt files and the PDF."
  },
  {
    "objectID": "labs/lab5.html#getting-the-documentation-pdf",
    "href": "labs/lab5.html#getting-the-documentation-pdf",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Getting the documentation PDF",
    "text": "Getting the documentation PDF\nWe can download the documentation PDF which provides a descriptions for the various columns as many are not self-explanatory. Here we can use download.file to download the PDF to our data directory.\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              '../data/camels_attributes_v2.0.pdf')"
  },
  {
    "objectID": "labs/lab5.html#getting-basin-characteristics",
    "href": "labs/lab5.html#getting-basin-characteristics",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Getting Basin characteristics",
    "text": "Getting Basin characteristics\nNow we want to download the .txt files that store the actual data documented in the PDF. Doing this file by file (like we did with the PDF) is possible, but lets look at a better/easier way‚Ä¶\n\nLets create a vector storing the data types/file names we want to download:\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n\n\n\n\n\n\nglue\n\n\n\n\n\nThe glue package provides an efficient way to interpolate and manipulate strings. It is particularly useful for dynamically constructing text, formatting outputs, and embedding R expressions within strings.\n\nKey Features of glue:\n\nString Interpolation: Embed R expressions inside strings using {}.\nImproved Readability: Eliminates the need for cumbersome paste(), paste0() and sprintf() commands.\nMulti-line Strings: Easily handle multi-line text formatting.\nSafe and Efficient: Optimized for performance and readability.\n\n\n\nBasic Usage\n\nTo use glue, you need to load the package and then call the glue() function with the desired string template. You can embed R expressions within curly braces {} to interpolate values into the string.\n\n\nclass &lt;- \"ESS 330\"\nyear  &lt;- 2024\n\nglue(\"We are taking {class} together in {year}\")\n\nWe are taking ESS 330 together in 2024\n\n\n\n\nMultiples\n\nYou can also use glue to interpolate multiple values at once\n\n\nclasses &lt;- c(\"ESS 330\", \"ESS 523c\")\n\nglue(\"We are taking {classes} together in {year}\")\n\nWe are taking ESS 330 together in 2024\nWe are taking ESS 523c together in 2024\n\n\n\n\n\n\n\nUsing glue, we can construct the needed URLs and file names for the data we want to download:\n\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n\nNow we can download the data: walk2 comes from the purrr package and is used to apply a function to multiple arguments in parallel (much like map2 works over paired lists). Here, we are asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.\n\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\nOnce downloaded, the data can be read it into R using readr::read_delim(), again instead of applying this to each file individually, we can use map to apply the function to each element of the local_files list.\n\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n\nThis gives us a list of data.frames, one for each file that we want to merge into a single table. So far in class we have focused on *_join functions to merge data based on a primary and foreign key relationship.\n\nIn this current list, we have &gt;2 tables, but, have a shared column called gauge_id that we can use to merge the data. However, since we have more then a left and right hand table, we need a more robust tool. We will use the powerjoin package to merge the data into a single data frame. powerjoin is a flexible package for joining lists of data.frames. It provides a wide range of join types, including inner, left, right, full, semi, anti, and cross joins making it a versatile tool for data manipulation and analysis, and one that should feel familiar to users of dplyr.\nIn this case, we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, we could have read straight form the urls. Strongly consider the implications of this approach as the longevity and persistence of the data is not guaranteed.\n\n# Read and merge data\ncamels &lt;- map(remote_files, read_delim, show_col_types = FALSE) |&gt; \n  power_full_join(by = 'gauge_id')"
  },
  {
    "objectID": "labs/lab5.html#exploratory-data-analysis",
    "href": "labs/lab5.html#exploratory-data-analysis",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirst, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\nWe can take a moment here to are learn a few new things about ggplot2!\n\n\n\n\n\n\nColor scales\n\n\n\nIn ggplot, sometimes you want different things (like bars, dots, or lines) to have different colors. But how does R know which colors to use? That‚Äôs where color scales come in!\nscales can be used to map data values to colors (scale_color_*) or fill aesthetics (scale_fill_*). There are two main types of color scales:\n\nDiscrete color scales ‚Äì for things that are categories, like ‚Äúapples,‚Äù ‚Äúbananas,‚Äù and ‚Äúcherries.‚Äù Each gets its own separate color.\n\n\nscale_color_manual(values = c(\"red\", \"yellow\", \"pink\")) #lets you pick your own colors.\n\nOr\n\nscale_color_brewer(palette = \"Set1\") #uses a built-in color set.\n\n\nContinuous color scales ‚Äì for numbers, like temperature (cold to hot) or height (short to tall). The color changes smoothly.\n\n\nscale_color_gradient(low = \"blue\", high = \"red\") #makes small numbers blue and big numbers red."
  },
  {
    "objectID": "labs/lab5.html#visual-eda",
    "href": "labs/lab5.html#visual-eda",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Visual EDA",
    "text": "Visual EDA\n\nLets start by looking that the 3 dimensions (variables) of this data. We‚Äôll start with a XY plot of aridity and rainfall. We are going to use the scale_color_viridis_c() function to color the points by the q_mean column. This scale functions maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color_* function is applied, it maps to the known color aesthetic in the plot.\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOk! so it looks like there is a relationship between rainfall, aridity, and rainfall but it looks like an exponential decay function and is certainly not linear.\nTo test a transformation, we can log transform the x and y axes using the scale_x_log10() and scale_y_log10() functions:\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nGreat! We can see a log-log relationship between aridity and rainfall provides a more linear relationship. This is a common relationship in hydrology and is often used to estimate rainfall in ungauged basins. However, once the data is transformed, the lack of spread in the streamflow data is quite evident with high mean flow values being compressed to the low end of aridity/high end of rainfall.\nTo address this, we can visualize how a log transform may benifit the q_mean data as well. Since the data is represented by color, rather then an axis, we can use the trans (transform) argument in the scale_color_viridis_c() function to log transform the color scale.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nExcellent! Treating these three right skewed variables as log transformed, we can see a more evenly spread relationship between aridity, rainfall, and mean flow. This is a good sign for building a model to predict mean flow using aridity and rainfall."
  },
  {
    "objectID": "labs/lab5.html#lets-start-by-splitting-the-data",
    "href": "labs/lab5.html#lets-start-by-splitting-the-data",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Lets start by splitting the data",
    "text": "Lets start by splitting the data\nFirst, we set a seed for reproducabilty, then transform the q_mean column to a log scale. Remember it is error prone to apply transformations to the outcome variable within a recipe. So, we‚Äôll do it a prioi.\nOnce set, we can split the data into a training and testing set. We are going to use 80% of the data for training and 20% for testing with no stratification.\nAdditionally, we are going to create a 10-fold cross validation dataset to help us evaluate multi-model setups.\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)"
  },
  {
    "objectID": "labs/lab5.html#preprocessor-recipe",
    "href": "labs/lab5.html#preprocessor-recipe",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Preprocessor: recipe",
    "text": "Preprocessor: recipe\nIn lecture, we have focused on using formulas as a workflow preprocessor. Separately we have used the recipe function to define a series of data preprocessing steps. Here, we are going to use the recipe function to define a series of data preprocessing steps.\nWe learned quite a lot about the data in the visual EDA. We know that the q_mean, aridity and p_mean columns are right skewed and can be helped by log transformations. We also know that the relationship between aridity and p_mean is non-linear and can be helped by adding an interaction term to the model. To implement these, lets build a recipe!\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())"
  },
  {
    "objectID": "labs/lab5.html#naive-base-lm-approach",
    "href": "labs/lab5.html#naive-base-lm-approach",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Naive base lm approach:",
    "text": "Naive base lm approach:\nOk, to start, lets do what we are comfortable with ‚Ä¶ fitting a linear model to the data. First, we use prep and bake on the training data to apply the recipe. Then, we fit a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/lab5.html#where-things-get-a-little-messy",
    "href": "labs/lab5.html#where-things-get-a-little-messy",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Where things get a little messy‚Ä¶",
    "text": "Where things get a little messy‚Ä¶\nOk so now we have our trained model lm_base and want to validate it on the test data.\n\nRemember a models ability to predict on new data is the most important part of the modeling process. It really doesnt matter how well it does on data it has already seen!\n\nWe have to be careful about how we do this with the base R approach:\n\nWrong version 1: augment\nThe broom package provides a convenient way to extract model predictions and residuals. We can use the augment function to add predicted values to the test data. However, if we use augment directly on the test data, we will get incorrect results because the preprocessing steps defined in the recipe object have not been applied to the test data.\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\nbroom::augment(lm_base, data = camels_test)\n\nError in `$&lt;-`:\n! Assigned data `predict(x, na.action = na.pass, ...) %&gt;% unname()` must\n  be compatible with existing data.\n‚úñ Existing data has 135 rows.\n‚úñ Assigned data has 535 rows.\n‚Ñπ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 535 to size 135.\n\n\n\n\nWrong version 2: predict\nThe predict function can be used to make predictions on new data. However, if we use predict directly on the test data, we will get incorrect results because the preprocessing steps defined in the recipe object have not been applied to the test data.\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCorrect version: prep -&gt; bake -&gt; predict\nTo correctly evaluate the model on the test data, we need to apply the same preprocessing steps to the test data that we applied to the training data. We can do this using the prep and bake functions with the recipe object. This ensures the test data is transformed in the same way as the training data before making predictions.\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)"
  },
  {
    "objectID": "labs/lab5.html#model-evaluation-statistical-and-visual",
    "href": "labs/lab5.html#model-evaluation-statistical-and-visual",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nNow that we have the predicted values, we can evaluate the model using the metrics function from the yardstick package. This function calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values.\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\nOk so that was a bit burdensome, is really error prone (fragile), and is worthless if we wanted to test a different algorithm‚Ä¶ lets look at a better approach!"
  },
  {
    "objectID": "labs/lab5.html#using-a-workflow-instead",
    "href": "labs/lab5.html#using-a-workflow-instead",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Using a workflow instead",
    "text": "Using a workflow instead\ntidymodels provides a framework for building and evaluating models using a consistent and modular workflow. The workflows package allows you to define a series of modeling steps, including data preprocessing, model fitting, and model fitting, in a single object. This makes it easier to experiment with different models, compare performance, and ensure reproducibility.\nworkflows are built from a model, a preprocessor, and a execution. Here, we are going to use the linear_reg function to define a linear regression model, set the engine to lm, and the mode to regression. We then add our recipe to the workflow, fit the model to the training data, and extract the model coefficients.\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\nLets ensure we replicated the results from the lm_base model. How do they look to you?\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01"
  },
  {
    "objectID": "labs/lab5.html#making-predictions",
    "href": "labs/lab5.html#making-predictions",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Making Predictions",
    "text": "Making Predictions\nNow that lm_wf is a workflow, data is not embedded in the model, we can use augment with the new_data argument to make predictions on the test data.\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62"
  },
  {
    "objectID": "labs/lab5.html#model-evaluation-statistical-and-visual-1",
    "href": "labs/lab5.html#model-evaluation-statistical-and-visual-1",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nAs with EDA, applying for graphical and statistical evaluation of the model is a key Here, we use the metrics function to extract the default metrics (rmse, rsq, mae) between the observed and predicted mean streamflow values.\nWe then create a scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()"
  },
  {
    "objectID": "labs/lab5.html#switch-it-up",
    "href": "labs/lab5.html#switch-it-up",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Switch it up!",
    "text": "Switch it up!\nThe real power of this approach is that we can easily switch out the models/recipes and see how it performs. Here, we are going to instead use a random forest model to predict mean streamflow. We define a random forest model using the rand_forest function, set the engine to ranger, and the mode to regression. We then add the recipe, fit the model, and evaluate the skill.\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)"
  },
  {
    "objectID": "labs/lab5.html#predictions",
    "href": "labs/lab5.html#predictions",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Predictions",
    "text": "Predictions\n\nMake predictions on the test data using the augment function and the new_data argument.\n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61"
  },
  {
    "objectID": "labs/lab5.html#model-evaluation-statistical-and-visual-2",
    "href": "labs/lab5.html#model-evaluation-statistical-and-visual-2",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nEvaluate the model using the metrics function and create a scatter plot of the observed vs predicted values, colored by aridity.\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nAwesome! We just set up a completely new model and were able to utilize all of the things we had done for the linear model. This is the power of the tidymodels framework!\nThat said, we still can reduce some to the repetition. Further, we are not really able to compare these models to one another as they"
  },
  {
    "objectID": "labs/lab5.html#a-workflowset-approach",
    "href": "labs/lab5.html#a-workflowset-approach",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "A workflowset approach",
    "text": "A workflowset approach\nworkflow_set is a powerful tool for comparing multiple models on the same data. It allows you to define a set of workflows, fit them to the same data, and evaluate their performance using a common metric. Here, we are going to create a workflow_set object with the linear regression and random forest models, fit them to the training data, and compare their performance using the autoplot and rank_results functions.\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 √ó 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore‚Ä¶ Prepro‚Ä¶ rmse    0.563  0.0247    10 recipe       rand‚Ä¶     1\n2 recipe_rand_fore‚Ä¶ Prepro‚Ä¶ rsq     0.771  0.0259    10 recipe       rand‚Ä¶     1\n3 recipe_linear_reg Prepro‚Ä¶ rmse    0.569  0.0260    10 recipe       line‚Ä¶     2\n4 recipe_linear_reg Prepro‚Ä¶ rsq     0.770  0.0223    10 recipe       line‚Ä¶     2\n\n\nOverall it seems the random forest model is outperforming the linear model. This is not surprising given the non-linear relationship between the predictors and the outcome :)"
  },
  {
    "objectID": "labs/lab5.html#final-model",
    "href": "labs/lab5.html#final-model",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Final Model",
    "text": "Final Model\n\nrf_fin &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nfinal &lt;- workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(rf_fin) |&gt; \n  fit(data = camels_train)"
  },
  {
    "objectID": "labs/lab5.html#evaluation",
    "href": "labs/lab5.html#evaluation",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Evaluation",
    "text": "Evaluation\nAs a last step, lets evaluate the Random Forest model‚Äôs performance in predicting streamflow using the vip, augment, and ggplot2. We‚Äôll starts by computing variable importance (vip::vip()) to understand which predictors most influence the model.\nNext, we‚Äôll apply the trained model (final) to the test dataset using augment to append predictions to the test data.\nModel performance is then assessed using metrics(), comparing the actual (logQmean) and predicted (.pred) log-transformed mean streamflow values.\nFinally, a scatter plot is generated , visualizing the observed vs.¬†predicted values, color-coded by aridity. The plot includes a 1:1 reference line (geom_abline()) to indicate perfect predictions and uses the viridis color scale to improve readability.\n\n# VIP: \nvip::vip(final)\n\n\n\n\n\n\n\n## Predcition\nrf_data &lt;- augment(final, new_data = camels_test)\n\n## Evaluation\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.593\n2 rsq     standard       0.735\n3 mae     standard       0.366\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  geom_smooth(method = \"lm\", col = 'red', lty = 2, se = FALSE) +\n  theme_linedraw() + \n  labs(title = \"Random Forest Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "labs/lab5.html#data-spliting-15",
    "href": "labs/lab5.html#data-spliting-15",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Data Spliting (15)",
    "text": "Data Spliting (15)\n\nSet a seed for reproducible\nCreate an initial split with 75% used for training and 25% for testing\nExtract your training and testing sets\nBuild a 10-fold CV dataset as well"
  },
  {
    "objectID": "labs/lab5.html#recipe-15",
    "href": "labs/lab5.html#recipe-15",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Recipe (15)",
    "text": "Recipe (15)\n\nDefine a formula you want to use to predict logQmean\nDescribe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision.\nBuild a recipe that you feel handles the predictors chosen well"
  },
  {
    "objectID": "labs/lab5.html#define-3-models-25",
    "href": "labs/lab5.html#define-3-models-25",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Define 3 models (25)",
    "text": "Define 3 models (25)\n\nDefine a random forest model using the rand_forest function\nSet the engine to ranger and the mode to regression\nDefine two other models of your choice"
  },
  {
    "objectID": "labs/lab5.html#workflowset",
    "href": "labs/lab5.html#workflowset",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "workflowset ()",
    "text": "workflowset ()\nWith your preprocessing steps and models defined, you can now build a workflow_set object to fit and evaluate your models. This will allow you to compare the performance of different models on the same data.\n\nCreate a workflow object\nAdd the recipe\nAdd the model(s)\nFit the model to the resamples"
  },
  {
    "objectID": "labs/lab5.html#evaluation-1",
    "href": "labs/lab5.html#evaluation-1",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Evaluation",
    "text": "Evaluation\n\nUse autoplot and rank_results to compare the models.\nDescribe what model you think is best and why!\n\n\nTune the best model\n\nUse the tune_grid function to tune at least one of the model hyperparameters\nUse show_best to find the best hyperparameter values for the metric of your choic\nUse a workflow to fit your final, tuned, model\n\n\n\nLook at VIP\n\nUse the vip::vip package to visualize the variable importance of your final model\nDescribe what you think of the results and if they make sense\nIf the model you elect cant provide VIP, instead discuss the pros and cons of a less interpretable model"
  },
  {
    "objectID": "labs/lab5.html#extact-and-evaluate",
    "href": "labs/lab5.html#extact-and-evaluate",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Extact and Evaluate",
    "text": "Extact and Evaluate\n\nUse augment to make predictions on the test data\nUse metrics to evaluate the model performance on the test data\nCreate a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale\nDescribe what you think of the results!"
  },
  {
    "objectID": "slides/week-6.html#introduction",
    "href": "slides/week-6.html#introduction",
    "title": "Week 6",
    "section": "Introduction ",
    "text": "Introduction \nMachine learning (ML) is widely used in environmental science for tasks such as land cover classification, climate modeling, and forested distribution prediction. This lecture explores five common ML model specifications the tidymodels framework in R, applied to the forested dataset.\n\nLinear Regression\n\nLogistic Regression\n\nTrees\n\n\nDecision Tree\n\nRandom Forest\n\nBoost\n\n\nSupport Vector Machines\n\nNeural Nets"
  },
  {
    "objectID": "slides/week-6.html#classification-vs.-prediction",
    "href": "slides/week-6.html#classification-vs.-prediction",
    "title": "Week 6",
    "section": "Classification vs.¬†Prediction",
    "text": "Classification vs.¬†Prediction\nMost machine learning applications in environmental science serve different purposes (modes):\nClassification Models ‚Üí Categorize features into predefined classes.\nPrediction Models ‚Üí Forecast numerical environmental variables based on past trends."
  },
  {
    "objectID": "slides/week-6.html#classification-models",
    "href": "slides/week-6.html#classification-models",
    "title": "Week 6",
    "section": "Classification Models",
    "text": "Classification Models\n\n\n\n\n\n\nGoal: Categorize phenomena into discrete classes.\n\n\n\nExamples:\n‚úÖ Land Cover Classification (Forest, Urban, Water, Agriculture)\n‚úÖ Flood Risk Assessment (High, Medium, Low)\n‚úÖ Drought Severity Levels (No Drought, Moderate, Severe)\n‚úÖ Wildfire Prediction (Fire vs.¬†No Fire)\n‚úÖ forested Identification (Bird forested, Plant Types)\nCommon Algorithms:\n\nDecision Trees\n\nRandom Forest\n\nSupport Vector Machines (SVM)¬†\nNeural Networks (for remote sensing & image analysis)\n\nK-Nearest Neighbors (KNN)"
  },
  {
    "objectID": "slides/week-6.html#prediction-models",
    "href": "slides/week-6.html#prediction-models",
    "title": "Week 6",
    "section": "Prediction Models",
    "text": "Prediction Models\n\n\n\n\n\n\nGoal: Estimate continuous environmental variables.\n\n\n\nExamples:\nüìà Streamflow Forecasting (Predict river discharge over time)\nüå°Ô∏è Temperature Projections (Future temperature changes under climate scenarios)\n‚òî Precipitation Forecasting (Rainfall estimates for flood preparedness)\nüìä Air Quality Index Prediction (Forecast pollution levels)\n\nCommon Algorithms:\n\nLinear & Multiple Regression\n\nRandom Forest Regression\n\nLong Short-Term Memory (LSTM) Neural Networks (for time series forecasting)"
  },
  {
    "objectID": "slides/week-6.html#choosing-the-right-model",
    "href": "slides/week-6.html#choosing-the-right-model",
    "title": "Week 6",
    "section": "Choosing the Right Model",
    "text": "Choosing the Right Model\n\nUse classification if: You need to categorize environmental states (e.g., classifying land use changes).\nUse prediction if: You need to forecast environmental conditions (e.g., predicting flood levels)\nUse hybrid approaches if: You need to classify and predict (e.g., classifying drought severity and then predicting future water availability)."
  },
  {
    "objectID": "slides/week-6.html#model-selection-considerations",
    "href": "slides/week-6.html#model-selection-considerations",
    "title": "Week 6",
    "section": "Model Selection Considerations",
    "text": "Model Selection Considerations\nChoosing an ML algorithm (model) depends on:\n\nDataset Size: e.g.¬†Large datasets benefit from ensemble methods like Random Forest and XGBoost.\nFeature Complexity: e.g.¬†SVM works well for high-dimensional data.\nInterpretability Needs: e.g.¬†Decision trees and LASSO regression provide intuitive insights.\nComputation Constraints: e.g.¬†GLM and Decision Trees are efficient compared to XGBoost.\nData: Variance, linearity, diminsionality"
  },
  {
    "objectID": "slides/week-6.html#load-required-libraries",
    "href": "slides/week-6.html#load-required-libraries",
    "title": "Week 6",
    "section": "Load Required Libraries   ",
    "text": "Load Required Libraries   \n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(forested)\nlibrary(flextable)"
  },
  {
    "objectID": "slides/week-6.html#data-preparation",
    "href": "slides/week-6.html#data-preparation",
    "title": "Week 6",
    "section": "Data Preparation  ",
    "text": "Data Preparation  \nWe will use the forested dataset for classification tasks. The dataset contains information about penguin forested, body measurements, and other environmental factors.\n\nset.seed(123)\nforested_split &lt;- initial_split(forested, strata = tree_no_tree)\nforested_train &lt;- training(forested_split)\nforested_test  &lt;- testing(forested_split)\nforested_folds &lt;- vfold_cv(forested_train, v = 10)\n\n# Feature Engineering: Classification\nforested_recipe &lt;- recipe(forested ~ ., data = forested_train)  |&gt; \n  step_dummy(all_nominal_predictors())  |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_impute_mean(all_numeric_predictors())"
  },
  {
    "objectID": "slides/week-6.html#unified-interface-for-ml-models",
    "href": "slides/week-6.html#unified-interface-for-ml-models",
    "title": "Week 6",
    "section": "Unified Interface for ML Models ",
    "text": "Unified Interface for ML Models \n\nThe parsnip package is a part of the tidymodels framework\nIt provides a consistent interface for specifying models through specifications\nThe combination of a specification, mode, and engine is called a model\nLets look at the parsnip documentation!"
  },
  {
    "objectID": "slides/week-6.html#what-are-hyperparameters",
    "href": "slides/week-6.html#what-are-hyperparameters",
    "title": "Week 6",
    "section": "What are hyperparameters?",
    "text": "What are hyperparameters?\n\nHyperparameters are settings that control the learning process of a model.\nThey are set before training and affect the model‚Äôs performance.\nHyperparameters can be tuned to optimize the model‚Äôs predictive power.\nMore on model tuning next week!"
  },
  {
    "objectID": "slides/week-6.html#components-of-linear-regression",
    "href": "slides/week-6.html#components-of-linear-regression",
    "title": "Week 6",
    "section": "Components of Linear Regression",
    "text": "Components of Linear Regression\n\nDependent Variable (Y): The variable to be predicted.\nIndependent Variables (X): Features that influence the dependent variable.\nRegression Line: Represents the relationship between X and Y.\nResiduals: Differences between predicted and actual values."
  },
  {
    "objectID": "slides/week-6.html#linear-regression-in-tidymodels",
    "href": "slides/week-6.html#linear-regression-in-tidymodels",
    "title": "Week 6",
    "section": "Linear Regression in tidymodels",
    "text": "Linear Regression in tidymodels\nSpecification\n\nlinear_reg()\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nengines & modes \n\nshow_engines(\"linear_reg\") |&gt; \n  mutate(specification = \"linear_reg\") |&gt; \n  flextable()\n\nenginemodespecificationlmregressionlinear_regglmregressionlinear_regglmnetregressionlinear_regstanregressionlinear_regsparkregressionlinear_regkerasregressionlinear_regbruleeregressionlinear_regquantregquantile regressionlinear_reg"
  },
  {
    "objectID": "slides/week-6.html#example",
    "href": "slides/week-6.html#example",
    "title": "Week 6",
    "section": "Example   ",
    "text": "Example   \n\nlm_mod &lt;- linear_reg(mode = \"regression\", engine = \"lm\")\n  \nworkflow() |&gt; \n  add_formula(elevation ~ .) |&gt; \n  add_model(lm_mod) |&gt; \n  fit_resamples(resample = forested_folds) |&gt; \n  collect_metrics()\n#&gt; # A tibble: 2 √ó 6\n#&gt;   .metric .estimator   mean     n  std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   82.6      10 1.23     Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.971    10 0.000733 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-6.html#why-use-logistic-regression",
    "href": "slides/week-6.html#why-use-logistic-regression",
    "title": "Week 6",
    "section": "Why Use Logistic Regression?",
    "text": "Why Use Logistic Regression?\nLogistic Regression is widely used in machine learning due to its:\n\nSimplicity: Easy to implement and interpret.\nEfficiency: Computationally inexpensive, even on large datasets.\nProbabilistic Predictions: Outputs probabilities instead of hard classifications.\nRegularization Support: Extensions like L1 (Lasso) and L2 (Ridge) help prevent overfitting."
  },
  {
    "objectID": "slides/week-6.html#components-of-logistic-regression",
    "href": "slides/week-6.html#components-of-logistic-regression",
    "title": "Week 6",
    "section": "Components of Logistic Regression",
    "text": "Components of Logistic Regression\n\n\n\nSigmoid Function: Converts linear outputs into probabilities between 0 and 1.\nDecision Boundary: A threshold (often 0.5) determines classification.\nLog-Loss (Binary Cross-Entropy): Measures the error between predicted probabilities and actual labels.\nRegularization (L1 and L2): Helps in feature selection and prevents overfitting."
  },
  {
    "objectID": "slides/week-6.html#variants-of-logistic-regression",
    "href": "slides/week-6.html#variants-of-logistic-regression",
    "title": "Week 6",
    "section": "Variants of Logistic Regression",
    "text": "Variants of Logistic Regression\n\nBinary Logistic Regression: Used when the target variable has two classes (e.g., forested vs.¬†not).\nMultinomial Logistic Regression: Extends logistic regression to multiple classes without assuming ordering.\nOrdinal Logistic Regression: Handles multi-class classification where order matters (e.g., rating scales)."
  },
  {
    "objectID": "slides/week-6.html#building-a-logistic-regression-model",
    "href": "slides/week-6.html#building-a-logistic-regression-model",
    "title": "Week 6",
    "section": "Building a Logistic Regression Model",
    "text": "Building a Logistic Regression Model\nConstructing a Logistic Regression Model involves:\n\nDefining feature (X) and target variable (y).\nApplying the sigmoid function to map predictions to probabilities.\nUsing a loss function (log-loss) to optimize model weights.\nUpdating weights iteratively via gradient descent.\n\nHyperparameters in Logistic Regression\n\npenalty: Type of regularization (L1, L2, or none).\nmixture: A number between zero and one giving the proportion of L1 regularization (i.e.¬†lasso) in the model"
  },
  {
    "objectID": "slides/week-6.html#advantages-and-disadvantages",
    "href": "slides/week-6.html#advantages-and-disadvantages",
    "title": "Week 6",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages:\n\nSimple and interpretable.\nEfficient on large datasets.\nOutputs probabilities for uncertainty estimation.\nWorks well when data is linearly separable.\n\nDisadvantages:\n\nAssumes a linear decision boundary.\nSensitive to outliers.\nCan underperform on complex, non-linear data."
  },
  {
    "objectID": "slides/week-6.html#logistic-regression-in-tidymodels",
    "href": "slides/week-6.html#logistic-regression-in-tidymodels",
    "title": "Week 6",
    "section": "Logistic Regression in tidymodels",
    "text": "Logistic Regression in tidymodels\nSpecification\n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\nengines & modes \n\nshow_engines('logistic_reg') |&gt; \n  mutate(specification = \"logistic_reg\") |&gt; \n  flextable()\n\nenginemodespecificationglmclassificationlogistic_regglmnetclassificationlogistic_regLiblineaRclassificationlogistic_regsparkclassificationlogistic_regkerasclassificationlogistic_regstanclassificationlogistic_regbruleeclassificationlogistic_reg"
  },
  {
    "objectID": "slides/week-6.html#example-1",
    "href": "slides/week-6.html#example-1",
    "title": "Week 6",
    "section": "Example   ",
    "text": "Example   \n\nlog_model &lt;- logistic_reg(penalty = .01) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode('classification')\n\nworkflow() |&gt;\n  add_recipe(forested_recipe) |&gt;\n  add_model(log_model) |&gt; \n  fit_resamples(resample = forested_folds) |&gt; \n  collect_metrics()\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.893     10 0.00429 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0786    10 0.00330 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.958     10 0.00304 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-6.html#conclusion",
    "href": "slides/week-6.html#conclusion",
    "title": "Week 6",
    "section": "Conclusion",
    "text": "Conclusion\nLogistic Regression is a fundamental classification algorithm known for its simplicity and efficiency. It serves as a strong baseline model and is widely applied in domains like medical diagnosis, credit scoring, and fraud detection. With proper tuning and regularization, it remains a powerful tool in predictive modeling."
  },
  {
    "objectID": "slides/week-6.html#why-use-decision-trees",
    "href": "slides/week-6.html#why-use-decision-trees",
    "title": "Week 6",
    "section": "Why Use Decision Trees?",
    "text": "Why Use Decision Trees?\nDecision Trees are popular due to their: - Simplicity and Interpretability: Easy to understand and visualize. - Non-Linearity Handling: Can capture complex relationships in data. - Feature Importance: Helps in identifying the most influential features. - Minimal Data Preprocessing: Requires little to no feature scaling."
  },
  {
    "objectID": "slides/week-6.html#components-of-a-decision-tree",
    "href": "slides/week-6.html#components-of-a-decision-tree",
    "title": "Week 6",
    "section": "Components of a Decision Tree",
    "text": "Components of a Decision Tree\n\n\n\nRoot Node: The starting point representing the entire dataset.\nDecision Nodes: Intermediate nodes where a dataset is split based on a feature.\nSplitting: The process of dividing a node into sub-nodes based on a feature value.\nPruning: The process of removing unnecessary branches to avoid overfitting.\nLeaf Nodes: The terminal nodes that provide the final output (class label or numerical prediction)."
  },
  {
    "objectID": "slides/week-6.html#building-a-decision-tree",
    "href": "slides/week-6.html#building-a-decision-tree",
    "title": "Week 6",
    "section": "Building a Decision Tree",
    "text": "Building a Decision Tree\nConstructing a Decision Tree involves:\n\nSelecting the best feature(s) to split the data.\nSplitting the data into subsets.\nRepeating this process recursively until stopping criteria (e.g., depth, minimum samples per leaf) are met.\nPruning the tree if necessary to reduce overfitting.\n\nSplitting Criteria\nSeveral criteria can be used to determine the best split:\n- Gini Impurity: Measures the impurity of a node, used in Classification and Regression Trees (CART).\n- Entropy (Information Gain): Measures the randomness in a dataset.\n- Mean Squared Error (MSE): Used for regression trees to minimize variance within nodes.\n\nHyperparameters in Decision Trees\nKey hyperparameters include:\n- cost_complexity: complexity parameter for pruning\n- tree_depth: maximum depth of the tree\n- min_n: minimum number of observations in a node"
  },
  {
    "objectID": "slides/week-6.html#advantages-and-disadvantages-1",
    "href": "slides/week-6.html#advantages-and-disadvantages-1",
    "title": "Week 6",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages:\n\nEasy to interpret and explain.\nCan handle both numerical and categorical data.\nRequires minimal data preparation.\nWorks well with missing values.\n\nDisadvantages:\n\nProne to overfitting, especially on small datasets.\nCan be unstable; small changes in data can lead to drastically different splits.\nBiased towards features with more levels."
  },
  {
    "objectID": "slides/week-6.html#decision-tree-using-tidymodels",
    "href": "slides/week-6.html#decision-tree-using-tidymodels",
    "title": "Week 6",
    "section": "Decision Tree using tidymodels",
    "text": "Decision Tree using tidymodels\nSpecification\n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\nengines & modes \n\nshow_engines('decision_tree') |&gt; \n  mutate(specification = \"decision_tree\") |&gt; \n  flextable()\n\nenginemodespecificationrpartclassificationdecision_treerpartregressiondecision_treeC5.0classificationdecision_treesparkclassificationdecision_treesparkregressiondecision_tree"
  },
  {
    "objectID": "slides/week-6.html#example-2",
    "href": "slides/week-6.html#example-2",
    "title": "Week 6",
    "section": "Example   ",
    "text": "Example   \n\ndt_model &lt;- decision_tree(tree_depth = 10, min_n = 3) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode('classification')\n\nworkflow() |&gt;\n  add_recipe(forested_recipe) |&gt;\n  add_model(dt_model) |&gt; \n  fit_resamples(resample = forested_folds) |&gt; \n  collect_metrics()\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.891     10 0.00361 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0911    10 0.00329 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.911     10 0.00526 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-6.html#conclusion-1",
    "href": "slides/week-6.html#conclusion-1",
    "title": "Week 6",
    "section": "Conclusion",
    "text": "Conclusion\nDecision Trees are a powerful and interpretable tool for both classification and regression problems. While they have limitations, techniques like pruning and ensemble methods (e.g., Random Forests) can help mitigate their weaknesses. Understanding Decision Trees within the tidymodels framework makes them accessible for practical applications in environmental science."
  },
  {
    "objectID": "slides/week-6.html#why-use-random-forest",
    "href": "slides/week-6.html#why-use-random-forest",
    "title": "Week 6",
    "section": "Why Use Random Forest?",
    "text": "Why Use Random Forest?\nRandom Forest offers several advantages: - Higher Accuracy: By combining multiple decision trees, it reduces variance and improves predictive performance. - Robustness to Overfitting: By averaging multiple trees, it mitigates the risk of overfitting. - Feature Importance Analysis: Helps identify the most influential features. - Handles Missing Data: Can work well with incomplete datasets."
  },
  {
    "objectID": "slides/week-6.html#components-of-a-random-forest",
    "href": "slides/week-6.html#components-of-a-random-forest",
    "title": "Week 6",
    "section": "Components of a Random Forest",
    "text": "Components of a Random Forest\n\nMultiple Decision Trees: The fundamental building blocks.\nBootstrap Sampling: Randomly selects subsets of data to train each tree.\nFeature Subsetting: Uses a random subset of features at each split to improve diversity among trees.\nAggregation (Bagging): Combines the outputs of individual trees through voting (classification) or averaging (regression)."
  },
  {
    "objectID": "slides/week-6.html#building-a-random-forest",
    "href": "slides/week-6.html#building-a-random-forest",
    "title": "Week 6",
    "section": "Building a Random Forest",
    "text": "Building a Random Forest\nConstructing a Random Forest involves:\n\nCreating multiple bootstrap samples from the dataset.\nTraining a decision tree on each bootstrap sample using a random subset of features.\nAggregating predictions from all trees to produce the final output.\n\nHyperparameters in Random Forest\n\nNumber of Trees (ntree): Determines how many trees are included in the forest.\nMaximum Depth (max_depth): Limits the depth of individual trees to prevent overfitting.\nMinimum Samples per Leaf (min_n): Specifies the minimum number of observations required in a leaf node.\nNumber of Features (mtry): Controls how many features are randomly selected at each split."
  },
  {
    "objectID": "slides/week-6.html#advantages-and-disadvantages-2",
    "href": "slides/week-6.html#advantages-and-disadvantages-2",
    "title": "Week 6",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages:\n\nHandles large datasets with high-dimensional feature spaces.\nReduces overfitting by averaging multiple decision trees.\nWorks well with both categorical and numerical data.\nProvides built-in feature selection.\n\nDisadvantages:\n\nRequires more computational resources than a single decision tree.\nLess interpretable than a single decision tree.\nCan be slower to make predictions due to ensemble averaging."
  },
  {
    "objectID": "slides/week-6.html#random-forest-implementation-using-tidymodels",
    "href": "slides/week-6.html#random-forest-implementation-using-tidymodels",
    "title": "Week 6",
    "section": "Random Forest Implementation using Tidymodels",
    "text": "Random Forest Implementation using Tidymodels\nSpecification\n\nrand_forest()\n#&gt; Random Forest Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: ranger\n\nengines & modes \n\nshow_engines('rand_forest') |&gt; \n  mutate(specification = \"rand_forest\") |&gt; \n  flextable()\n\nenginemodespecificationrangerclassificationrand_forestrangerregressionrand_forestrandomForestclassificationrand_forestrandomForestregressionrand_forestsparkclassificationrand_forestsparkregressionrand_forest"
  },
  {
    "objectID": "slides/week-6.html#example-3",
    "href": "slides/week-6.html#example-3",
    "title": "Week 6",
    "section": "Example   ",
    "text": "Example   \n\n# Define a Random Forest model\nrf_model &lt;- rand_forest(trees = 10) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\nworkflow() |&gt;\n  add_recipe(forested_recipe) |&gt;\n  add_model(rf_model) |&gt; \n  fit_resamples(resample = forested_folds) |&gt; \n  collect_metrics()\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.909     10 0.00386 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0677    10 0.00232 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.964     10 0.00213 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-6.html#conclusion-2",
    "href": "slides/week-6.html#conclusion-2",
    "title": "Week 6",
    "section": "Conclusion",
    "text": "Conclusion\nRandom Forest is a powerful ensemble learning method that improves upon decision trees by reducing overfitting and increasing accuracy. By leveraging the tidymodels framework in R, it can be effectively applied to various environmental science problems."
  },
  {
    "objectID": "slides/week-6.html#why-use-boosting-machines",
    "href": "slides/week-6.html#why-use-boosting-machines",
    "title": "Week 6",
    "section": "Why Use Boosting Machines?",
    "text": "Why Use Boosting Machines?\nXGBoost is widely used in machine learning due to its: - High Accuracy: Often outperforms other algorithms in predictive tasks. - Efficiency: Optimized for speed and parallel processing. - Feature Importance: Provides insights into which features influence predictions. - Robustness to Overfitting: Uses regularization techniques to enhance generalization."
  },
  {
    "objectID": "slides/week-6.html#components-of-gradient-boosting",
    "href": "slides/week-6.html#components-of-gradient-boosting",
    "title": "Week 6",
    "section": "Components of Gradient Boosting",
    "text": "Components of Gradient Boosting\n\nWeak Learners: Typically small decision trees (stumps).\nGradient Descent Optimization: Each tree corrects the residual errors of the previous trees.\nLearning Rate (eta): Controls the contribution of each tree to the final prediction.\nRegularization (lambda and alpha): Penalizes complex trees to prevent overfitting."
  },
  {
    "objectID": "slides/week-6.html#popular-boosting-algorithms",
    "href": "slides/week-6.html#popular-boosting-algorithms",
    "title": "Week 6",
    "section": "Popular Boosting Algorithms",
    "text": "Popular Boosting Algorithms\n\nGradient Boosting Machines (GBM): A general boosting method that minimizes loss using gradient descent.\nXGBoost (Extreme Gradient Boosting): An optimized version of GBM that is computationally efficient and includes regularization.\nLightGBM: Designed for efficiency on large datasets by using histogram-based learning and reducing memory usage.\nCatBoost: Specialized for categorical data, using ordered boosting and permutation techniques to reduce bias."
  },
  {
    "objectID": "slides/week-6.html#building-an-xgboost-model",
    "href": "slides/week-6.html#building-an-xgboost-model",
    "title": "Week 6",
    "section": "Building an XGBoost Model",
    "text": "Building an XGBoost Model\nThe process involves:\n\nInitializing predictions with a simple model (e.g., the mean for regression).\nComputing the residual errors.\nTraining a new tree to predict these residuals.\nUpdating the predictions by adding a fraction of the new tree‚Äôs output.\nRepeating until a stopping criterion is met (e.g., a maximum number of trees or performance threshold).\n\nHyperparameters in XGBoost\n\nnrounds (Number of Trees): The number of boosting iterations.\nmax_depth: The maximum depth of trees.\neta (Learning Rate): Controls how much each tree contributes to the model.\ngamma: Minimum loss reduction required to split a node.\ncolsample_bytree: Fraction of features to consider for each tree."
  },
  {
    "objectID": "slides/week-6.html#advantages-and-disadvantages-3",
    "href": "slides/week-6.html#advantages-and-disadvantages-3",
    "title": "Week 6",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages:\n\nHandles large datasets efficiently.\nReduces bias and variance compared to single decision trees.\nProvides feature importance analysis.\nCan be used for both classification and regression.\n\nDisadvantages:\n\nMore complex and harder to interpret than a single decision tree.\nRequires tuning of hyperparameters for optimal performance.\nCan overfit if not properly regularized."
  },
  {
    "objectID": "slides/week-6.html#boost-implementation-using-tidymodels",
    "href": "slides/week-6.html#boost-implementation-using-tidymodels",
    "title": "Week 6",
    "section": "Boost Implementation using Tidymodels",
    "text": "Boost Implementation using Tidymodels\n\n# Define an XGBoost model\nb_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"classification\")\n\nworkflow() |&gt;\n  add_recipe(forested_recipe) |&gt;\n  add_model(b_model) |&gt; \n  fit_resamples(resample = forested_folds) |&gt; \n  collect_metrics()\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.909     10 0.00461 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0657    10 0.00259 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.969     10 0.00197 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-6.html#conclusion-3",
    "href": "slides/week-6.html#conclusion-3",
    "title": "Week 6",
    "section": "Conclusion",
    "text": "Conclusion\nXGBoost is a powerful ensemble learning method that significantly improves predictive accuracy while mitigating overfitting. By leveraging tidymodels in R, environmental scientists can apply XGBoost to various challenges such as climate modeling, pollution forecasting, and disaster risk assessment."
  },
  {
    "objectID": "slides/week-6.html#why-use-svm",
    "href": "slides/week-6.html#why-use-svm",
    "title": "Week 6",
    "section": "2. Why Use SVM?",
    "text": "2. Why Use SVM?\nSVM is widely used in machine learning due to its:\n- Effective for High-Dimensional Data: Works well with datasets with many features.\n- Robustness to Overfitting: Uses regularization techniques to prevent overfitting.\n- Versatility: Can be applied to linear and non-linear classification tasks using different kernel functions.\n- Support for Small Datasets: Works well when the number of samples is limited."
  },
  {
    "objectID": "slides/week-6.html#components-of-svm",
    "href": "slides/week-6.html#components-of-svm",
    "title": "Week 6",
    "section": "3. Components of SVM",
    "text": "3. Components of SVM\n\nSupport Vectors: Data points that define the hyperplane.\n\nMargin: The distance between the hyperplane and the nearest support vectors.\n\nKernel Functions: Transform data to a higher dimension to make it separable."
  },
  {
    "objectID": "slides/week-6.html#building-an-svm-model",
    "href": "slides/week-6.html#building-an-svm-model",
    "title": "Week 6",
    "section": "Building an SVM Model",
    "text": "Building an SVM Model\nThe process involves:\n\nSelecting an appropriate kernel function (linear, polynomial, radial basis function, etc.).\n\nFinding the hyperplane that best separates the data.\n\nMaximizing the margin between the hyperplane and the closest points.\n\nUsing a regularization parameter (C) to control trade-offs between a wider margin and misclassifications.\n\n\nHyperparameters in SVM\nKey hyperparameters include:\n- C (Regularization Parameter): Controls the trade-off between maximizing margin and minimizing classification error.\n- kernel: Determines the transformation of input space (e.g., linear, radial basis function (RBF), polynomial).\n- gamma: Controls the influence of individual training"
  },
  {
    "objectID": "slides/week-6.html#advantages-and-disadvantages-4",
    "href": "slides/week-6.html#advantages-and-disadvantages-4",
    "title": "Week 6",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages:\n\nWorks well with small to medium-sized datasets.\nEffective for both linear and non-linear classification.\nHandles high-dimensional spaces well.\n\nDisadvantages:\n\nComputationally expensive for large datasets.\nRequires careful tuning of hyperparameters.\nNot as interpretable as decision trees."
  },
  {
    "objectID": "slides/week-6.html#svm-implementation-using-tidymodels",
    "href": "slides/week-6.html#svm-implementation-using-tidymodels",
    "title": "Week 6",
    "section": "SVM Implementation using Tidymodels",
    "text": "SVM Implementation using Tidymodels\n\n# Define an SVM model\nsvm_model &lt;- svm_poly() |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"classification\")\n\nworkflow() |&gt;\n  add_recipe(forested_recipe) |&gt;\n  add_model(svm_model) |&gt; \n  fit_resamples(resample = forested_folds) |&gt; \n  collect_metrics()\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.905     10 0.00421 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0744    10 0.00320 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.956     10 0.00293 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-6.html#conclusion-4",
    "href": "slides/week-6.html#conclusion-4",
    "title": "Week 6",
    "section": "Conclusion",
    "text": "Conclusion\nSupport Vector Machines (SVM) is a powerful algorithm for classification and regression tasks, especially when working with high-dimensional datasets. By leveraging tidymodels in R, environmental scientists can apply SVM to various challenges such as land cover classification, pollution prediction, and climate modeling."
  },
  {
    "objectID": "slides/week-6.html#why-use-neural-networks",
    "href": "slides/week-6.html#why-use-neural-networks",
    "title": "Week 6",
    "section": "Why Use Neural Networks?",
    "text": "Why Use Neural Networks?\nNeural Networks are popular due to their:\n- Ability to Model Complex Patterns: Can capture intricate relationships in data.\n- Scalability: Performs well with large datasets.\n- Feature Learning: Automatically extracts relevant features from raw data.\n- Generalization: Can adapt to different types of data with proper training."
  },
  {
    "objectID": "slides/week-6.html#components-of-a-neural-network",
    "href": "slides/week-6.html#components-of-a-neural-network",
    "title": "Week 6",
    "section": "Components of a Neural Network",
    "text": "Components of a Neural Network\n\n\n\nNeurons: The fundamental units that receive, process, and transmit information.\nInput Layer: The initial layer that receives raw data.\nHidden Layers: Intermediate layers where computations occur to learn features.\nOutput Layer: Produces the final prediction or classification.\nWeights & Biases: Parameters that are optimized during training.\nActivation Functions: Functions like ReLU, Sigmoid, and Softmax that introduce non-linearity."
  },
  {
    "objectID": "slides/week-6.html#training-a-neural-network",
    "href": "slides/week-6.html#training-a-neural-network",
    "title": "Week 6",
    "section": "Training a Neural Network",
    "text": "Training a Neural Network\nThe training process involves:\n\nForward Propagation: Inputs pass through the network, producing an output.\nLoss Calculation: The difference between predicted and actual values is measured.\nBackpropagation: Errors are propagated backward to update weights.\nOptimization: Gradient descent (or its variants) adjusts weights to minimize loss.\nIteration: The process repeats over multiple epochs until convergence.\n\nHyperparameters in Neural Networks\n\nhidden_units: Number of neurons in hidden layers.\npenalty: Regularization term to prevent overfitting.\ndropout: Fraction of neurons randomly dropped during training.\nepochs: Number of training iterations.\nactivation: Activation function for neurons (e.g., ReLU, Sigmoid).\nlearn_rate: Step size for weight updates."
  },
  {
    "objectID": "slides/week-6.html#advantages-and-disadvantages-5",
    "href": "slides/week-6.html#advantages-and-disadvantages-5",
    "title": "Week 6",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages:\n\nCan model complex, non-linear relationships.\nWorks well with high-dimensional data.\nAutomatically extracts features from data.\n\nDisadvantages:\n\nRequires large datasets for effective training.\nComputationally expensive.\nDifficult to interpret compared to simpler models like Decision Trees."
  },
  {
    "objectID": "slides/week-6.html#varients-of-neural-networks",
    "href": "slides/week-6.html#varients-of-neural-networks",
    "title": "Week 6",
    "section": "Varients of Neural Networks",
    "text": "Varients of Neural Networks\nmlp() ‚Üí Generic MLP model (uses ‚Äúnnet‚Äù, ‚Äúkeras‚Äù, or ‚Äúbrulee‚Äù as an engine).\nbag_mlp() ‚Üí Bagged MLP model using ‚Äúnnet‚Äù (reduces variance).\nbrulee::mlp() ‚Üí MLP model using torch via brulee (more scalable and flexible).\n\n\n\n\n\n\nWhich one you choose depends on your dataset size, computational resources, and need for ensembling. If you need better scalability, brulee::mlp() is likely a better choice. If you want a quick MLP with some regularization, mlp() with ‚Äúnnet‚Äù or ‚Äúkeras‚Äù works. If variance reduction is a concern, bag_mlp() is a solid option."
  },
  {
    "objectID": "slides/week-6.html#neural-network-implementation",
    "href": "slides/week-6.html#neural-network-implementation",
    "title": "Week 6",
    "section": "Neural Network Implementation",
    "text": "Neural Network Implementation\nmlp() defines a multilayer perceptron model (a.k.a. a single layer, feed-forward neural network). This function can fit classification and regression models.\nSpecification\n\nmlp()\n#&gt; Single Layer Neural Network Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: nnet\n\nengines & modes \n\nshow_engines(\"mlp\") |&gt; \n  mutate(specification = \"mlp\") |&gt; \n  flextable()\n\nenginemodespecificationkerasclassificationmlpkerasregressionmlpnnetclassificationmlpnnetregressionmlpbruleeclassificationmlpbruleeregressionmlpbrulee_two_layerclassificationmlpbrulee_two_layerregressionmlp"
  },
  {
    "objectID": "slides/week-6.html#example-4",
    "href": "slides/week-6.html#example-4",
    "title": "Week 6",
    "section": "Example   ",
    "text": "Example   \n\nnn_model &lt;- mlp(hidden_units = 5, penalty = 0.01) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\")\n\nworkflow() |&gt;\n  add_recipe(forested_recipe) |&gt;\n  add_model(nn_model) |&gt; \n  fit_resamples(resample = forested_folds) |&gt; \n  collect_metrics()\n#&gt; # A tibble: 3 √ó 6\n#&gt;   .metric     .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.908    10 0.00462 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.118    10 0.00139 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.965    10 0.00211 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/week-6.html#conclusion-5",
    "href": "slides/week-6.html#conclusion-5",
    "title": "Week 6",
    "section": "Conclusion",
    "text": "Conclusion\nNeural Networks are a powerful tool for solving complex problems in various domains. While they require significant computational resources, proper training and regularization techniques can make them highly effective for predictive modeling. Understanding Neural Networks within the torch framework allows for practical applications in data science and environmental modeling."
  },
  {
    "objectID": "slides/week-6.html#wrap-up",
    "href": "slides/week-6.html#wrap-up",
    "title": "Week 6",
    "section": "Wrap up   ",
    "text": "Wrap up   \nLets combine all the models and evaluate their performance using cross-validation.\n\nWe learned about cross-validation last Monday and its importance in evaluating model performance.\nWe will use a workflow set (seen last Wednesday) to fit multiple models at once and compare their performance.\nRemember, we have not implmented any hyperparameter tuning yet, so these are just base models.\n\n\nforested_folds &lt;-  vfold_cv(forested_train, v = 10)\n\nwf &lt;-  workflow_set(list(forested_recipe), \n                  list(log_model, \n                       dt_model, \n                       rf_model, \n                       b_model, \n                       svm_model, \n                       nn_model)) |&gt; \n workflow_map('fit_resamples', resamples = forested_folds)"
  },
  {
    "objectID": "slides/week-6.html#model-performance-comparision",
    "href": "slides/week-6.html#model-performance-comparision",
    "title": "Week 6",
    "section": "Model Performance Comparision ",
    "text": "Model Performance Comparision \n\nautoplot(wf) +\n  theme_linedraw(18) + \n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "slides/week-6.html#model-evaluation",
    "href": "slides/week-6.html#model-evaluation",
    "title": "Week 6",
    "section": "Model Evaluation ",
    "text": "Model Evaluation \nUnderstanding the boost model provided the best performance out of the box we can (1) select that model (2)\n\nfinal_fit &lt;- workflow() |&gt;\n  add_recipe(forested_recipe) |&gt;\n  # Selected Model\n  add_model(b_model) |&gt;\n  # Trained with full training dataset\n  fit(data = forested_train) |&gt; \n  # Validated against hold out data\n  augment(new_data = forested_test)\n\n# Final Results!\nmetrics(final_fit,  truth = forested, estimate = .pred_class) \n#&gt; # A tibble: 2 √ó 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.917\n#&gt; 2 kap      binary         0.833\nconf_mat(final_fit, truth = forested, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction Yes  No\n#&gt;        Yes 910  86\n#&gt;        No   61 720"
  },
  {
    "objectID": "slides/week-6.html#the-whole-game---status-update",
    "href": "slides/week-6.html#the-whole-game---status-update",
    "title": "Week 6",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "slides/week-6.html#recaping",
    "href": "slides/week-6.html#recaping",
    "title": "Week 6",
    "section": "Recaping",
    "text": "Recaping\n\nlibrary(tidymodels)\nlibrary(forested)\n\n# Set a seed\nset.seed(123)\n\n# IntitalizeInitialize Split\nforested_split &lt;- initial_split(forested, prop = 0.8)\nforested_train &lt;- training(forested_split)\nforested_test  &lt;- testing(forested_split)\n\n# Build Resamples\nforested_folds &lt;- vfold_cv(forested_train, v = 10)\n\n# Set a model specification with mode (default engine)\ndt_mod         &lt;- decision_tree(cost_complexity = 0.0001, mode = \"classification\")\n\n# Bare bones workflow & fit\nforested_wflow &lt;- workflow(forested ~ ., dt_mod)\nforested_fit   &lt;- fit(forested_wflow, forested_train)\n\n# Extracting Predictions:\naugment(forested_fit, new_data = forested_train)\n#&gt; # A tibble: 5,685 √ó 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 No             0.0114   0.989  No        2016       464       -5       -99\n#&gt;  2 Yes            0.636    0.364  Yes       2016       166       92        37\n#&gt;  3 No             0.0114   0.989  No        2016       644      -85       -52\n#&gt;  4 Yes            0.977    0.0226 Yes       2014      1285        4        99\n#&gt;  5 Yes            0.977    0.0226 Yes       2013       822       87        48\n#&gt;  6 Yes            0.808    0.192  Yes       2017         3        6       -99\n#&gt;  7 Yes            0.977    0.0226 Yes       2014      2041      -95        28\n#&gt;  8 Yes            0.977    0.0226 Yes       2015      1009       -8        99\n#&gt;  9 No             0.0114   0.989  No        2017       436      -98        19\n#&gt; 10 No             0.0114   0.989  No        2018       775       63        76\n#&gt; # ‚Ñπ 5,675 more rows\n#&gt; # ‚Ñπ 14 more variables: roughness &lt;dbl&gt;, tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;,\n#&gt; #   precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;,\n#&gt; #   temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;,\n#&gt; #   vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;"
  },
  {
    "objectID": "slides/week-6.html#evaluation",
    "href": "slides/week-6.html#evaluation",
    "title": "Week 6",
    "section": "Evaluation",
    "text": "Evaluation\n\nSo far we have used metrics and collect_metrics to evaluate and compare models\nThe default metrics for classification problems are: accuracy, brier_score, roc_auc\n\nAll classification metrics stem from the classic confusion matrix\n\n\n\n\nThe default metrics for regression problems are: rsq, rmse, mae\n\nThe majority of these are measures of correlation and residual error"
  },
  {
    "objectID": "slides/week-6.html#confusion-matrix",
    "href": "slides/week-6.html#confusion-matrix",
    "title": "Week 6",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\nA confusion matrix is a table that describes the performance of a classification model on a set of data for which the true values are known.\nIt counts the number of accurate and false predictions, separated by the truth state\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  conf_mat(truth = forested, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction  Yes   No\n#&gt;        Yes 2991  176\n#&gt;        No   144 2374"
  },
  {
    "objectID": "slides/week-6.html#accuracy",
    "href": "slides/week-6.html#accuracy",
    "title": "Week 6",
    "section": "1. Accuracy ",
    "text": "1. Accuracy \n\nAccuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined.\nIt is a measure of the correctness of the model‚Äôs predictions.\nIt is the most common metric used to evaluate classification models.\n\n\n\n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 √ó 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.944"
  },
  {
    "objectID": "slides/week-6.html#sensitivity",
    "href": "slides/week-6.html#sensitivity",
    "title": "Week 6",
    "section": "2. Sensitivity ",
    "text": "2. Sensitivity \n\nSensitivity is the proportion of true positives to the sum of true positives and false negatives.\nIt is useful for identifying the presence of a condition.\nIt is also known as the true positive rate, recall, or probability of detection.\n\n\n\n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 √ó 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.954"
  },
  {
    "objectID": "slides/week-6.html#specificity",
    "href": "slides/week-6.html#specificity",
    "title": "Week 6",
    "section": "3. Specificity ",
    "text": "3. Specificity \n\nSpecificity is the proportion of true negatives to the sum of true negatives and false positives.\nIt is useful for identifying the absence of a condition.\nIt is also known as the true negative rate, and is the complement of sensitivity.\n\n\n\n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 √ó 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.954\n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  specificity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 √ó 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 specificity binary         0.931"
  },
  {
    "objectID": "slides/week-6.html#two-class-data",
    "href": "slides/week-6.html#two-class-data",
    "title": "Week 6",
    "section": "Two class data",
    "text": "Two class data\nThese metrics assume that we know the threshold for converting ‚Äúsoft‚Äù probability predictions into ‚Äúhard‚Äù class predictions.\n\nFor example, if the predicted probability of a tree is 0.7, we might say that the model predicts ‚ÄúTree‚Äù for that observation.\n\nIf the predicted probability of a tree is 0.4, we might say that the model predicts ‚ÄúNo tree‚Äù for that observation.\n\nThe threshold is the value that separates the two classes.\n\nThe default threshold is 0.5, but this can be changed.\n\n\n\nIs a 50% threshold good?\n\n\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity ‚¨áÔ∏è, specificity ‚¨ÜÔ∏è\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity ‚¨ÜÔ∏è, specificity ‚¨áÔ∏è"
  },
  {
    "objectID": "slides/week-6.html#varying-the-threshold",
    "href": "slides/week-6.html#varying-the-threshold",
    "title": "Week 6",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\n\n\nThe threshold can be varied to see how it affects the sensitivity and specificity of the model.\nThis is done by plotting the sensitivity and specificity against the threshold.\nThe threshold is varied from 0 to 1, and the sensitivity and specificity are calculated at each threshold.\nThe plot shows the trade-off between sensitivity and specificity at different thresholds.\nThe threshold can be chosen based on the desired balance between sensitivity and specificity."
  },
  {
    "objectID": "slides/week-6.html#roc-curves",
    "href": "slides/week-6.html#roc-curves",
    "title": "Week 6",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nFor an ROC (receiver operator characteristic) curve, we plot\n\nthe false positive rate (1 - specificity) on the x-axis\nthe true positive rate (sensitivity) on the y-axis\n\nwith sensitivity and specificity calculated at all possible thresholds."
  },
  {
    "objectID": "slides/week-6.html#roc-curves-1",
    "href": "slides/week-6.html#roc-curves-1",
    "title": "Week 6",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nThe ROC AUC is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance.\n\nThe ROC AUC is a measure of how well the model separates the two classes.\nThe ROC AUC is a number between 0 and 1.\nROC AUC = 1 üíØ\n\nperfect classification\n\nROC AUC = 0.5 üòê\n\nrandom guessing\n\nROC AUC &lt; 0.5 üò±\n\nworse than random guessing"
  },
  {
    "objectID": "slides/week-6.html#roc-curves-2",
    "href": "slides/week-6.html#roc-curves-2",
    "title": "Week 6",
    "section": "ROC curves ",
    "text": "ROC curves \n(A ROC AUC of 0.7-0.8 is considered acceptable, 0.8-0.9 is good, and &gt;0.9 is excellent.)\n\n# Assumes _first_ factor level is event; there are options to change that\naugment(forested_fit, new_data = forested_train) |&gt;  \n  roc_curve(truth = forested, .pred_Yes)  |&gt; \n  dplyr::slice(1, 20, 50)\n#&gt; # A tibble: 3 √ó 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -Inf           0           1    \n#&gt; 2      0.235       0.885       0.972\n#&gt; 3      0.909       0.969       0.826\n\naugment(forested_fit, new_data = forested_train) |&gt;  \n  roc_auc(truth = forested, .pred_Yes)\n#&gt; # A tibble: 1 √ó 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.975"
  },
  {
    "objectID": "slides/week-6.html#brier-score",
    "href": "slides/week-6.html#brier-score",
    "title": "Week 6",
    "section": "Brier score",
    "text": "Brier score\n\nThe Brier score is a measure of how well the predicted probabilities of an event match the actual outcomes.\nIt is the mean squared difference between predicted probabilities and actual outcomes.\nThe Brier score is a number between 0 and 1.\nThe Brier score is analogous to the mean squared error in regression models:\nBrier = 1 üò±\n\npredicted probabilities are completely wrong\n\nBrier = 0.5 üòê\n\nbad\n\nBrier &lt; 0.25\n\nacceptable\n\nBrier = 0 üíØ\n\npredicted probabilities are perfect\n\n\n\\[\nBrier_{class} = \\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C (y_{ik} - \\hat{p}_{ik})^2\n\\]"
  },
  {
    "objectID": "slides/week-6.html#brier-score-1",
    "href": "slides/week-6.html#brier-score-1",
    "title": "Week 6",
    "section": "Brier score",
    "text": "Brier score\n\naugment(forested_fit, new_data = forested_train) |&gt;  \n  brier_class(truth = forested, .pred_Yes) \n#&gt; # A tibble: 1 √ó 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0469"
  },
  {
    "objectID": "slides/week-6.html#separation-vs-calibration",
    "href": "slides/week-6.html#separation-vs-calibration",
    "title": "Week 6",
    "section": "Separation vs calibration",
    "text": "Separation vs calibration\n\n\nThe ROC captures separation. - The ROC curve shows the trade-off between sensitivity and specificity at different thresholds.\n\n\n\n\n\n\n\n\n\n\nThe Brier score captures calibration. - The Brier score is a measure of how well the predicted probabilities of an event match the actual outcomes.\n\n\n\n\n\n\n\n\n\n\n\nGood separation: the densities don‚Äôt overlap.\nGood calibration: the calibration line follows the diagonal.\n\nCalibration plot: We bin observations according to predicted probability. In the bin for 20%-30% predicted prob, we should see an event rate of ~25% if the model is well-calibrated."
  },
  {
    "objectID": "slides/week-6.html#r-squared-r2",
    "href": "slides/week-6.html#r-squared-r2",
    "title": "Week 6",
    "section": "R-squared (\\(R^2\\))",
    "text": "R-squared (\\(R^2\\))\n\nMeasures how well the model explains the variance in the data.\nFormula:\n\n\\[\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n\\] where:\n- \\(SS_{res}\\) is the sum of squared residuals.\n\n\n\\(SS_{tot}\\) is the total sum of squares.\n\nValues range from \\(-\\infty\\) to 1.\n\n\n1: Perfect fit\n\n0: No improvement over mean prediction\n\nNegative: Worse than just using the mean"
  },
  {
    "objectID": "slides/week-6.html#root-mean-squared-error-rmse",
    "href": "slides/week-6.html#root-mean-squared-error-rmse",
    "title": "Week 6",
    "section": "Root Mean Squared Error (RMSE)",
    "text": "Root Mean Squared Error (RMSE)\n\nMeasures the model‚Äôs prediction error in the same unit as the dependent variable.\nFormula:\n\n\\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\nwhere: - \\(y_i\\) is the actual value. - \\(\\hat{y}_i\\) is the predicted value.\n\nPenalizes large errors more than MAE.\nLower RMSE indicates better model performance."
  },
  {
    "objectID": "slides/week-6.html#mean-absolute-error-mae",
    "href": "slides/week-6.html#mean-absolute-error-mae",
    "title": "Week 6",
    "section": "Mean Absolute Error (MAE)",
    "text": "Mean Absolute Error (MAE)\n\nMeasures average absolute errors between predictions and actual values.\nFormula:\n\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\n\nLess sensitive to outliers than RMSE.\nLower MAE indicates better model performance."
  },
  {
    "objectID": "slides/week-6.html#comparing-metrics",
    "href": "slides/week-6.html#comparing-metrics",
    "title": "Week 6",
    "section": "Comparing Metrics",
    "text": "Comparing Metrics\n\n\n\nMetric\nInterpretation\nSensitivity to Outliers\n\n\n\n\n\\(R^2\\)\nVariance explained\nModerate\n\n\nRMSE\nPenalizes large errors more\nHigh\n\n\nMAE\nAverage error size\nLow\n\n\n\n\nChoosing the Right Metric\n\nUse \\(R^2\\) to assess model fit and explainability.\nUse RMSE if large errors are especially undesirable.\nUse MAE for a more interpretable, robust metric."
  },
  {
    "objectID": "slides/week-6.html#metrics-selection-model-performance",
    "href": "slides/week-6.html#metrics-selection-model-performance",
    "title": "Week 6",
    "section": "Metrics Selection model performance ",
    "text": "Metrics Selection model performance \nWe can use metric_set() to combine multiple calculations into one\n\nforested_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  forested_metrics(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 3 √ó 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    binary         0.944\n#&gt; 2 specificity binary         0.931\n#&gt; 3 sensitivity binary         0.954\n\n\nMetrics and metric sets work with grouped data frames!\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  group_by(tree_no_tree) |&gt;\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 √ó 4\n#&gt;   tree_no_tree .metric  .estimator .estimate\n#&gt;   &lt;fct&gt;        &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Tree         accuracy binary         0.946\n#&gt; 2 No tree      accuracy binary         0.941\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  group_by(tree_no_tree) |&gt;\n  specificity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 √ó 4\n#&gt;   tree_no_tree .metric     .estimator .estimate\n#&gt;   &lt;fct&gt;        &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Tree         specificity binary         0.582\n#&gt; 2 No tree      specificity binary         0.974\n\n\n\n\n\n\n\nNote\n\n\nThe specificity for \"Tree\" is a good bit lower than it is for \"No tree\".\nSo, when this index classifies the plot as having a tree, the model does not do well at correctly identifying the plot as non-forested when it is indeed non-forested."
  },
  {
    "objectID": "slides/week-6.html#previously---setup",
    "href": "slides/week-6.html#previously---setup",
    "title": "Week 6",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\nlibrary(tidyverse)\n# Ingest Data\n# URLs for COVID-19 case data and census population data\ncovid_url &lt;- 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv'\npop_url &lt;-  '/Users/mikejohnson/github/csu-ess-330/resources/co-est2023-alldata.csv'\n#pop_url   &lt;- 'https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv'\n\n# Clean Census Data\ncensus = readr::read_csv(pop_url)  |&gt; \n  filter(COUNTY == \"000\") |&gt;  # Filter for state-level data only\n  mutate(fips = STATE) |&gt;      # Create a new FIPS column for merging\n  select(fips, contains(\"2021\"))  # Select relevant columns for 2021 data\n\n# Process COVID-19 Data\nstate_data &lt;-  readr::read_csv(covid_url) |&gt; \n  group_by(fips) |&gt; \n  mutate(\n    new_cases  = pmax(0, cases  - dplyr::lag(cases)),   # Compute new cases, ensuring no negative values\n    new_deaths = pmax(0, deaths - dplyr::lag(deaths))  # Compute new deaths, ensuring no negative values\n  ) |&gt; \n  ungroup() |&gt; \n  left_join(census, by = \"fips\") |&gt;  # Merge with census data\n  mutate(\n    m = month(date), y = year(date),\n    season = case_when(   # Define seasons based on month\n      m %in% 3:5 ~ \"Spring\",\n      m %in% 6:8 ~ \"Summer\",\n      m %in% 9:11 ~ \"Fall\",\n      m %in% c(12, 1, 2) ~ \"Winter\"\n    )\n  ) |&gt; \n  group_by(state, y, season) |&gt; \n  mutate(\n    season_cases  = sum(new_cases, na.rm = TRUE),  # Aggregate seasonal cases\n    season_deaths = sum(new_deaths, na.rm = TRUE)  # Aggregate seasonal deaths\n  )  |&gt; \n  distinct(state, y, season, .keep_all = TRUE) |&gt;  # Keep only distinct rows by state, year, season\n  ungroup() |&gt; \n  select(state, contains('season'), y, POPESTIMATE2021, BIRTHS2021, DEATHS2021) |&gt;  # Select relevant columns\n  drop_na() |&gt;  # Remove rows with missing values\n  mutate(logC = log(season_cases +1))  # Log-transform case numbers for modeling"
  },
  {
    "objectID": "slides/week-6.html#previously---data-usage",
    "href": "slides/week-6.html#previously---data-usage",
    "title": "Week 6",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nsplit &lt;- initial_split(state_data, prop = 0.8, strata = season)  # 80/20 train-test split\ntrain &lt;- training(split)  # Training set\ntest &lt;- testing(split)  # Test set\n\nset.seed(3045)\nfolds &lt;- vfold_cv(train, v = 10)  # 10-fold cross-validation"
  },
  {
    "objectID": "slides/week-6.html#previously---feature-engineering",
    "href": "slides/week-6.html#previously---feature-engineering",
    "title": "Week 6",
    "section": "Previously - Feature engineering ",
    "text": "Previously - Feature engineering \n\nrec = recipe(logC ~ . , data = train) |&gt; \n  step_rm(state, season_cases) |&gt;  # Remove non-predictive columns\n  step_dummy(all_nominal()) |&gt;  # Convert categorical variables to dummy variables\n  step_scale(all_numeric_predictors()) |&gt;  # Scale numeric predictors\n  step_center(all_numeric_predictors())"
  },
  {
    "objectID": "slides/week-6.html#tuning-parameters",
    "href": "slides/week-6.html#tuning-parameters",
    "title": "Week 6",
    "section": "Tuning parameters",
    "text": "Tuning parameters\n\nSome model or preprocessing parameters cannot be estimated directly from the data.\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "slides/week-6.html#tagging-parameters-for-tuning",
    "href": "slides/week-6.html#tagging-parameters-for-tuning",
    "title": "Week 6",
    "section": "Tagging parameters for tuning ",
    "text": "Tagging parameters for tuning \nWith tidymodels, you can mark the parameters that you want to optimize with a value of tune().\n\nThe function itself just returns‚Ä¶ itself:\n\ntune()\n#&gt; tune()\n\nstr(tune())\n#&gt;  language tune()\n\n# optionally add a label\ntune(\"Marker\")\n#&gt; tune(\"Marker\")"
  },
  {
    "objectID": "slides/week-6.html#boosted-trees",
    "href": "slides/week-6.html#boosted-trees",
    "title": "Week 6",
    "section": "Boosted Trees",
    "text": "Boosted Trees\nIn last weeks live demo, a boosted tree proved most effective.\nBoosted Trees are popular ensemble methods that build a sequence of tree models.\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted.\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble."
  },
  {
    "objectID": "slides/week-6.html#boosted-tree-tuning-parameters",
    "href": "slides/week-6.html#boosted-tree-tuning-parameters",
    "title": "Week 6",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nSome possible parameters:\n\nmtry: The number of predictors randomly sampled at each split (in \\([1, ncol(x)]\\) or \\((0, 1]\\)).\ntrees: The number of trees (\\([1, \\infty]\\), but usually up to thousands)\nmin_n: The number of samples needed to further split (\\([1, n]\\)).\nlearn_rate: The rate that each tree adapts from previous iterations (\\((0, \\infty]\\), usual maximum is 0.1).\nstop_iter: The number of iterations of boosting where no improvement was shown before stopping (\\([1, trees]\\))"
  },
  {
    "objectID": "slides/week-6.html#boosted-tree-tuning-parameters-1",
    "href": "slides/week-6.html#boosted-tree-tuning-parameters-1",
    "title": "Week 6",
    "section": "Boosted Tree Tuning Parameters  ",
    "text": "Boosted Tree Tuning Parameters  \n\nb_mod &lt;- \n  boost_tree(trees = tune(), learn_rate = tune()) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"xgboost\")\n\n(b_wflow &lt;- workflow(rec, b_mod))\n#&gt; ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#&gt; Preprocessor: Recipe\n#&gt; Model: boost_tree()\n#&gt; \n#&gt; ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; ‚Ä¢ step_rm()\n#&gt; ‚Ä¢ step_dummy()\n#&gt; ‚Ä¢ step_scale()\n#&gt; ‚Ä¢ step_center()\n#&gt; \n#&gt; ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt; Boosted Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = tune()\n#&gt;   learn_rate = tune()\n#&gt; \n#&gt; Computational engine: xgboost"
  },
  {
    "objectID": "slides/week-6.html#optimize-tuning-parameters",
    "href": "slides/week-6.html#optimize-tuning-parameters",
    "title": "Week 6",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search üí† which tests a pre-defined set of candidate values\nIterative search üåÄ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "slides/week-6.html#grid-search",
    "href": "slides/week-6.html#grid-search",
    "title": "Week 6",
    "section": "1. Grid search",
    "text": "1. Grid search\n\nMost basic (but very effective) way to tune models\nA small grid of points trying to minimize the error via learning rate:"
  },
  {
    "objectID": "slides/week-6.html#grid-search-1",
    "href": "slides/week-6.html#grid-search-1",
    "title": "Week 6",
    "section": "1. Grid search",
    "text": "1. Grid search\nIn reality we would probably sample the space more densely:"
  },
  {
    "objectID": "slides/week-6.html#iterative-search",
    "href": "slides/week-6.html#iterative-search",
    "title": "Week 6",
    "section": "2. Iterative Search",
    "text": "2. Iterative Search\nWe could start with a few points and search the space:"
  },
  {
    "objectID": "slides/week-6.html#parameters",
    "href": "slides/week-6.html#parameters",
    "title": "Week 6",
    "section": "Parameters",
    "text": "Parameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid."
  },
  {
    "objectID": "slides/week-6.html#different-types-of-grids",
    "href": "slides/week-6.html#different-types-of-grids",
    "title": "Week 6",
    "section": "Different types of grids ",
    "text": "Different types of grids \n\n\n\n\n\n\n\n\n\nSpace-filling designs (SFD) attempt to cover the parameter space without redundant candidates. We recommend these the most."
  },
  {
    "objectID": "slides/week-6.html#create-a-grid",
    "href": "slides/week-6.html#create-a-grid",
    "title": "Week 6",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n# Extac\n(b_wflow |&gt; \n  extract_parameter_set_dials())\n\n# Individual functions: \n(trees())\n(learn_rate())\n\n\nA parameter set can be updated (e.g.¬†to change the ranges)."
  },
  {
    "objectID": "slides/week-6.html#create-a-grid-1",
    "href": "slides/week-6.html#create-a-grid-1",
    "title": "Week 6",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nThe grid_*() functions create a grid of parameter values to evaluate.\nThe grid_space_filling() function creates a space-filling design (SFD) of parameter values to evaluate.\nThe grid_regular() function creates a regular grid of parameter values to evaluate.\nThe grid_random() function creates a random grid of parameter values to evaluate.\nThe grid_latin_hypercube() function creates a Latin hypercube design of parameter values to evaluate.\nThe grid_max_entropy() function creates a maximum entropy design of parameter values to evaluate."
  },
  {
    "objectID": "slides/week-6.html#create-a-sfd-curve",
    "href": "slides/week-6.html#create-a-sfd-curve",
    "title": "Week 6",
    "section": "Create a SFD curve  ",
    "text": "Create a SFD curve  \n\nset.seed(12)\n\n(grid &lt;- \n  b_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_space_filling(size = 25))\n#&gt; # A tibble: 25 √ó 2\n#&gt;    trees learn_rate\n#&gt;    &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1     1    0.0287 \n#&gt;  2    84    0.00536\n#&gt;  3   167    0.121  \n#&gt;  4   250    0.00162\n#&gt;  5   334    0.0140 \n#&gt;  6   417    0.0464 \n#&gt;  7   500    0.196  \n#&gt;  8   584    0.00422\n#&gt;  9   667    0.00127\n#&gt; 10   750    0.0178 \n#&gt; # ‚Ñπ 15 more rows"
  },
  {
    "objectID": "slides/week-6.html#create-a-regular-grid",
    "href": "slides/week-6.html#create-a-regular-grid",
    "title": "Week 6",
    "section": "Create a regular grid  ",
    "text": "Create a regular grid  \n\nset.seed(12)\n\n(grid &lt;- \n  b_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_regular(levels = 4))\n#&gt; # A tibble: 16 √ó 2\n#&gt;    trees learn_rate\n#&gt;    &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1     1    0.001  \n#&gt;  2   667    0.001  \n#&gt;  3  1333    0.001  \n#&gt;  4  2000    0.001  \n#&gt;  5     1    0.00681\n#&gt;  6   667    0.00681\n#&gt;  7  1333    0.00681\n#&gt;  8  2000    0.00681\n#&gt;  9     1    0.0464 \n#&gt; 10   667    0.0464 \n#&gt; 11  1333    0.0464 \n#&gt; 12  2000    0.0464 \n#&gt; 13     1    0.316  \n#&gt; 14   667    0.316  \n#&gt; 15  1333    0.316  \n#&gt; 16  2000    0.316"
  },
  {
    "objectID": "slides/week-6.html#update-parameter-ranges",
    "href": "slides/week-6.html#update-parameter-ranges",
    "title": "Week 6",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nb_param &lt;- \n  b_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\nset.seed(712)\n(grid &lt;- \n  b_param |&gt; \n  grid_space_filling(size = 25))\n#&gt; # A tibble: 25 √ó 2\n#&gt;    trees learn_rate\n#&gt;    &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1     1  0.00215  \n#&gt;  2     5  0.000147 \n#&gt;  3     9  0.0215   \n#&gt;  4    13  0.0000215\n#&gt;  5    17  0.000681 \n#&gt;  6    21  0.00464  \n#&gt;  7    25  0.0464   \n#&gt;  8    29  0.0001   \n#&gt;  9    34  0.0000147\n#&gt; 10    38  0.001    \n#&gt; # ‚Ñπ 15 more rows"
  },
  {
    "objectID": "slides/week-6.html#the-results",
    "href": "slides/week-6.html#the-results",
    "title": "Week 6",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid |&gt; \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nNote that the learning rates are uniform on the log-10 scale and this shows 2 of 4 dimensions."
  },
  {
    "objectID": "slides/week-6.html#choosing-tuning-parameters",
    "href": "slides/week-6.html#choosing-tuning-parameters",
    "title": "Week 6",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLet‚Äôs take our previous model and tune more parameters:\n\nb_mod &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"xgboost\")\n\nb_wflow &lt;- workflow(rec, b_mod)\n\n# Update the feature hash ranges (log-2 units)\nb_param &lt;-\n  b_wflow |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(trees = trees(c(1L, 1000L)))"
  },
  {
    "objectID": "slides/week-6.html#grid-search-2",
    "href": "slides/week-6.html#grid-search-2",
    "title": "Week 6",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE)\n\nb_res &lt;-\n  b_wflow |&gt;\n  tune_grid(\n    resamples = folds,\n    grid = 25,\n    # The options below are not required by default\n    param_info = b_param, \n    control = ctrl,\n    metrics = metric_set(mae)\n  )\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "slides/week-6.html#grid-search-3",
    "href": "slides/week-6.html#grid-search-3",
    "title": "Week 6",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nb_res \n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 √ó 5\n#&gt;    splits           id     .metrics          .notes           .predictions        \n#&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;              \n#&gt;  1 &lt;split [513/57]&gt; Fold01 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  2 &lt;split [513/57]&gt; Fold02 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  3 &lt;split [513/57]&gt; Fold03 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  4 &lt;split [513/57]&gt; Fold04 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  5 &lt;split [513/57]&gt; Fold05 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  6 &lt;split [513/57]&gt; Fold06 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  7 &lt;split [513/57]&gt; Fold07 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  8 &lt;split [513/57]&gt; Fold08 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt;  9 &lt;split [513/57]&gt; Fold09 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;\n#&gt; 10 &lt;split [513/57]&gt; Fold10 &lt;tibble [25 √ó 7]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [1,425 √ó 7]&gt;"
  },
  {
    "objectID": "slides/week-6.html#grid-results",
    "href": "slides/week-6.html#grid-results",
    "title": "Week 6",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(b_res)"
  },
  {
    "objectID": "slides/week-6.html#tuning-results",
    "href": "slides/week-6.html#tuning-results",
    "title": "Week 6",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(b_res)\n#&gt; # A tibble: 25 √ó 9\n#&gt;    trees min_n learn_rate .metric .estimator  mean     n std_err .config              \n#&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1   458     2    0.00422 mae     standard   1.54     10  0.0259 Preprocessor1_Model01\n#&gt;  2   417     3    0.0590  mae     standard   0.347    10  0.0143 Preprocessor1_Model02\n#&gt;  3   833     5    0.0226  mae     standard   0.333    10  0.0133 Preprocessor1_Model03\n#&gt;  4   125     6    0.00536 mae     standard   5.34     10  0.0476 Preprocessor1_Model04\n#&gt;  5   708     8    0.196   mae     standard   0.359    10  0.0120 Preprocessor1_Model05\n#&gt;  6   791     9    0.00205 mae     standard   2.08     10  0.0300 Preprocessor1_Model06\n#&gt;  7    84    11    0.0464  mae     standard   0.405    10  0.0186 Preprocessor1_Model07\n#&gt;  8   250    13    0.316   mae     standard   0.356    10  0.0133 Preprocessor1_Model08\n#&gt;  9   292    14    0.00127 mae     standard   7.20     10  0.0562 Preprocessor1_Model09\n#&gt; 10   583    16    0.0110  mae     standard   0.353    10  0.0153 Preprocessor1_Model10\n#&gt; # ‚Ñπ 15 more rows"
  },
  {
    "objectID": "slides/week-6.html#choose-a-parameter-combination",
    "href": "slides/week-6.html#choose-a-parameter-combination",
    "title": "Week 6",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(b_res, metric = \"mae\")\n#&gt; # A tibble: 5 √ó 9\n#&gt;   trees min_n learn_rate .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1   833     5     0.0226 mae     standard   0.333    10  0.0133 Preprocessor1_Model03\n#&gt; 2   542    21     0.121  mae     standard   0.342    10  0.0146 Preprocessor1_Model13\n#&gt; 3   958    17     0.0750 mae     standard   0.344    10  0.0130 Preprocessor1_Model11\n#&gt; 4   750    30     0.249  mae     standard   0.346    10  0.0123 Preprocessor1_Model19\n#&gt; 5   417     3     0.0590 mae     standard   0.347    10  0.0143 Preprocessor1_Model02"
  },
  {
    "objectID": "slides/week-6.html#choose-a-parameter-combination-1",
    "href": "slides/week-6.html#choose-a-parameter-combination-1",
    "title": "Week 6",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\n(b_best &lt;- select_best(b_res, metric = \"mae\"))\n#&gt; # A tibble: 1 √ó 4\n#&gt;   trees min_n learn_rate .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1   833     5     0.0226 Preprocessor1_Model03"
  },
  {
    "objectID": "slides/week-6.html#checking-calibration",
    "href": "slides/week-6.html#checking-calibration",
    "title": "Week 6",
    "section": "Checking Calibration  ",
    "text": "Checking Calibration  \n\n\nlibrary(probably)\n\nb_res |&gt;\n  collect_predictions(\n    parameters = b_best\n  ) |&gt;\n  cal_plot_regression(\n    truth = logC,\n    estimate = .pred\n  )"
  },
  {
    "objectID": "slides/week-6.html#the-final-fit",
    "href": "slides/week-6.html#the-final-fit",
    "title": "Week 6",
    "section": "The final fit!",
    "text": "The final fit!\nSuppose that we are happy with our model.\nLet‚Äôs fit the model on the training set and verify our performance using the test set.\n\nWe‚Äôve seen fit() and predict() (+ augment()) but there is a shortcut:\n\n\n\n# the boosted tree workflow can be `finalized` with the best parameters\nworkflow &lt;- finalize_workflow(b_wflow, b_best)\n\n# forested_split has train + test info\n(final_fit &lt;- last_fit(workflow, split))\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 √ó 6\n#&gt;   splits            id               .metrics         .notes           .predictions       .workflow \n#&gt;   &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;           &lt;list&gt;             &lt;list&gt;    \n#&gt; 1 &lt;split [570/144]&gt; train/test split &lt;tibble [2 √ó 4]&gt; &lt;tibble [0 √ó 3]&gt; &lt;tibble [144 √ó 4]&gt; &lt;workflow&gt;"
  },
  {
    "objectID": "slides/week-6.html#the-final-fit-1",
    "href": "slides/week-6.html#the-final-fit-1",
    "title": "Week 6",
    "section": "The final fit!",
    "text": "The final fit!\n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 2 √ó 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard       0.391 Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.918 Preprocessor1_Model1\n\ncollect_predictions(final_fit) |&gt; \n  ggplot(aes(.pred, logC)) +\n  geom_point() + \n  geom_abline() + \n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "slides/week-6.html#the-whole-game",
    "href": "slides/week-6.html#the-whole-game",
    "title": "Week 6",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Lab 6: Timeseries Data",
    "section": "",
    "text": "library(dataRetrieval)\nlibrary(tidymodels)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.2.0 ‚îÄ‚îÄ\n\n\n‚úî broom        1.0.7     ‚úî recipes      1.1.1\n‚úî dials        1.4.0     ‚úî rsample      1.2.1\n‚úî dplyr        1.1.4     ‚úî tibble       3.2.1\n‚úî ggplot2      3.5.1     ‚úî tidyr        1.3.1\n‚úî infer        1.0.7     ‚úî tune         1.3.0\n‚úî modeldata    1.4.0     ‚úî workflows    1.2.0\n‚úî parsnip      1.3.0     ‚úî workflowsets 1.1.0\n‚úî purrr        1.0.4     ‚úî yardstick    1.3.2\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n‚úñ purrr::discard() masks scales::discard()\n‚úñ dplyr::filter()  masks stats::filter()\n‚úñ dplyr::lag()     masks stats::lag()\n‚úñ recipes::step()  masks stats::step()\n‚Ä¢ Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî forcats   1.0.0     ‚úî readr     2.1.5\n‚úî lubridate 1.9.4     ‚úî stringr   1.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ readr::col_factor() masks scales::col_factor()\n‚úñ purrr::discard()    masks scales::discard()\n‚úñ dplyr::filter()     masks stats::filter()\n‚úñ stringr::fixed()    masks recipes::fixed()\n‚úñ dplyr::lag()        masks stats::lag()\n‚úñ readr::spec()       masks yardstick::spec()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tsibble)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\nAttaching package: 'tsibble'\n\nThe following object is masked from 'package:lubridate':\n\n    interval\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(modeltime)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(feasts)\n\nLoading required package: fabletools\n\nAttaching package: 'fabletools'\n\nThe following object is masked from 'package:yardstick':\n\n    accuracy\n\nThe following object is masked from 'package:parsnip':\n\n    null_model\n\nThe following objects are masked from 'package:infer':\n\n    generate, hypothesize\n\nlibrary(timetk)\nIn this lab, you will download streamflow data from the Cache la Poudre River (USGS site 06752260) and analyze it using a few time series methods. You will then use modeltime to\nFirst, use this code to download the data from the USGS site.\n# Example: Cache la Poudre River at Mouth (USGS site 06752260)\npoudre_flow &lt;- readNWISdv(siteNumber = \"06752260\",    # Download data from USGS for site 06752260\n                          parameterCd = \"00060\",      # Parameter code 00060 = discharge in cfs)\n                          startDate = \"2013-01-01\",   # Set the start date\n                          endDate = \"2023-12-31\") |&gt;  # Set the end date\n  renameNWISColumns() |&gt;                              # Rename columns to standard names (e.g., \"Flow\", \"Date\")\n  mutate(Date = yearmonth(Date)) |&gt;                   # Convert daily Date values into a year-month format (e.g., \"2023 Jan\")\n  group_by(Date) |&gt;                                   # Group the data by the new monthly Date\n  summarise(Flow = mean(Flow))                       # Calculate the average daily flow for each month\n\nGET:https://waterservices.usgs.gov/nwis/dv/?site=06752260&format=waterml%2C1.1&ParameterCd=00060&StatCd=00003&startDT=2013-01-01&endDT=2023-12-31"
  },
  {
    "objectID": "labs/lab6.html#predict",
    "href": "labs/lab6.html#predict",
    "title": "Lab 6: Timeseries Data",
    "section": "5. Predict",
    "text": "5. Predict\n\nUse modeltime to forecast the next 12 months of streamflow data in the Poudre River based on last time assignment.\nUse the prophet_reg(), and arima_reg() function to create a Prophet model for forecasting.\nUse dataRetrieval to download daily streamflow for the next 12 months. Aggregate this data to monthly averages and compare it to the predictions made by your modeltime model.\nCompute the R2 value between the model predictions and the observed data using a linear model and report the meaning.\nLast, generate a plot of the Predicted vs Observed values and include a 1:1 line, and a linear model line.\n\n\n\npf_tbl &lt;-  pf |&gt;\n  as_tibble() |&gt;\n  mutate(date = as.Date(Date), Date = NULL) \n\nsplits &lt;- time_series_split(pf_tbl, \n                            assess = \"60 months\", \n                            cumulative = TRUE)\n\nUsing date_var: date\n\ntraining &lt;-  training(splits)\ntesting  &lt;-  testing(splits)\n\nmods &lt;- list(\n  arima_reg() |&gt;  set_engine(\"auto_arima\"),\n  \n  arima_boost() |&gt; set_engine(engine = \"auto_arima_xgboost\"),\n  \n  prophet_reg() |&gt; set_engine(\"prophet\"),\n  \n  prophet_boost() |&gt; set_engine(\"prophet_xgboost\"),\n  \n  # Exponential Smoothing State Space model\n  exp_smoothing() |&gt; set_engine(engine = \"ets\"),\n  \n  # Multivariate Adaptive Regression Spline model\n  mars(mode = \"regression\") |&gt; set_engine(\"earth\") \n)\n\nmodels &lt;- map(mods, ~ fit(.x, Flow ~ date + factor(month(date, label = TRUE), ordered = F), data = training))\n\nfrequency = 12 observations per 1 year\n\n\nfrequency = 12 observations per 1 year\n\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nfrequency = 12 observations per 1 year\n\n\n\nAttaching package: 'plotrix'\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n(models_tbl &lt;- as_modeltime_table(models))\n\n# Modeltime Table\n# A tibble: 6 √ó 3\n  .model_id .model   .model_desc                              \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                                    \n1         1 &lt;fit[+]&gt; REGRESSION WITH ARIMA(1,0,0) ERRORS      \n2         2 &lt;fit[+]&gt; ARIMA(1,0,0)(0,1,0)[12] W/ XGBOOST ERRORS\n3         3 &lt;fit[+]&gt; PROPHET W/ REGRESSORS                    \n4         4 &lt;fit[+]&gt; PROPHET W/ XGBOOST ERRORS                \n5         5 &lt;fit[+]&gt; ETS(M,A,M)                               \n6         6 &lt;fit[+]&gt; EARTH                                    \n\n(calibration_table &lt;- modeltime_calibrate(models_tbl, testing, quiet = FALSE))\n\n# Modeltime Table\n# A tibble: 6 √ó 5\n  .model_id .model   .model_desc                         .type .calibration_data\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                               &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;fit[+]&gt; REGRESSION WITH ARIMA(1,0,0) ERRORS Test  &lt;tibble [60 √ó 4]&gt;\n2         2 &lt;fit[+]&gt; ARIMA(1,0,0)(0,1,0)[12] W/ XGBOOST‚Ä¶ Test  &lt;tibble [60 √ó 4]&gt;\n3         3 &lt;fit[+]&gt; PROPHET W/ REGRESSORS               Test  &lt;tibble [60 √ó 4]&gt;\n4         4 &lt;fit[+]&gt; PROPHET W/ XGBOOST ERRORS           Test  &lt;tibble [60 √ó 4]&gt;\n5         5 &lt;fit[+]&gt; ETS(M,A,M)                          Test  &lt;tibble [60 √ó 4]&gt;\n6         6 &lt;fit[+]&gt; EARTH                               Test  &lt;tibble [60 √ó 4]&gt;\n\n\n\nmodeltime_accuracy(calibration_table) |&gt; \n  arrange(mae)\n\n# A tibble: 6 √ó 9\n  .model_id .model_desc                .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         2 ARIMA(1,0,0)(0,1,0)[12] W‚Ä¶ Test   115.  143. 0.734  94.0  227. 0.406\n2         1 REGRESSION WITH ARIMA(1,0‚Ä¶ Test   155.  230. 0.986  87.5  286. 0.628\n3         3 PROPHET W/ REGRESSORS      Test   170.  299. 1.08  155.   277. 0.662\n4         4 PROPHET W/ XGBOOST ERRORS  Test   180.  326. 1.14  150.   270. 0.645\n5         6 EARTH                      Test   392. 1218. 2.50  177.   430. 0.510\n6         5 ETS(M,A,M)                 Test   509.  992. 3.25  125.   932. 0.485\n\n\n\n(forecast &lt;- calibration_table  |&gt; \n  modeltime_forecast(h = \"60 months\", \n                     new_data = testing,\n                     actual_data = pf_tbl) )\n\n# Forecast Results\n  \n\n\nConf Method: conformal_default | Conf Interval: 0.95 | Conf By ID: FALSE\n(GLOBAL CONFIDENCE)\n\n\n# A tibble: 492 √ó 7\n   .model_id .model_desc .key   .index      .value .conf_lo .conf_hi\n       &lt;int&gt; &lt;chr&gt;       &lt;fct&gt;  &lt;date&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1        NA ACTUAL      actual 2013-01-01   18.1        NA       NA\n 2        NA ACTUAL      actual 2013-02-01   18.0        NA       NA\n 3        NA ACTUAL      actual 2013-03-01    8.21       NA       NA\n 4        NA ACTUAL      actual 2013-04-01    5.94       NA       NA\n 5        NA ACTUAL      actual 2013-05-01  333.         NA       NA\n 6        NA ACTUAL      actual 2013-06-01  300.         NA       NA\n 7        NA ACTUAL      actual 2013-07-01   75.6        NA       NA\n 8        NA ACTUAL      actual 2013-08-01   48.8        NA       NA\n 9        NA ACTUAL      actual 2013-09-01 1085.         NA       NA\n10        NA ACTUAL      actual 2013-10-01  146.         NA       NA\n# ‚Ñπ 482 more rows"
  },
  {
    "objectID": "labs/lab6.html#vizualize",
    "href": "labs/lab6.html#vizualize",
    "title": "Lab 6: Timeseries Data",
    "section": "Vizualize",
    "text": "Vizualize\n\nplot_modeltime_forecast(forecast)"
  },
  {
    "objectID": "labs/lab6.html#refit-to-full-dataset-forecast-forward",
    "href": "labs/lab6.html#refit-to-full-dataset-forecast-forward",
    "title": "Lab 6: Timeseries Data",
    "section": "Refit to Full Dataset & Forecast Forward",
    "text": "Refit to Full Dataset & Forecast Forward\nThe final step is to refit the models to the full dataset using modeltime_refit() and forecast them forward.\n\nrefit_tbl &lt;-  modeltime_refit(calibration_table, data = pf_tbl)\n\nfrequency = 12 observations per 1 year\nfrequency = 12 observations per 1 year\n\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nfrequency = 12 observations per 1 year\n\nmodeltime_accuracy(refit_tbl)\n\n# A tibble: 6 √ó 9\n  .model_id .model_desc                .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 UPDATE: REGRESSION WITH A‚Ä¶ Test   155.  230. 0.986  87.5  286. 0.628\n2         2 UPDATE: ARIMA(0,0,2)(0,1,‚Ä¶ Test   115.  143. 0.734  94.0  227. 0.406\n3         3 PROPHET W/ REGRESSORS      Test   170.  299. 1.08  155.   277. 0.662\n4         4 PROPHET W/ XGBOOST ERRORS  Test   180.  326. 1.14  150.   270. 0.645\n5         5 UPDATE: ETS(M,N,M)         Test   509.  992. 3.25  125.   932. 0.485\n6         6 EARTH                      Test   392. 1218. 2.50  177.   430. 0.510\n\nrefit_tbl |&gt;\n    modeltime_forecast(h = \"10 years\", actual_data = pf_tbl) |&gt;\n    plot_modeltime_forecast()"
  },
  {
    "objectID": "slides/week-7.html#learning-objectives",
    "href": "slides/week-7.html#learning-objectives",
    "title": "Week 7",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture, you should be able to:\n\nUnderstand what time series data is and why it‚Äôs useful\nWork with date and time objects in R\nLoad and visualize time series data in R\nApply basic time series operations: decomposition, smoothing, and forecasting\nUse ts, zoo, and xts objects for time-indexed data\nUnderstand tidy approaches using tsibble and feasts\nRecognize real-world environmental science applications of time series"
  },
  {
    "objectID": "slides/week-7.html#what-is-time-series-data",
    "href": "slides/week-7.html#what-is-time-series-data",
    "title": "Week 7",
    "section": "What is Time Series Data?",
    "text": "What is Time Series Data?\nTime series data is a collection of observations recorded sequentially over time. Unlike other data types, time series data is ordered, and that order carries critical information.\nExamples:\n\nStreamflow measurements (e.g., daily CFS at a USGS gage)\nAtmospheric CO‚ÇÇ levels (e.g., Mauna Loa Observatory)\nSnowpack depth over time\nUrban water consumption by month\n\nWhy Time Series Matter\nTime series analysis helps us:\n\nUnderstand trends and patterns\nDetect anomalies (e.g., droughts, sensor failures)\nForecast future values (e.g., streamflow, temperature)"
  },
  {
    "objectID": "slides/week-7.html#working-with-date-objects-in-r",
    "href": "slides/week-7.html#working-with-date-objects-in-r",
    "title": "Week 7",
    "section": "Working with Date Objects in R",
    "text": "Working with Date Objects in R\n\nTime series analysis depends on accurate and well-formatted date/time information.\nBefore we can analyis time series data, we need to understand how R handles dates and times.\nR has built-in classes for date and time objects, which are essential for time series analysis."
  },
  {
    "objectID": "slides/week-7.html#base-r-date-functions",
    "href": "slides/week-7.html#base-r-date-functions",
    "title": "Week 7",
    "section": "Base R Date Functions",
    "text": "Base R Date Functions\nR has several native classes for handling dates and times:\n\nDate: Date object\n\n\nas.Date(\"2024-04-08\")\n#&gt; [1] \"2024-04-08\"\n\n\nPOSIXct: Date and time\n\n\nas.POSIXct(\"2024-04-08 14:30:00\")\n#&gt; [1] \"2024-04-08 14:30:00 MDT\"\n\n\nPOSIXlt: List of date and time components\n\n\n(lt &lt;- as.POSIXlt(\"2024-04-08 14:30:00\"))\n#&gt; [1] \"2024-04-08 14:30:00 MDT\"\n\nlt$hour\n#&gt; [1] 14"
  },
  {
    "objectID": "slides/week-7.html#date-sequences",
    "href": "slides/week-7.html#date-sequences",
    "title": "Week 7",
    "section": "Date Sequences",
    "text": "Date Sequences\nYou can create sequences of dates using the seq.Date() function:\n\nfrom: Start date\nto: End date (optional)\nby: Interval (e.g., ‚Äúday‚Äù, ‚Äúmonth‚Äù, ‚Äúyear‚Äù)\nlength.out: Number of dates to generate\n\n\nseq.Date(from = as.Date(\"2024-01-01\"), by = \"month\", length.out = 12)\n#&gt;  [1] \"2024-01-01\" \"2024-02-01\" \"2024-03-01\" \"2024-04-01\" \"2024-05-01\"\n#&gt;  [6] \"2024-06-01\" \"2024-07-01\" \"2024-08-01\" \"2024-09-01\" \"2024-10-01\"\n#&gt; [11] \"2024-11-01\" \"2024-12-01\""
  },
  {
    "objectID": "slides/week-7.html#section",
    "href": "slides/week-7.html#section",
    "title": "Week 7",
    "section": "",
    "text": "You can also use seq() to create sequences of POSIXct/lt objects:\n\nfrom: Start date/time\nto: End date/time (optional)\nby: Interval (e.g., ‚Äúhour‚Äù, ‚Äúminute‚Äù, ‚Äúsecond‚Äù)\nlength.out: Number of dates to generate\n\n\nseq(from = as.POSIXct(\"2024-01-01 00:00\"), \n    to = as.POSIXct(\"2024-01-02 00:00\"), \n    by = \"hour\")\n#&gt;  [1] \"2024-01-01 00:00:00 MST\" \"2024-01-01 01:00:00 MST\"\n#&gt;  [3] \"2024-01-01 02:00:00 MST\" \"2024-01-01 03:00:00 MST\"\n#&gt;  [5] \"2024-01-01 04:00:00 MST\" \"2024-01-01 05:00:00 MST\"\n#&gt;  [7] \"2024-01-01 06:00:00 MST\" \"2024-01-01 07:00:00 MST\"\n#&gt;  [9] \"2024-01-01 08:00:00 MST\" \"2024-01-01 09:00:00 MST\"\n#&gt; [11] \"2024-01-01 10:00:00 MST\" \"2024-01-01 11:00:00 MST\"\n#&gt; [13] \"2024-01-01 12:00:00 MST\" \"2024-01-01 13:00:00 MST\"\n#&gt; [15] \"2024-01-01 14:00:00 MST\" \"2024-01-01 15:00:00 MST\"\n#&gt; [17] \"2024-01-01 16:00:00 MST\" \"2024-01-01 17:00:00 MST\"\n#&gt; [19] \"2024-01-01 18:00:00 MST\" \"2024-01-01 19:00:00 MST\"\n#&gt; [21] \"2024-01-01 20:00:00 MST\" \"2024-01-01 21:00:00 MST\"\n#&gt; [23] \"2024-01-01 22:00:00 MST\" \"2024-01-01 23:00:00 MST\"\n#&gt; [25] \"2024-01-02 00:00:00 MST\""
  },
  {
    "objectID": "slides/week-7.html#timezones",
    "href": "slides/week-7.html#timezones",
    "title": "Week 7",
    "section": "Timezones",
    "text": "Timezones\n\nBy default, R uses the system‚Äôs timezone.\nThe Sys.timezone() function returns the current timezone.\nIf you want to use anthor timezone, say GMT/UTC, you can change it:\n\n\nSys.timezone()\n#&gt; [1] \"America/Denver\"\n\nas.POSIXct(\"2024-04-08 14:30:00\", tz = \"GMT\")\n#&gt; [1] \"2024-04-08 14:30:00 GMT\"\n\n(obj = as.POSIXct(\"2024-04-08 14:30:00\"))\n#&gt; [1] \"2024-04-08 14:30:00 MDT\"\n\nformat(obj, tz = \"EST\")\n#&gt; [1] \"2024-04-08 15:30:00\""
  },
  {
    "objectID": "slides/week-7.html#lubridate-package",
    "href": "slides/week-7.html#lubridate-package",
    "title": "Week 7",
    "section": "lubridate Package ",
    "text": "lubridate Package \n\nThe lubridate package comes with the tidyverse and is designed to make working with dates and times easier.\nIt provides a consistent set of functions for parsing, manipulating, and formatting date-time objects.\n\n\nlibrary(lubridate)\nymd(\"20240408\")        # Parse date\n#&gt; [1] \"2024-04-08\"\nymd_hms(\"20240408 123000\")\n#&gt; [1] \"2024-04-08 12:30:00 UTC\"\n\nnow()                  # Current date and time\n#&gt; [1] \"2025-05-04 23:43:24 MDT\"\ndate &lt;- ymd(\"2023-12-01\")\nmonth(date)           # Extract month\n#&gt; [1] 12\nday(date)\n#&gt; [1] 1\nweek(date)\n#&gt; [1] 48\n\nUse lubridate to handle inconsistent formats and to align data with time zones, daylight savings, etc."
  },
  {
    "objectID": "slides/week-7.html#lubridate-timezones",
    "href": "slides/week-7.html#lubridate-timezones",
    "title": "Week 7",
    "section": "lubridate timezones ",
    "text": "lubridate timezones \nlubridate also provides functions for working with time zones. You can convert between time zones using with_tz() and force_tz().\n\n(date &lt;- ymd_hms(\"2024-04-08 14:30:00\", tz = \"America/New_York\"))\n#&gt; [1] \"2024-04-08 14:30:00 EDT\"\n\n# Change time zone without changing time\nforce_tz(date, \"America/Los_Angeles\")  \n#&gt; [1] \"2024-04-08 14:30:00 PDT\"\n\n# Convert to another timezone\nwith_tz(date, \"America/Los_Angeles\")  \n#&gt; [1] \"2024-04-08 11:30:00 PDT\""
  },
  {
    "objectID": "slides/week-7.html#r-packages-for-time-series",
    "href": "slides/week-7.html#r-packages-for-time-series",
    "title": "Week 7",
    "section": "R Packages for Time Series",
    "text": "R Packages for Time Series\nR has several packages/systme for time series analysis, including:\n\nts: Base R time series class\nzoo: Provides a flexible class for ordered observations\nxts: Extensible time series class for irregularly spaced data\nforecast: Functions for forecasting time series data\ntsibble: Tidy time series data frames\nfeasts: Functions for time series analysis and visualization\nmodeltime: Time series modeling with tidymodels\n\n\nlibrary(tidyverse)\nlibrary(zoo)\nlibrary(tsibble)\nlibrary(feasts)"
  },
  {
    "objectID": "slides/week-7.html#when-to-use-which-time-series-format",
    "href": "slides/week-7.html#when-to-use-which-time-series-format",
    "title": "Week 7",
    "section": "When to Use Which Time Series Format?",
    "text": "When to Use Which Time Series Format?\n\n\n\n\n\n\n\n\n\nFormat\nBest For\nAdvantages\nLimitations\n\n\n\n\nts\nRegular, simple time series\nBuilt into base R, supported by forecast\nRigid time format, inflexible for joins\n\n\nzoo\nIrregular time steps\nArbitrary index, supports irregular data\nMore complex syntax\n\n\nxts\nFinancial-style or irregular data\nGood for date-time indexing and joins\nMore complex to visualize\n\n\ntsibble\nTidyverse workflows\nPipes with dplyr, ggplot2, forecasting with fable\nRequires tsibble structure\n\n\ntibble + Date\nGeneral purpose\nFlexible, tidy\nNeeds conversion for time series modeling"
  },
  {
    "objectID": "slides/week-7.html#key-concepts-in-time-series",
    "href": "slides/week-7.html#key-concepts-in-time-series",
    "title": "Week 7",
    "section": "Key Concepts in Time Series",
    "text": "Key Concepts in Time Series\n\n1. Trend: Long-term increase or decrease in the data\n2. Seasonality: Repeating short-term cycle (e.g., annual snowmelt)\n3. Noise: Random variation\n4. Stationarity: Statistical properties (mean, variance) don‚Äôt change over time\n5. Autocorrelation: Correlation of a time series with its own past values"
  },
  {
    "objectID": "slides/week-7.html#introducing-the-mauna-loa-co‚ÇÇ-dataset",
    "href": "slides/week-7.html#introducing-the-mauna-loa-co‚ÇÇ-dataset",
    "title": "Week 7",
    "section": "Introducing the Mauna Loa CO‚ÇÇ Dataset",
    "text": "Introducing the Mauna Loa CO‚ÇÇ Dataset"
  },
  {
    "objectID": "slides/week-7.html#mauna-loa-co‚ÇÇ-dataset",
    "href": "slides/week-7.html#mauna-loa-co‚ÇÇ-dataset",
    "title": "Week 7",
    "section": "Mauna Loa CO‚ÇÇ Dataset",
    "text": "Mauna Loa CO‚ÇÇ Dataset\n\nThe co2 dataset is a classic example of time series data. It contains monthly atmospheric CO‚ÇÇ concentrations measured at the Mauna Loa Observatory in Hawaii.\nco2 is a built-in dataset representing monthly CO‚ÇÇ concentrations from 1959 onward.\n\n\nclass(co2)\n#&gt; [1] \"ts\"\nco2\n#&gt;         Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n#&gt; 1959 315.42 316.31 316.50 317.56 318.13 318.00 316.39 314.65 313.68 313.18\n#&gt; 1960 316.27 316.81 317.42 318.87 319.87 319.43 318.01 315.74 314.00 313.68\n#&gt; 1961 316.73 317.54 318.38 319.31 320.42 319.61 318.42 316.63 314.83 315.16\n#&gt; 1962 317.78 318.40 319.53 320.42 320.85 320.45 319.45 317.25 316.11 315.27\n#&gt; 1963 318.58 318.92 319.70 321.22 322.08 321.31 319.58 317.61 316.05 315.83\n#&gt; 1964 319.41 320.07 320.74 321.40 322.06 321.73 320.27 318.54 316.54 316.71\n#&gt; 1965 319.27 320.28 320.73 321.97 322.00 321.71 321.05 318.71 317.66 317.14\n#&gt; 1966 320.46 321.43 322.23 323.54 323.91 323.59 322.24 320.20 318.48 317.94\n#&gt; 1967 322.17 322.34 322.88 324.25 324.83 323.93 322.38 320.76 319.10 319.24\n#&gt; 1968 322.40 322.99 323.73 324.86 325.40 325.20 323.98 321.95 320.18 320.09\n#&gt; 1969 323.83 324.26 325.47 326.50 327.21 326.54 325.72 323.50 322.22 321.62\n#&gt; 1970 324.89 325.82 326.77 327.97 327.91 327.50 326.18 324.53 322.93 322.90\n#&gt; 1971 326.01 326.51 327.01 327.62 328.76 328.40 327.20 325.27 323.20 323.40\n#&gt; 1972 326.60 327.47 327.58 329.56 329.90 328.92 327.88 326.16 324.68 325.04\n#&gt; 1973 328.37 329.40 330.14 331.33 332.31 331.90 330.70 329.15 327.35 327.02\n#&gt; 1974 329.18 330.55 331.32 332.48 332.92 332.08 331.01 329.23 327.27 327.21\n#&gt; 1975 330.23 331.25 331.87 333.14 333.80 333.43 331.73 329.90 328.40 328.17\n#&gt; 1976 331.58 332.39 333.33 334.41 334.71 334.17 332.89 330.77 329.14 328.78\n#&gt; 1977 332.75 333.24 334.53 335.90 336.57 336.10 334.76 332.59 331.42 330.98\n#&gt; 1978 334.80 335.22 336.47 337.59 337.84 337.72 336.37 334.51 332.60 332.38\n#&gt; 1979 336.05 336.59 337.79 338.71 339.30 339.12 337.56 335.92 333.75 333.70\n#&gt; 1980 337.84 338.19 339.91 340.60 341.29 341.00 339.39 337.43 335.72 335.84\n#&gt; 1981 339.06 340.30 341.21 342.33 342.74 342.08 340.32 338.26 336.52 336.68\n#&gt; 1982 340.57 341.44 342.53 343.39 343.96 343.18 341.88 339.65 337.81 337.69\n#&gt; 1983 341.20 342.35 342.93 344.77 345.58 345.14 343.81 342.21 339.69 339.82\n#&gt; 1984 343.52 344.33 345.11 346.88 347.25 346.62 345.22 343.11 340.90 341.18\n#&gt; 1985 344.79 345.82 347.25 348.17 348.74 348.07 346.38 344.51 342.92 342.62\n#&gt; 1986 346.11 346.78 347.68 349.37 350.03 349.37 347.76 345.73 344.68 343.99\n#&gt; 1987 347.84 348.29 349.23 350.80 351.66 351.07 349.33 347.92 346.27 346.18\n#&gt; 1988 350.25 351.54 352.05 353.41 354.04 353.62 352.22 350.27 348.55 348.72\n#&gt; 1989 352.60 352.92 353.53 355.26 355.52 354.97 353.75 351.52 349.64 349.83\n#&gt; 1990 353.50 354.55 355.23 356.04 357.00 356.07 354.67 352.76 350.82 351.04\n#&gt; 1991 354.59 355.63 357.03 358.48 359.22 358.12 356.06 353.92 352.05 352.11\n#&gt; 1992 355.88 356.63 357.72 359.07 359.58 359.17 356.94 354.92 352.94 353.23\n#&gt; 1993 356.63 357.10 358.32 359.41 360.23 359.55 357.53 355.48 353.67 353.95\n#&gt; 1994 358.34 358.89 359.95 361.25 361.67 360.94 359.55 357.49 355.84 356.00\n#&gt; 1995 359.98 361.03 361.66 363.48 363.82 363.30 361.94 359.50 358.11 357.80\n#&gt; 1996 362.09 363.29 364.06 364.76 365.45 365.01 363.70 361.54 359.51 359.65\n#&gt; 1997 363.23 364.06 364.61 366.40 366.84 365.68 364.52 362.57 360.24 360.83\n#&gt;         Nov    Dec\n#&gt; 1959 314.66 315.43\n#&gt; 1960 314.84 316.03\n#&gt; 1961 315.94 316.85\n#&gt; 1962 316.53 317.53\n#&gt; 1963 316.91 318.20\n#&gt; 1964 317.53 318.55\n#&gt; 1965 318.70 319.25\n#&gt; 1966 319.63 320.87\n#&gt; 1967 320.56 321.80\n#&gt; 1968 321.16 322.74\n#&gt; 1969 322.69 323.95\n#&gt; 1970 323.85 324.96\n#&gt; 1971 324.63 325.85\n#&gt; 1972 326.34 327.39\n#&gt; 1973 327.99 328.48\n#&gt; 1974 328.29 329.41\n#&gt; 1975 329.32 330.59\n#&gt; 1976 330.14 331.52\n#&gt; 1977 332.24 333.68\n#&gt; 1978 333.75 334.78\n#&gt; 1979 335.12 336.56\n#&gt; 1980 336.93 338.04\n#&gt; 1981 338.19 339.44\n#&gt; 1982 339.09 340.32\n#&gt; 1983 340.98 342.82\n#&gt; 1984 342.80 344.04\n#&gt; 1985 344.06 345.38\n#&gt; 1986 345.48 346.72\n#&gt; 1987 347.64 348.78\n#&gt; 1988 349.91 351.18\n#&gt; 1989 351.14 352.37\n#&gt; 1990 352.69 354.07\n#&gt; 1991 353.64 354.89\n#&gt; 1992 354.09 355.33\n#&gt; 1993 355.30 356.78\n#&gt; 1994 357.59 359.05\n#&gt; 1995 359.61 360.74\n#&gt; 1996 360.80 362.38\n#&gt; 1997 362.49 364.34"
  },
  {
    "objectID": "slides/week-7.html#initial-plot",
    "href": "slides/week-7.html#initial-plot",
    "title": "Week 7",
    "section": "Initial Plot",
    "text": "Initial Plot\n\nThe co2 dataset is a time series object (ts) with 12 observations per year, starting from January 1959.\nThere is a default plot method for ts objects that we can take advantage of:\n\n\nplot(co2, main = \"Atmospheric CO2 at Mauna Loa\", ylab = \"ppm\")\n\n\nWe see:\n\nAn upward trend\nRegular seasonal oscillations (higher in winter, lower in summer)"
  },
  {
    "objectID": "slides/week-7.html#understanding-ts-objects",
    "href": "slides/week-7.html#understanding-ts-objects",
    "title": "Week 7",
    "section": "Understanding ts Objects",
    "text": "Understanding ts Objects\nYou can think of ts objects as numeric vectors with time attributes.\n\nts objects are used to represent time series data and are created using the ts() function.\nThey have attributes like start, end, and frequency that define the time index.\nstart and end define the time range of the data.\nfrequency defines the number of observations per unit of time (e.g., 12 for monthly data).\n\n\nclass(co2)    \n#&gt; [1] \"ts\"\nstart(co2)    \n#&gt; [1] 1959    1\nend(co2)      \n#&gt; [1] 1997   12\nfrequency(co2)\n#&gt; [1] 12"
  },
  {
    "objectID": "slides/week-7.html#subsetting-and-plotting",
    "href": "slides/week-7.html#subsetting-and-plotting",
    "title": "Week 7",
    "section": "Subsetting and Plotting",
    "text": "Subsetting and Plotting\n\nLike vectors, you can subset ts objects using indexing.\nFor example, to extract the first year of data:\n\n\nco2[1:12]  # First year\n#&gt;  [1] 315.42 316.31 316.50 317.56 318.13 318.00 316.39 314.65 313.68 313.18\n#&gt; [11] 314.66 315.43\n\n\n\nOr, a specific range of years:\n\n\nwindow(co2, start = c(1990, 1), end = c(1995, 12))\n#&gt;         Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n#&gt; 1990 353.50 354.55 355.23 356.04 357.00 356.07 354.67 352.76 350.82 351.04\n#&gt; 1991 354.59 355.63 357.03 358.48 359.22 358.12 356.06 353.92 352.05 352.11\n#&gt; 1992 355.88 356.63 357.72 359.07 359.58 359.17 356.94 354.92 352.94 353.23\n#&gt; 1993 356.63 357.10 358.32 359.41 360.23 359.55 357.53 355.48 353.67 353.95\n#&gt; 1994 358.34 358.89 359.95 361.25 361.67 360.94 359.55 357.49 355.84 356.00\n#&gt; 1995 359.98 361.03 361.66 363.48 363.82 363.30 361.94 359.50 358.11 357.80\n#&gt;         Nov    Dec\n#&gt; 1990 352.69 354.07\n#&gt; 1991 353.64 354.89\n#&gt; 1992 354.09 355.33\n#&gt; 1993 355.30 356.78\n#&gt; 1994 357.59 359.05\n#&gt; 1995 359.61 360.74"
  },
  {
    "objectID": "slides/week-7.html#decomposing-a-time-series",
    "href": "slides/week-7.html#decomposing-a-time-series",
    "title": "Week 7",
    "section": "Decomposing a Time Series",
    "text": "Decomposing a Time Series\n\nDecomposition is a technique to separate a time series into its components: trend, seasonality, and residuals.\nThis helps separate structure from noise.\nIn environmental science, this is useful for:\nRemoving seasonality to study droughts\nAnalyzing long-term trends\nUnderstanding seasonal patterns in streamflow\nIdentifying anomalies in water quality data\nDetecting changes in vegetation phenology\nMonitoring seasonal patterns in temperature\nAnalyzing seasonal patterns in water demand\n‚Ä¶"
  },
  {
    "objectID": "slides/week-7.html#what-is-decomposition",
    "href": "slides/week-7.html#what-is-decomposition",
    "title": "Week 7",
    "section": "What is Decomposition?",
    "text": "What is Decomposition?\n\nDecomposition separates a time series into interpretable components:\n\n\nTrend: Long-term movement\nSeasonality: Regular, periodic fluctuations\nResidual/Irregular: Random noise or anomalies"
  },
  {
    "objectID": "slides/week-7.html#additive-vs-multiplicative",
    "href": "slides/week-7.html#additive-vs-multiplicative",
    "title": "Week 7",
    "section": "Additive vs Multiplicative",
    "text": "Additive vs Multiplicative\n\nAdditive model:\n\\[ Y_t = T_t + S_t + R_t \\]\nMultiplicative model:\n\\[ Y_t = T_t \\times S_t \\times R_t \\]\n\n\nUse additive if seasonal variation is roughly constant.\nUse multiplicative if it grows/shrinks with the trend."
  },
  {
    "objectID": "slides/week-7.html#example",
    "href": "slides/week-7.html#example",
    "title": "Week 7",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/week-7.html#why-decompose",
    "href": "slides/week-7.html#why-decompose",
    "title": "Week 7",
    "section": "Why Decompose?",
    "text": "Why Decompose?\n\nTrend: Are CO‚ÇÇ levels increasing?\nSeasonality: Are there predictable cycles each year?\nRemainder: What‚Äôs left after we remove trend and season? (what is the randomness?)"
  },
  {
    "objectID": "slides/week-7.html#decompostion-in-r",
    "href": "slides/week-7.html#decompostion-in-r",
    "title": "Week 7",
    "section": "Decompostion in R",
    "text": "Decompostion in R\n\nThe decompose() function in R can be used to perform this operation.\nThe decompose() function by default assumes that the time series is additive\n\n\ndecomp = decompose(co2, type = \"additive\")\nplot(decomp)"
  },
  {
    "objectID": "slides/week-7.html#deep-dive-trend-component",
    "href": "slides/week-7.html#deep-dive-trend-component",
    "title": "Week 7",
    "section": "Deep Dive: Trend Component",
    "text": "Deep Dive: Trend Component\n\n\n\nplot(decomp$trend, main = \"Trend Component of CO‚ÇÇ\", ylab = \"ppm\", col = \"darkred\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nSteady upward slope from ~316 ppm in 1959 to ~365 ppm in the late 1990s\nCaptures the long-term forcing from human activity:\n\nFossil fuel combustion\nDeforestation\n\nThis trend underpins climate change science ‚Äî known as the Keeling Curve\nNotice how the trend smooths short-term fluctuations"
  },
  {
    "objectID": "slides/week-7.html#interpreting-the-trend",
    "href": "slides/week-7.html#interpreting-the-trend",
    "title": "Week 7",
    "section": "Interpreting the Trend",
    "text": "Interpreting the Trend\n\nA linear model or loess smoother can also help quantify the trend:\n\n\n\n\nco2_df &lt;- data.frame(time = time(co2), co2 = as.numeric(co2))\n\nlm(as.numeric(co2) ~ time(co2)) |&gt; \n  summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = as.numeric(co2) ~ time(co2))\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -6.0399 -1.9476 -0.0017  1.9113  6.5149 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -2.250e+03  2.127e+01  -105.8   &lt;2e-16 ***\n#&gt; time(co2)    1.308e+00  1.075e-02   121.6   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.618 on 466 degrees of freedom\n#&gt; Multiple R-squared:  0.9695, Adjusted R-squared:  0.9694 \n#&gt; F-statistic: 1.479e+04 on 1 and 466 DF,  p-value: &lt; 2.2e-16\n\n\n\nco2_df |&gt;\n  ggplot(aes(x = time, y = co2)) +\n  geom_line(alpha = 0.5) +\n  geom_smooth(method = \"loess\", span = 0.2, color = \"red\", se = FALSE) +\n  labs(title = \"CO‚ÇÇ Trend with Loess Smoothing\", y = \"ppm\")\n\n\n\n\n\n\n\n\n\nDo you see acceleration in the rise over time?"
  },
  {
    "objectID": "slides/week-7.html#de-trending",
    "href": "slides/week-7.html#de-trending",
    "title": "Week 7",
    "section": "De-Trending",
    "text": "De-Trending\n\nDetrending is the process of removing the trend component from a time series.\nThis can help for modeling the trend + residual only:\n\n\ndeseasonalized &lt;- co2 - decomp$trend\nplot(deseasonalized, main = \"De-trended Series\")"
  },
  {
    "objectID": "slides/week-7.html#deep-dive-seasonal-component",
    "href": "slides/week-7.html#deep-dive-seasonal-component",
    "title": "Week 7",
    "section": "Deep Dive: Seasonal Component",
    "text": "Deep Dive: Seasonal Component\n\n\n\nplot(decomp$seasonal, main = \"Seasonal Component of CO‚ÇÇ\", \n     ylab = \"ppm\", col = \"darkgreen\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nRepeats every 12 months\nPeaks around May, drops in September‚ÄìOctober\nDriven by biospheric fluxes:\n\nPhotosynthesis during spring/summer ‚Üí CO‚ÇÇ drawdown\nDecomposition and respiration in winter ‚Üí CO‚ÇÇ release\n\nSeasonal Cycle is Northern Hemisphere-Dominated (Mauna Loa is in the Northern Hemisphere)\nNorthern Hemisphere contains more landmass and vegetation\nSo its biosphere exerts a stronger influence on global CO‚ÇÇ than the Southern Hemisphere\nThis explains the pronounced seasonal cycle in the signal"
  },
  {
    "objectID": "slides/week-7.html#de-seasonalizing",
    "href": "slides/week-7.html#de-seasonalizing",
    "title": "Week 7",
    "section": "De-seasonalizing",
    "text": "De-seasonalizing\nThis can help for modeling the seasonal + residual only:\n\ndeseasonalized &lt;- co2 - decomp$seasonal\nplot(deseasonalized, main = \"De-seasonalized Series\")"
  },
  {
    "objectID": "slides/week-7.html#deep-dive-remainder-component",
    "href": "slides/week-7.html#deep-dive-remainder-component",
    "title": "Week 7",
    "section": "Deep Dive: Remainder Component",
    "text": "Deep Dive: Remainder Component\n\n\n\nplot(decomp$random, main = \"Remainder Component (Residuals)\", \n     ylab = \"ppm\", col = \"gray40\", lwd = 1)\n\n\n\n\n\n\n\n\n\n\nResiduals are the ‚Äúleftover‚Äù part of the time series after removing trend and seasonality\nContains irregular, short-term fluctuations\nCan be used to identify anomalies or outliers\nImportant for understanding noise in the data\nPossible sources:\n\nVolcanic activity (e.g.¬†El Chich√≥n, Mt. Pinatubo)\nMeasurement error\nEl Ni√±o/La Ni√±a events (which affect carbon flux)\n\nTypically small amplitude: ¬±0.2 ppm"
  },
  {
    "objectID": "slides/week-7.html#asssessing-patterns-in-error",
    "href": "slides/week-7.html#asssessing-patterns-in-error",
    "title": "Week 7",
    "section": "Asssessing Patterns in Error?",
    "text": "Asssessing Patterns in Error?\nYou might compute the standard deviation of the residuals to assess noise:\n\nsd(na.omit(decomp$random))\n#&gt; [1] 0.2651064\n\n\nOr evaluate the residules like we did for Linear Modeling!\n\nggpubr::ggdensity(decomp$random, main = \"Residuals Histogram\", xlab = \"Residuals\")"
  },
  {
    "objectID": "slides/week-7.html#smoothing-to-remove-noise",
    "href": "slides/week-7.html#smoothing-to-remove-noise",
    "title": "Week 7",
    "section": "Smoothing to Remove Noise",
    "text": "Smoothing to Remove Noise\nIf your data is noisy - and without usefull pattern - you can use a moving average to smooth it out.\n\n\n\n\nThe zoo package provides a convenient function for this that we have already seen in the COVID lab.\nThe rollmean() function from the zoo package is useful for this.\nThe k parameter specifies the window size for the moving average.\nThe align parameter specifies how to align the moving average with the original data (e.g., ‚Äúcenter‚Äù, ‚Äúleft‚Äù, ‚Äúright‚Äù).\nThe na.pad parameter specifies whether to pad the result with NA values.\n\n\n\nco2_smooth &lt;- zoo::rollmean(co2, k = 12, align = \"center\", na.pad = TRUE)\n\nplot(co2, col = \"grey\", main = \"12-Month Moving Average\")\nlines(co2_smooth, col = \"blue\", lwd = 2)"
  },
  {
    "objectID": "slides/week-7.html#stl-decomposition-loess-based",
    "href": "slides/week-7.html#stl-decomposition-loess-based",
    "title": "Week 7",
    "section": "STL Decomposition (Loess-based)",
    "text": "STL Decomposition (Loess-based)\n\nSTL (Seasonal-Trend decomposition using Loess) adapts to changing trend or seasonality over time\nDoesn‚Äôt assume constant seasonal effect like decompose() does\nParticularly valuable when working with:\n\nLong datasets\nEnvironmental time series affected by nonlinear changes"
  },
  {
    "objectID": "slides/week-7.html#stl-decomposition-loess-based-1",
    "href": "slides/week-7.html#stl-decomposition-loess-based-1",
    "title": "Week 7",
    "section": "STL Decomposition (Loess-based)",
    "text": "STL Decomposition (Loess-based)\n\nstl() uses local regression (loess) to estimate the trend and seasonal components.\ns.window = \"periodic\" specifies that the seasonal component is periodic.\nOther options include\n\n‚Äúnone‚Äù (no seasonal component)\nor a numeric value for the seasonal window size.\n\n\n\n\n\nplot(decomp)\n\n\n\n\n\n\n\n\n\n\n?stl(co2, s.window = \"periodic\") |&gt;  \n  plot()\n#&gt; Help on topic 'plot' was found in the following packages:\n#&gt; \n#&gt;   Package               Library\n#&gt;   graphics              /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n#&gt;   base                  /Library/Frameworks/R.framework/Resources/library\n#&gt; \n#&gt; \n#&gt; Using the first match ..."
  },
  {
    "objectID": "slides/week-7.html#comparing-classical-vs-stl",
    "href": "slides/week-7.html#comparing-classical-vs-stl",
    "title": "Week 7",
    "section": "Comparing Classical vs STL",
    "text": "Comparing Classical vs STL\n\n\n\n\n\n\n\n\n\nMethod\nAssumes Constant Season?\nRobust to Outliers?\nAdaptive Smoothing?\n\n\n\n\ndecompose\n‚úÖ Yes\n‚ùå No\n‚ùå No\n\n\nstl\n‚úÖ or üîÑ (customizable)\n‚úÖ Yes\n‚úÖ Yes\n\n\n\n\nRecommendation: Use stl() for most real-world environmental time series."
  },
  {
    "objectID": "slides/week-7.html#bringing-it-all-together",
    "href": "slides/week-7.html#bringing-it-all-together",
    "title": "Week 7",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together\nFor the CO2 dataset, we can summarize the components of the time series:\n\nTrend: A human fingerprint ‚Äî atmospheric CO‚ÇÇ continues to rise year over year\nSeasonality: Driven by biospheric rhythms with not multiplicative gains\nRemainder: Small, but potentially rich with short-term signals\n\nUnderstanding these components lets us:\n\n‚úÖ Track long-term progress\n\n‚úÖ Forecast future CO‚ÇÇ\n\n‚úÖ Communicate patterns clearly to policymakers"
  },
  {
    "objectID": "slides/week-7.html#time-series-in-the-tidyverse-tsibble-feasts",
    "href": "slides/week-7.html#time-series-in-the-tidyverse-tsibble-feasts",
    "title": "Week 7",
    "section": "Time Series in the Tidyverse: tsibble / feasts",
    "text": "Time Series in the Tidyverse: tsibble / feasts\n\ntsibble is a tidy data frame for time series data. It extends the tibble class to include time series attributes.\nCompatible with dplyr, ggplot2\nfeasts provides functions for time series analysis and visualization.\n\n\nco2_tbl &lt;- as_tsibble(co2)\nhead(co2_tbl)\n#&gt; # A tsibble: 6 x 2 [1M]\n#&gt;      index value\n#&gt;      &lt;mth&gt; &lt;dbl&gt;\n#&gt; 1 1959 Jan  315.\n#&gt; 2 1959 Feb  316.\n#&gt; 3 1959 Mar  316.\n#&gt; 4 1959 Apr  318.\n#&gt; 5 1959 May  318.\n#&gt; 6 1959 Jun  318"
  },
  {
    "objectID": "slides/week-7.html#decomposing-with-feasts",
    "href": "slides/week-7.html#decomposing-with-feasts",
    "title": "Week 7",
    "section": "Decomposing with feasts",
    "text": "Decomposing with feasts\nfeasts provides:\n\na STL() function for seasonal decomposition.\ncomponents() extracts the components of the decomposition.\ngg_season() and gg_subseries() visualize seasonal patterns.\ngg_lag() visualizes autocorrelation and lagged relationships.\n\n\nco2_decomp &lt;- co2_tbl |&gt;\n  model(STL(value ~ season(window = \"periodic\"))) |&gt;\n  components()\n\nglimpse(co2_decomp)\n#&gt; Rows: 468\n#&gt; Columns: 7\n#&gt; Key: .model [1]\n#&gt; : value = trend + season_year + remainder\n#&gt; $ .model        &lt;chr&gt; \"STL(value ~ season(window = \\\"periodic\\\"))\", \"STL(value‚Ä¶\n#&gt; $ index         &lt;mth&gt; 1959 Jan, 1959 Feb, 1959 Mar, 1959 Apr, 1959 May, 1959 J‚Ä¶\n#&gt; $ value         &lt;dbl&gt; 315.42, 316.31, 316.50, 317.56, 318.13, 318.00, 316.39, ‚Ä¶\n#&gt; $ trend         &lt;dbl&gt; 315.1954, 315.3023, 315.4093, 315.5147, 315.6201, 315.71‚Ä¶\n#&gt; $ season_year   &lt;dbl&gt; -0.06100103, 0.59463870, 1.32899651, 2.46904706, 2.95704‚Ä¶\n#&gt; $ remainder     &lt;dbl&gt; 0.28564410, 0.41305456, -0.23825306, -0.42371128, -0.447‚Ä¶\n#&gt; $ season_adjust &lt;dbl&gt; 315.4810, 315.7154, 315.1710, 315.0910, 315.1730, 315.68‚Ä¶"
  },
  {
    "objectID": "slides/week-7.html#component-access",
    "href": "slides/week-7.html#component-access",
    "title": "Week 7",
    "section": "Component Access",
    "text": "Component Access\n\n\nAutoplot\n\nautoplot(co2_decomp) +\n  labs(title = \"STL Decomposition of CO‚ÇÇ\", y = \"ppm\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nComponent Access\n\nggpubr::ggdensity(co2_decomp$remainder, main = \"Residual Component\")\n\n\n\n\n\n\n\nshapiro.test(co2_decomp$remainder)\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  co2_decomp$remainder\n#&gt; W = 0.99485, p-value = 0.1202"
  },
  {
    "objectID": "slides/week-7.html#advanced-visualizationmodeling-with-feasts",
    "href": "slides/week-7.html#advanced-visualizationmodeling-with-feasts",
    "title": "Week 7",
    "section": "Advanced Visualization/Modeling with feasts",
    "text": "Advanced Visualization/Modeling with feasts"
  },
  {
    "objectID": "slides/week-7.html#gg_lag",
    "href": "slides/week-7.html#gg_lag",
    "title": "Week 7",
    "section": "gg_lag()",
    "text": "gg_lag()\n\nA lag plot is a scatterplot of a time series against itself, but with a time shift (or ‚Äúlag‚Äù) applied to one of the series.\nIt helps us visualize the relationship between current and past values of a time series.\n\nüì¶ Here‚Äôs the simple idea:\n\n\nYou take your CO‚ÇÇ measurements over time.\n\nThen you make a graph where you plot today‚Äôs CO‚ÇÇ (on the Y-axis)‚Ä¶\n\n‚Ä¶against CO‚ÇÇ from a few months ago (on the X-axis).\n\nThis shows you if the past helps predict the present!\n\n\nüìä If it makes a curvy shape or a line‚Ä¶\n\n\n‚Ä¶that means there‚Äôs a pattern! Your data remembers what happened before ‚Äî like a smart friend who learns from their past.\n\nBut if the dots look like a big messy spaghetti mess that means the data is random, with no memory of what happened before."
  },
  {
    "objectID": "slides/week-7.html#gg_lag-1",
    "href": "slides/week-7.html#gg_lag-1",
    "title": "Week 7",
    "section": "gg_lag()",
    "text": "gg_lag()\nüß† Why this is useful:\n\n\nIt helps us see if there are patterns in the data.\n\nIt helps us understand how past values affect current values.\n\nIt helps us decide if we can use this data to make predictions in the future.\n\n\n\nco2_tbl |&gt; \n  gg_lag() +\n  labs(title = \"Lag Plot of CO‚ÇÇ\", x = \"Lagged CO‚ÇÇ\", y = \"Current CO‚ÇÇ\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-7.html#gg_subseries",
    "href": "slides/week-7.html#gg_subseries",
    "title": "Week 7",
    "section": "gg_subseries",
    "text": "gg_subseries\n\nImagine the co2 dataset as a big collection of monthly CO‚ÇÇ data from Mauna Loa. Each month is a group, and each year is a new object\nNow, we want to see how each month behaves across the years, so we can spot if January always has higher or lower CO‚ÇÇ levels, for example.\n\nüìä What this does:\n\nEach month: A different color shows up for each month (like January in blue, February in red, etc.)\nLines: You see the CO‚ÇÇ values for each month over different years. For example, you might notice that CO‚ÇÇ levels tend to be higher in the spring and lower in the fall.\n\nüß† Why this is useful:\n\nWe can spot seasonal trends: Does CO‚ÇÇ rise in the winter? Or fall in the summer?\nWe can also compare months across the years: Is January more or less CO‚ÇÇ-heavy compared to other months?"
  },
  {
    "objectID": "slides/week-7.html#gg_subseries-1",
    "href": "slides/week-7.html#gg_subseries-1",
    "title": "Week 7",
    "section": "gg_subseries()",
    "text": "gg_subseries()\n\ngg_subseries(co2_tbl) +\n  labs(title = \"Monthly CO‚ÇÇ Patterns\", y = \"CO‚ÇÇ (ppm)\", x = \"Year\") + \n  theme_minimal()"
  },
  {
    "objectID": "slides/week-7.html#gg_season",
    "href": "slides/week-7.html#gg_season",
    "title": "Week 7",
    "section": "gg_season",
    "text": "gg_season\n\nThink of gg_season() like a way to look at a yearly picture of CO‚ÇÇ and see if the Earth follows a seasonal rhythm.\nIt makes the changes in CO‚ÇÇ easy to spot when you look at months side-by-side.\n\nüìä What this does:\n\nIt shows you how CO‚ÇÇ changes during each month of the year, but it puts all the years together, so you can see if the same thing happens every year in January, February, and so on.\n\nüß† Why this is useful:\n\nYou‚Äôll see a smooth curve for each month. Each curve shows how CO‚ÇÇ goes up and down each year in the same pattern."
  },
  {
    "objectID": "slides/week-7.html#gg_season-1",
    "href": "slides/week-7.html#gg_season-1",
    "title": "Week 7",
    "section": "gg_season",
    "text": "gg_season\n\ngg_season(co2_tbl) +\n  labs(title = \"Seasonal Patterns of CO‚ÇÇ\", y = \"CO‚ÇÇ (ppm)\", x = \"Month\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-7.html#bonus-interactive-plotting",
    "href": "slides/week-7.html#bonus-interactive-plotting",
    "title": "Week 7",
    "section": "Bonus: Interactive Plotting",
    "text": "Bonus: Interactive Plotting\n\nIn last weeks demo, we used plotly to generate an interactive plot of the hyperparameter tuning process.\nYou can use the plotly package to add interactivity to your ggplots directly with ggplotly()!\n\n\nlibrary(plotly)\n\nco2_plot &lt;- co2_tbl |&gt;\n  autoplot() +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"Interactive CO‚ÇÇ Time Series\", x = \"Date\", y = \"ppm\")"
  },
  {
    "objectID": "slides/week-7.html#bonus-interactive-plotting-1",
    "href": "slides/week-7.html#bonus-interactive-plotting-1",
    "title": "Week 7",
    "section": "Bonus: Interactive Plotting",
    "text": "Bonus: Interactive Plotting\n\nggplotly(co2_plot)"
  },
  {
    "objectID": "slides/week-7.html#monday",
    "href": "slides/week-7.html#monday",
    "title": "Week 7",
    "section": "Monday",
    "text": "Monday\n\nWe covered the following topics:\n\nsate-time data in R (date, POSIXct, POSIXlt)\nTime series data/structure (ts, tsibble, tibble + Date column)\nTime series decomposition\n\nTrend, seasonality, and residuals\n\nSmoothing time series data\n\nSmoothing with moving averages"
  },
  {
    "objectID": "slides/week-7.html#today",
    "href": "slides/week-7.html#today",
    "title": "Week 7",
    "section": "Today",
    "text": "Today\n\nForecasting with two distinct models (1) ARIMA (2) Prophet\nUnderstanding modeltime + tidymodels integration\nForecasting Process for time series"
  },
  {
    "objectID": "slides/week-7.html#toy-example-river-time-series-arima",
    "href": "slides/week-7.html#toy-example-river-time-series-arima",
    "title": "Week 7",
    "section": "Toy Example: River Time Series & ARIMA",
    "text": "Toy Example: River Time Series & ARIMA\nLet‚Äôs say we‚Äôre watching how much water flows in a river every day:\n\nMonday: 50 cfs\n\nTuesday: 60 cfs\n\nWednesday: 70 cfs\n\nWe want to guess tomorrow‚Äôs flow."
  },
  {
    "objectID": "slides/week-7.html#autoregressive-ar",
    "href": "slides/week-7.html#autoregressive-ar",
    "title": "Week 7",
    "section": "1. AutoRegressive (AR)",
    "text": "1. AutoRegressive (AR)\n\nThis means: ‚ÄúLook at yesterday and the day before!‚Äù\nIf flow has been going up each day, AR says: ** ‚ÄúTomorrow might go up again ‚Ä¶‚Äù **\nIt‚Äôs like the river has a pattern."
  },
  {
    "objectID": "slides/week-7.html#integrated-i",
    "href": "slides/week-7.html#integrated-i",
    "title": "Week 7",
    "section": "2. Integrated (I)",
    "text": "2. Integrated (I)\n\nSometimes the flow just keeps climbing ‚Äî like during a spring snowmelt!\nTo help ARIMA think better, we subtract yesterday‚Äôs number from today‚Äôs.\nThis makes the numbers more steady so ARIMA can do its magic."
  },
  {
    "objectID": "slides/week-7.html#moving-average-ma",
    "href": "slides/week-7.html#moving-average-ma",
    "title": "Week 7",
    "section": "3. Moving Average (MA)",
    "text": "3. Moving Average (MA)\n\nThe river sometimes gets a surprise flux (like a big rainstorm üåßÔ∏è).\nMA looks at those surprises and helps smooth them out.\nSo if it rained two days ago, MA might say: ‚ÄúDon‚Äôt expect another surprise tomorrow.‚Äù"
  },
  {
    "objectID": "slides/week-7.html#put-it-all-together-ar-i-ma-arima",
    "href": "slides/week-7.html#put-it-all-together-ar-i-ma-arima",
    "title": "Week 7",
    "section": "Put It All Together: AR + I + MA = ARIMA",
    "text": "Put It All Together: AR + I + MA = ARIMA\n\nAR: Uses the river‚Äôs memory (trend)\nI: Calms the sturcutral components (season)\nMA: Handles noisy surprises (noise)\n\nNow we can forecast tomorrow‚Äôs flow!"
  },
  {
    "objectID": "slides/week-7.html#arima",
    "href": "slides/week-7.html#arima",
    "title": "Week 7",
    "section": "ARIMA",
    "text": "ARIMA\nThe basics of a ARIMA (AutoRegressive Integrated Moving Average) Model include:\n\n\nAR: AutoRegressive part (past values):\n\n\nAR(1) = current value depends on the previous value\n\nAR(2) = current value depends on the previous two values\n\n\nI: Integrated part (differencing to make data stationary)\n\n\nDifferencing removes trends and seasonality\n\ne.g., diff(co2, lag = 12) removes annual seasonality\n\n\nMA: Moving Average part (past errors)\n\n\nMA(1) = current value depends on the previous error\n\nMA(2) = current value depends on the previous two errors\n\n\np, d, q: Parameters for AR, I, and MA\n\n\np = number of lagged values (AR)\n\nd = number of differences (I)\n\nq = number of lagged errors (MA)"
  },
  {
    "objectID": "slides/week-7.html#aic",
    "href": "slides/week-7.html#aic",
    "title": "Week 7",
    "section": "AIC",
    "text": "AIC\nThe AIC metric helps choose between models/parameters:\n\nIt rewards:\n\nGood predictions\nSimplicity\n\nIt punishes:\n\nComplexity (too many parameters)\nOverfitting (fitting noise instead of the trend)\n\nLower AIC = better model"
  },
  {
    "objectID": "slides/week-7.html#a-simple-forecasting-example",
    "href": "slides/week-7.html#a-simple-forecasting-example",
    "title": "Week 7",
    "section": "A Simple Forecasting Example",
    "text": "A Simple Forecasting Example\n\nauto.arima() is a function from the forecast package that automatically selects the best ARIMA model for your data accoridning toe to the AICc criterion.\nThe AICc (Akaike Information Criterion corrected) is a measure of the relative quality of statistical models for a given dataset.\nIt is used to compare different models and select the one that best fits the data while penalizing for complexity.\nThe auto.arima() function will automatically select the best parameters (p,d,q) based on the AICc criterion."
  },
  {
    "objectID": "slides/week-7.html#section-1",
    "href": "slides/week-7.html#section-1",
    "title": "Week 7",
    "section": "",
    "text": "library(forecast)\n\nco2_arima &lt;- auto.arima(co2)\n\nsummary(co2_arima)\n#&gt; Series: co2 \n#&gt; ARIMA(1,1,1)(1,1,2)[12] \n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1      ma1     sar1     sma1     sma2\n#&gt;       0.2569  -0.5847  -0.5489  -0.2620  -0.5123\n#&gt; s.e.  0.1406   0.1204   0.5880   0.5701   0.4819\n#&gt; \n#&gt; sigma^2 = 0.08576:  log likelihood = -84.39\n#&gt; AIC=180.78   AICc=180.97   BIC=205.5\n#&gt; \n#&gt; Training set error measures:\n#&gt;                      ME     RMSE       MAE         MPE       MAPE      MASE\n#&gt; Training set 0.01742092 0.287159 0.2303994 0.005073769 0.06845665 0.1819636\n#&gt;                      ACF1\n#&gt; Training set -0.002858162"
  },
  {
    "objectID": "slides/week-7.html#forecasting-with-arima",
    "href": "slides/week-7.html#forecasting-with-arima",
    "title": "Week 7",
    "section": "Forecasting with ARIMA",
    "text": "Forecasting with ARIMA\n\nThe forecast() function is used to generate forecasts from the fitted ARIMA model.\nThe h argument specifies the number of periods to forecast into the future.\n\n\nco2_forecast &lt;- forecast(co2_arima, h = 60)\n\nplot(co2_forecast)"
  },
  {
    "objectID": "slides/week-7.html#arima11111212",
    "href": "slides/week-7.html#arima11111212",
    "title": "Week 7",
    "section": "üî¢ ARIMA(1,1,1)(1,1,2)[12]",
    "text": "üî¢ ARIMA(1,1,1)(1,1,2)[12]\nARIMA Notation can be broken into two parts:\n1. Non-seasonal part: ARIMA(1, 1, 1)\nThis is the ‚Äúregular‚Äù ARIMA: - AR(1): One autoregressive term ‚Äî the model looks at one lag of the time series.\n\nI(1): One differencing ‚Äî the model uses the change in values instead of raw values to make the series more stationary.\nMA(1): One moving average term ‚Äî the model corrects using one lag of the error term.\n\n2. Seasonal part: (1, 1, 2)[12]\nThis is the seasonal pattern ‚Äî repeated every 12 time units (like months in a year):\n\nSAR(1): One seasonal autoregressive term ‚Äî it uses the value from 12 time steps ago.\nSI(1): One seasonal difference ‚Äî subtracts the value from 12 steps ago to remove seasonal patterns.\nSMA(2): Two seasonal moving average terms ‚Äî uses errors from 12 and 24 time steps ago.\n[12]: This is the seasonal period, i.e., it‚Äôs a yearly pattern with monthly data."
  },
  {
    "objectID": "slides/week-7.html#arima11111212-1",
    "href": "slides/week-7.html#arima11111212-1",
    "title": "Week 7",
    "section": "üî¢ ARIMA(1,1,1)(1,1,2)[12]",
    "text": "üî¢ ARIMA(1,1,1)(1,1,2)[12]\nHey, ARIMA please‚Ä¶\n\n‚ÄúModel the data using a mix of its last value, the last error, and their seasonal versions from 12 months ago ‚Äî but first difference it once to remove trend and once seasonally to remove yearly patterns.‚Äù"
  },
  {
    "objectID": "slides/week-7.html#note",
    "href": "slides/week-7.html#note",
    "title": "Week 7",
    "section": "Note",
    "text": "Note\nARIMA modeling works well when data is stationary. - Stationarity means the statistical properties of the series (mean, variance) do not change over time. - Non-stationary data can lead to unreliable forecasts and misleading results."
  },
  {
    "objectID": "slides/week-7.html#prophet",
    "href": "slides/week-7.html#prophet",
    "title": "Week 7",
    "section": "Prophet",
    "text": "Prophet\n\nProphet is an open-source tool for forecasting time series data.\nDeveloped by Facebook (Meta)\nDesigned for analysts and data scientists\nHandles missing data, outliers, and seasonality"
  },
  {
    "objectID": "slides/week-7.html#key-features-of-prophet",
    "href": "slides/week-7.html#key-features-of-prophet",
    "title": "Week 7",
    "section": "Key Features of Prophet",
    "text": "Key Features of Prophet\n‚úÖ Additive Model: Trend + Seasonality + Holidays + Noise\n‚úÖ Automatic Changepoint Detection\n‚úÖ Support for Custom Holidays & Events\n‚úÖ Flexible Seasonality (daily/weekly/yearly)\n‚úÖ Easy-to-use API in R and Python"
  },
  {
    "objectID": "slides/week-7.html#prophets-model-structure",
    "href": "slides/week-7.html#prophets-model-structure",
    "title": "Week 7",
    "section": "Prophet‚Äôs Model Structure",
    "text": "Prophet‚Äôs Model Structure\nProphet decomposes time series into components:\n\\[ y(t) = g(t) + s(t) + h(t) + Œµ(t) \\]\n\ng(t): Trend (linear or logistic growth)\ns(t): Seasonality (Fourier series)\nh(t): Holiday effects\nŒµ(t): Error term (noise)\n\nüìå Assumes additive components by default; multiplicative also possible"
  },
  {
    "objectID": "slides/week-7.html#a-simple-forecasting-example-1",
    "href": "slides/week-7.html#a-simple-forecasting-example-1",
    "title": "Week 7",
    "section": "A Simple Forecasting Example",
    "text": "A Simple Forecasting Example\nYour time series must have: (1) ds column (date/timestamp) (2) y column (value to forecast)\n\nlibrary(prophet)\nprophet_mod &lt;- tsibble::as_tsibble(co2) |&gt; \n  # prophet requires ds and y columns\n  dplyr::rename(ds = index, y = value) |&gt; \n  prophet()\n\n\n# Make future dataframe and predict\nfuture   &lt;- make_future_dataframe(prophet_mod, periods = 1000)\n\nforecast &lt;- predict(prophet_mod, future)\n\n# Plot the forecast\nplot(prophet_mod, forecast) + \n  theme_minimal()"
  },
  {
    "objectID": "slides/week-7.html#pros-cons",
    "href": "slides/week-7.html#pros-cons",
    "title": "Week 7",
    "section": "Pros / Cons",
    "text": "Pros / Cons\n\n\n\n\n\n\n\n\nFeature\nARIMA\nProphet\n\n\n\n\nStatistical rigor\nBased on strong statistical theory; well-studied\nIntuitive, decomposable model (trend + seasonality + events)\n\n\nInterpretability\nClear interpretation of AR, MA, differencing terms\nPlots components like trend/seasonality directly\n\n\nFlexibility (SARIMA)\nSeasonal ARIMA can handle seasonal structure\nHandles multiple seasonalities natively (yearly, weekly, daily)\n\n\nControl over params\nFine-tuned control over differencing, lags, and model order\nEasy to specify changepoints, seasonality, and custom events\n\n\nStatistical testing\nIncludes AIC/BIC for model selection\nCross-validation support; uncertainty intervals included\n\n\nRequires stationarity\nTime series must be stationary or made so (differencing)\nHandles non-stationary data out of the box\n\n\nModel complexity\nNeeds careful tuning (p,d,q) and domain expertise\nDefaults work well; limited tuning needed\n\n\nHoliday effects\nMust be encoded manually\nEasy to include holidays/events\n\n\nMultivariate support\nBasic ARIMA doesn‚Äôt support exogenous variables easily (need ARIMAX)\nSupports external regressors with add_regressor()\n\n\nNon-linear trends\nPoor performance with structural breaks or non-linear growth\nHandles changepoints and logistic growth models well\n\n\nSeasonality limits\nSARIMA handles only one seasonal period well\nBuilt-in multiple seasonal components (e.g., daily, weekly, yearly)\n\n\n\nTL;DR\n\nUse ARIMA if you want a classic, statistical model with deep customization and you‚Äôre comfortable making your data stationary.\nUse Prophet if you want a fast, robust, and intuitive model for business or environmental data with strong seasonal effects and irregular events."
  },
  {
    "objectID": "slides/week-7.html#to-much-complexity",
    "href": "slides/week-7.html#to-much-complexity",
    "title": "Week 7",
    "section": "To much complexity!!",
    "text": "To much complexity!!\n\nEach model has it own requirements, arguments, and tuning parameters.\nSimular to our ML models, this introduces a large time sink, opportunity for error, and complexity.\n\n\n\nmodeltime brings tidy workflows to time series forecasting using the parsnip and workflows frameworks from tidymodels.\nCombine multiple models (ARIMA, Prophet, XGBoost) in one framework to"
  },
  {
    "objectID": "slides/week-7.html#modeltime-integration",
    "href": "slides/week-7.html#modeltime-integration",
    "title": "Week 7",
    "section": "Modeltime Integration  ",
    "text": "Modeltime Integration  \n\nlibrary(modeltime)\nlibrary(tidymodels)\nlibrary(timetk)"
  },
  {
    "objectID": "slides/week-7.html#create-a-time-series-split",
    "href": "slides/week-7.html#create-a-time-series-split",
    "title": "Week 7",
    "section": "1. Create a time series split ‚Ä¶",
    "text": "1. Create a time series split ‚Ä¶\n\nUse time_series_split() to make a train/test set.\nSetting assess = ‚Äú‚Ä¶‚Äù tells the function to use the last ‚Ä¶ of data as the testing set.\nSetting cumulative = TRUE tells the sampling to use all of the prior data as the training set.\n\n\nco2_tbl &lt;-  tsibble::as_tsibble(co2) |&gt; \n  as_tibble() |&gt;\n  mutate(date = as.Date(index), index = NULL) \n\nsplits &lt;- time_series_split(co2_tbl, assess = \"60 months\", cumulative = TRUE)\n\ntraining &lt;-  training(splits)\ntesting  &lt;-  testing(splits)"
  },
  {
    "objectID": "slides/week-7.html#specify-models",
    "href": "slides/week-7.html#specify-models",
    "title": "Week 7",
    "section": "2. Specify Models ‚Ä¶",
    "text": "2. Specify Models ‚Ä¶\nJust like tidymodels ‚Ä¶\n\nModel Spec: arima_reg()/prophet_reg()/prophet_boost() &lt;‚Äì This sets up your general model algorithm and key parameters\nModel Mode: mode = \"regression\"/mode = \"classification\" &lt;‚Äì This sets the model mode to regression or classification (timeseries is always regression!)\nSet Engine: set_engine(\"auto_arima\")/set_engine(\"prophet\") / &lt;‚Äì This selects the specific package-function to use, you can add any function-level arguments here.\n\n\nmods &lt;- list(\n  arima_reg() |&gt;  set_engine(\"auto_arima\"),\n  \n  arima_boost(min_n = 2, learn_rate = 0.015) |&gt; set_engine(engine = \"auto_arima_xgboost\"),\n  \n  prophet_reg() |&gt; set_engine(\"prophet\"),\n  \n  prophet_boost() |&gt; set_engine(\"prophet_xgboost\"),\n  \n  # Exponential Smoothing State Space model\n  exp_smoothing() |&gt; set_engine(engine = \"ets\"),\n  \n  # Multivariate Adaptive Regression Spline model\n  mars(mode = \"regression\") |&gt; set_engine(\"earth\") \n)"
  },
  {
    "objectID": "slides/week-7.html#fit-models",
    "href": "slides/week-7.html#fit-models",
    "title": "Week 7",
    "section": "3. Fit Models ‚Ä¶",
    "text": "3. Fit Models ‚Ä¶\n\nUse purrr::map() to fit the models to the training data.\nFit Model: fit(value ~ date, training) &lt;‚Äì All modeltime models require a date column to be a regressor.\n\n\nmodels &lt;- map(mods, ~ fit(.x, value ~ date, data = training))"
  },
  {
    "objectID": "slides/week-7.html#build-modeltime-table",
    "href": "slides/week-7.html#build-modeltime-table",
    "title": "Week 7",
    "section": "4. Build modeltime table ‚Ä¶",
    "text": "4. Build modeltime table ‚Ä¶\n\nUse modeltime_table() to combine multiple models into a single table that can be used for calibration, accuracy, and forecasting.\n\nmodeltime_table():\n\nCreates a table of models\nValidates that all objects are models (parsnip or workflows objects) and all models have been fitted\nProvides an ID and Description of the models\n\nas_modeltime_table():\n\nConverts a list of models to a modeltime table. Useful if programatically creating Modeltime Tables from models stored in a list (e.g.¬†from map).\n\n\n(models_tbl &lt;- as_modeltime_table(models))\n#&gt; # Modeltime Table\n#&gt; # A tibble: 6 √ó 3\n#&gt;   .model_id .model   .model_desc            \n#&gt;       &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                  \n#&gt; 1         1 &lt;fit[+]&gt; ARIMA(1,1,1)(2,1,2)[12]\n#&gt; 2         2 &lt;fit[+]&gt; ARIMA(1,1,1)(0,1,1)[12]\n#&gt; 3         3 &lt;fit[+]&gt; PROPHET                \n#&gt; 4         4 &lt;fit[+]&gt; PROPHET                \n#&gt; 5         5 &lt;fit[+]&gt; ETS(M,A,A)             \n#&gt; 6         6 &lt;fit[+]&gt; EARTH"
  },
  {
    "objectID": "slides/week-7.html#notes",
    "href": "slides/week-7.html#notes",
    "title": "Week 7",
    "section": "Notes:",
    "text": "Notes:\nmodeltime_table() does some basic checking to ensure all models are fit and organized into a scalable structure called a ‚ÄúModeltime Table‚Äù that is used as part of our forecasting workflow.\n\nIt‚Äôs expected that tuning and parameter selection is performed prior to incorporating into a Modeltime Table.\nIf you try to add an unfitted model, the modeltime_table() will complain (throw an informative error) saying you need to fit() the model."
  },
  {
    "objectID": "slides/week-7.html#calibrate-the-models",
    "href": "slides/week-7.html#calibrate-the-models",
    "title": "Week 7",
    "section": "5. Calibrate the Models ‚Ä¶",
    "text": "5. Calibrate the Models ‚Ä¶\n\nUse modeltime_calibrate() to evaluate the models on the test set.\nCalibrating adds a new column, .calibration_data, with the test predictions and residuals inside.\n\n\n(calibration_table &lt;- modeltime_calibrate(models_tbl, testing, quiet = FALSE))\n#&gt; # Modeltime Table\n#&gt; # A tibble: 6 √ó 5\n#&gt;   .model_id .model   .model_desc             .type .calibration_data\n#&gt;       &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n#&gt; 1         1 &lt;fit[+]&gt; ARIMA(1,1,1)(2,1,2)[12] Test  &lt;tibble [60 √ó 4]&gt;\n#&gt; 2         2 &lt;fit[+]&gt; ARIMA(1,1,1)(0,1,1)[12] Test  &lt;tibble [60 √ó 4]&gt;\n#&gt; 3         3 &lt;fit[+]&gt; PROPHET                 Test  &lt;tibble [60 √ó 4]&gt;\n#&gt; 4         4 &lt;fit[+]&gt; PROPHET                 Test  &lt;tibble [60 √ó 4]&gt;\n#&gt; 5         5 &lt;fit[+]&gt; ETS(M,A,A)              Test  &lt;tibble [60 √ó 4]&gt;\n#&gt; 6         6 &lt;fit[+]&gt; EARTH                   Test  &lt;tibble [60 √ó 4]&gt;\n\nA few notes on Calibration:\n\nCalibration builds confidence intervals and accuracy metrics\nCalibration Data is the predictions and residuals that are calculated from out-of-sample data.\nAfter calibrating, the calibration data follows the data through the forecasting workflow."
  },
  {
    "objectID": "slides/week-7.html#testing-forecast-accuracy-evaluation",
    "href": "slides/week-7.html#testing-forecast-accuracy-evaluation",
    "title": "Week 7",
    "section": "Testing Forecast & Accuracy Evaluation",
    "text": "Testing Forecast & Accuracy Evaluation\nThere are 2 critical parts to an evaluation.\n\nEvaluating the Test (Out of Sample) Accuracy\nVisualizing the Forecast vs Test Data Set"
  },
  {
    "objectID": "slides/week-7.html#accuracy",
    "href": "slides/week-7.html#accuracy",
    "title": "Week 7",
    "section": "Accuracy",
    "text": "Accuracy\nmodeltime_accuracy() collects common accuracy metrics using yardstick functions:\n\nMAE - Mean absolute error, mae()\nMAPE - Mean absolute percentage error, mape()\nMASE - Mean absolute scaled error, mase()\nSMAPE - Symmetric mean absolute percentage error, smape()\nRMSE - Root mean squared error, rmse()\nRSQ - R-squared, rsq()\n\n\nmodeltime_accuracy(calibration_table) |&gt; \n  arrange(mae)\n#&gt; # A tibble: 6 √ó 9\n#&gt;   .model_id .model_desc             .type   mae  mape  mase smape  rmse   rsq\n#&gt;       &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1         1 ARIMA(1,1,1)(2,1,2)[12] Test  0.810 0.224 0.687 0.224 0.967 0.973\n#&gt; 2         2 ARIMA(1,1,1)(0,1,1)[12] Test  0.885 0.244 0.750 0.245 1.05  0.969\n#&gt; 3         3 PROPHET                 Test  1.06  0.295 0.899 0.294 1.16  0.986\n#&gt; 4         4 PROPHET                 Test  1.06  0.295 0.899 0.294 1.16  0.986\n#&gt; 5         5 ETS(M,A,A)              Test  1.48  0.409 1.25  0.410 1.71  0.958\n#&gt; 6         6 EARTH                   Test  1.89  0.526 1.61  0.526 2.25  0.499"
  },
  {
    "objectID": "slides/week-7.html#forecast",
    "href": "slides/week-7.html#forecast",
    "title": "Week 7",
    "section": "Forecast",
    "text": "Forecast\n\nUse modeltime_forecast() to generate forecasts for the next 120 months (10 years).\nUse plot_modeltime_forecast() to visualize the forecasts.\n\n\n(forecast &lt;- calibration_table  |&gt; \n  modeltime_forecast(h = \"60 months\", \n                     new_data = testing,\n                     actual_data = co2_tbl) )\n#&gt; # Forecast Results\n#&gt; \n#&gt; # A tibble: 828 √ó 7\n#&gt;    .model_id .model_desc .key   .index     .value .conf_lo .conf_hi\n#&gt;        &lt;int&gt; &lt;chr&gt;       &lt;fct&gt;  &lt;date&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1        NA ACTUAL      actual 1959-01-01   315.       NA       NA\n#&gt;  2        NA ACTUAL      actual 1959-02-01   316.       NA       NA\n#&gt;  3        NA ACTUAL      actual 1959-03-01   316.       NA       NA\n#&gt;  4        NA ACTUAL      actual 1959-04-01   318.       NA       NA\n#&gt;  5        NA ACTUAL      actual 1959-05-01   318.       NA       NA\n#&gt;  6        NA ACTUAL      actual 1959-06-01   318        NA       NA\n#&gt;  7        NA ACTUAL      actual 1959-07-01   316.       NA       NA\n#&gt;  8        NA ACTUAL      actual 1959-08-01   315.       NA       NA\n#&gt;  9        NA ACTUAL      actual 1959-09-01   314.       NA       NA\n#&gt; 10        NA ACTUAL      actual 1959-10-01   313.       NA       NA\n#&gt; # ‚Ñπ 818 more rows"
  },
  {
    "objectID": "slides/week-7.html#vizualize",
    "href": "slides/week-7.html#vizualize",
    "title": "Week 7",
    "section": "Vizualize",
    "text": "Vizualize\n\nplot_modeltime_forecast(forecast)"
  },
  {
    "objectID": "slides/week-7.html#refit-to-full-dataset-forecast-forward",
    "href": "slides/week-7.html#refit-to-full-dataset-forecast-forward",
    "title": "Week 7",
    "section": "Refit to Full Dataset & Forecast Forward",
    "text": "Refit to Full Dataset & Forecast Forward\nThe final step is to refit the models to the full dataset using modeltime_refit() and forecast them forward.\n\nrefit_tbl &lt;- calibration_table |&gt;\n    modeltime_refit(data = co2_tbl)\n\nrefit_tbl |&gt;\n    modeltime_forecast(h = \"3 years\", actual_data = co2_tbl) |&gt;\n    plot_modeltime_forecast()"
  },
  {
    "objectID": "slides/week-7.html#why-refit",
    "href": "slides/week-7.html#why-refit",
    "title": "Week 7",
    "section": "Why refit?",
    "text": "Why refit?\nThe models have all changed! This is the (potential) benefit of refitting.\nMore often than not refitting is a good idea. Refitting:\n\nRetrieves your model and preprocessing steps\nRefits the model to the new data\nRecalculates any automations. This includes:\n\nRecalculating the changepoints for the Earth Model\nRecalculating the ARIMA and ETS parameters\n\nPreserves any parameter selections. This includes:\nAny other defaults that are not automatic calculations are used."
  },
  {
    "objectID": "slides/week-7.html#growing-ecosystem",
    "href": "slides/week-7.html#growing-ecosystem",
    "title": "Week 7",
    "section": "Growing Ecosystem",
    "text": "Growing Ecosystem"
  },
  {
    "objectID": "slides/week-7.html#summary-takeaways",
    "href": "slides/week-7.html#summary-takeaways",
    "title": "Week 7",
    "section": "Summary & Takeaways",
    "text": "Summary & Takeaways\n\nEnvironmental science is rich with time series applications\nTime series data is sequential and ordered\nlubridate simplifies handling and extracting date-time features\nUse ts, zoo, xts for classic or irregular data\nUse tsibble, feasts, and fable for tidy workflows\nLearn to decompose, smooth, and forecast\nUse modeltime for advanced modeling and forecasting"
  },
  {
    "objectID": "slides/week-7.html#forecasting",
    "href": "slides/week-7.html#forecasting",
    "title": "Week 7",
    "section": "Forecasting:",
    "text": "Forecasting:\n\nForecasting with two distinct models (1) ARIMA (2) Prophet\nUnderstanding modeltime + tidymodels integration\nForecasting Process for time series"
  },
  {
    "objectID": "labs/lab6.html#modeltime-prediction",
    "href": "labs/lab6.html#modeltime-prediction",
    "title": "Lab 6: Timeseries Data",
    "section": "Modeltime Prediction",
    "text": "Modeltime Prediction\n\nData Prep\nNow we are going to use the modeltime package to predict future streamflow using the climate data. We will use the modeltime package to create a time series model. First, we need to prep the data. In this case we want both the histroic and future data to be in the same format where the date column is a Date object (name it date for ease) and the other columns are numeric. In both cases each need to be a tibble:\nWe will use the time_series_split() function to split the data into training and testing sets. The training set will be used to train the model and the testing set will be used to test the model. Rememer in this case we are using a time series split, so the training set will be defined by a period rather then a perent of hold out data. In this case we will use a 24 month period testing set (assess). Make sure to set a seed and extract the trainign and testing data.frames with training() and testing().\n\n\nModel Definition\nChose at least three models to test - with one being arima and one being prophet. Store these models (specification + engine) in a list.\n\n\nModel Fitting\nNext, we will use the fit() function to fit the models to the training data. We will use the map() function from the purrr package to iterate over the list of models and fit each model to the training data. The fit() function takes a formula, a model specification, and a data.frame as arguments. Here you can build any formula you see fit. What components of the date object do you want to use (e.g.¬†month? season?). What climate variables do you want to use?\nThe only requirement is that the response variable is Flow and the date variable is included as a predictor.\nOnce the models are fitted, we will use the as_modeltime_table() function to convert the list of fit models into a modeltime table. This will allow us to use the modeltime package to make predictions.\n\n\nModel Calibration\nNext, we will use the modeltime_calibrate() function to calibrate the models. This function takes a modeltime table and a data.frame of testing data as arguments. The function will return a modeltime table with the calibrated models.\nThe results of the calibration can be passed to modeltime_accuracy() to assess the accuracy of the models.\nCalibrate the models using the testing data and assess the accuracy of the models describing what you see!\n\n\nForecasting\nWith a calibrated model set in place, we can now make predictions using the modeltime_forecast(). Becasue we are using exogenous data, we need to pass the actual data to the function. This is because the model needs to know what the actual values are in order to make predictions.\nAs a first step, lets use the calibrated models to make predictions on the testing data. Here you will need to specifcy the actual_data (historic data tibble) and the new_data (testing).\nThe outputs can be passed to the plot_modeltime_forecast() function to visualize the predictions.\n\n\nRefitting the Model\nNow that we have a calibrated model set, we can refit the models to the full dataset. This is important because the models are only as good as the data they are trained on. By refitting the models to the full dataset, we can improve the accuracy of the predictions.\nTo do this, we will use the modeltime_refit() function. This function takes a calibrataion table and the full historic tibble as the data argument. The function will return a modeltime table with the refitted models. Like before, the accuracy of the models can be assessed using the modeltime_accuracy() function.\n\n\nLooking into the future\nNow we are at the end! We can use the refitted models to make predictions on the future data. This is where we will use the future tibble we created earlier as the new_data. The actual_data argument will be the historic data tibble.\nUsing your refitted models, the modeltime_forecast() function will return a modeltime table with the predictions that can be passed to the plot_modeltime_forecast() function to visualize the predictions.\n\n\nWrap up\nLooking at your predictions what do you think? How do the models compare? What do you think the future streamflow will be? What are the limitations of this analysis? What are the assumptions of the models?\nPerhaps you see negative values in the predictions; perhaps you dont see strong agreement between the furture starting points around Jan 2024; perhaps you see other patterns that seem systemic and questionablly wrong. At this stage that is quite alright. In this simple lab, we did not worry about issue like cross validation, featrue engineering, or hyperparameter tuning. These are all important steps in the modeling process and should be considered in a real world application.\nIf interested, the processes learned in tidymodels are directly applicable here and you can take this model as far as you like adding new data, correcting non-normal data, and implemeting a resampling approach to improve the model. That said, you now have a string introdcutory grasp on what the world of timeseries is about."
  }
]